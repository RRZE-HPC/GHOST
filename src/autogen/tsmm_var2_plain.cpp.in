/*!GHOST_AUTOGEN_TSMM *,* */
#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/math.h"
#include "ghost/instr.h"
#include "ghost/util.h"
#include "ghost/omp.h"
#include "ghost/locality.h"
#include "ghost/tsmm_var2_plain_gen.h"
#include <math.h>
#include <complex>
#include <float.h>
#include <typeinfo>

namespace {

template<typename T, typename iT, bool ROWMAJOR>
ghost_error tsmm_u_plain(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL)
    ghost_error ret = GHOST_SUCCESS;

    ghost_lidx N = w->traits.ncols;
    ghost_lidx M = v->traits.ncols;
    ghost_lidx K = DM_NROWS(v);


    const char *storageOrderString = "Col-Major";
    if (ROWMAJOR) storageOrderString = "Row-Major";
    GHOST_INFO_LOG("In %s TSMM with arbitrary block sizes %dx%d <- %dx%d * %dx%d, %s %s",
        storageOrderString, K, N, K, M, M, N, typeid(T).name(), typeid(iT).name());


    const T *const __restrict__ vval = (const T *)v->val;
    const iT *const __restrict__ wval = (const iT *)w->val;
    T *const __restrict__ xval = (T *)x->val;

    const ghost_lidx ldv = v->stride;
    const ghost_lidx ldw = w->stride;
    const ghost_lidx ldx = x->stride;

    const iT dalpha = *(iT *)alpha;
    const iT dbeta = *(iT *)beta;

#pragma omp parallel for schedule(static)
    for (ghost_lidx k = 0; k < K; k++) {
        for (ghost_lidx n = 0; n < N; n++) {
            if (ROWMAJOR) {
                iT temp = dbeta * (iT)xval[k * ldx + n];
                for (ghost_lidx m = 0; m < M; m++) {
                    temp += dalpha * (iT)vval[k * ldv + m] * wval[n * ldw + m];
                }
                xval[k * ldx + n] = (T)temp;
            } else {
                iT temp = dbeta * (iT)xval[n * ldx + k];
                for (ghost_lidx m = 0; m < M; m++) {
                    temp += dalpha * (iT)vval[m * ldv + k] * wval[n * ldw + m];
                }
                xval[n * ldx + k] = (T)temp;
            }
        }
    }
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL)
    return ret;
}

template<bool ROWMAJOR>
ghost_error tsmm_dispatcher(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
    if (x->traits.datatype & GHOST_DT_REAL) {
        if (x->traits.datatype & GHOST_DT_DOUBLE && w->traits.datatype & GHOST_DT_DOUBLE) {
            return tsmm_u_plain<double, double, ROWMAJOR>(x, v, w, alpha, beta);
        } else if (x->traits.datatype & GHOST_DT_FLOAT && w->traits.datatype & GHOST_DT_FLOAT) {
          return tsmm_u_plain<float, float, ROWMAJOR>(x, v, w, alpha, beta);
        } else if (x->traits.datatype & GHOST_DT_FLOAT && w->traits.datatype & GHOST_DT_DOUBLE) {
          return tsmm_u_plain<float, double, ROWMAJOR>(x, v, w, alpha, beta);
        }
    } else if (x->traits.datatype & GHOST_DT_COMPLEX) {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
            return tsmm_u_plain<std::complex<double>, std::complex<double>, ROWMAJOR>(x, v, w, alpha, beta);
        } else if (x->traits.datatype & GHOST_DT_FLOAT) {
          return tsmm_u_plain<std::complex<float>, std::complex<float>, ROWMAJOR>(x, v, w, alpha, beta);
        }
    }
    return GHOST_ERR_DATATYPE;
}
}


ghost_error ghost_tsmm__u_plain_x_x_x_1_1_cm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
    return tsmm_dispatcher<false>(x, v, w, alpha, beta);
}

ghost_error ghost_tsmm__u_plain_x_x_x_1_1_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
    return tsmm_dispatcher<true>(x, v, w, alpha, beta);
}
