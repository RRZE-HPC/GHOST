/*!GHOST_AUTOGEN_TSMTTSM *,* */
#include "ghost/config.h"
#include "ghost/densemat.h"
#include "ghost/instr.h"
#include "ghost/locality.h"
#include "ghost/log.h"
#include "ghost/rand.h"
#include "ghost/timing.h"
#include "ghost/tsmttsm_var2_cu_gen.h"
#include "ghost/types.h"
#include "ghost/util.h"
#include "ghost/cu_temp_buffer_malloc.h"

#include <complex.h>
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <curand.h>
#include <stdio.h>
#include <sys/types.h>
#include <unistd.h>
#include <iostream>

#include "ghost/cu_complex.h"

namespace {

template<typename T>
__global__ void deviceReduce(T *blockResults, T *result, T alpha, T beta, int blockCount,
    size_t lda, size_t ldb, size_t ldc, int M, int N)
{
    size_t tidx = blockDim.x * blockIdx.x + threadIdx.x;

    if (tidx >= M * N) return;

    int n = tidx / M;
    int m = tidx % M;

    T sum;
    zero(sum);

    for (int i = 0; i < blockCount; i++) { sum = accu(sum, blockResults[i * N * M + n * M + m]); }

    result[n * ldc + m] = axpby(sum, result[n * ldc + m], alpha, beta);
}

template<typename T, typename oT, bool conjv, int BLOCKSIZE>
__global__ void varBlockProductKernel(
    const T *A, const T *B, oT *out, size_t K, size_t lda, size_t ldb, size_t ldc, int M, int N)
{
    size_t tidx = blockDim.x * blockIdx.x + threadIdx.x;

    for (size_t mn = threadIdx.x; mn < M * N; mn += BLOCKSIZE) {
        zero(out[blockIdx.x * N * M + mn]);
    }

    __shared__ oT blockStorage[BLOCKSIZE];

    zero(blockStorage[threadIdx.x]);

    int m = tidx % M;
    int n = (tidx / M) % N;

    if (blockDim.x * gridDim.x / M / N == tidx / M / N) return;

    oT threadSum;
    zero(threadSum);

    if (conjv) {
#pragma unroll 4
        for (size_t idx = tidx / M / N; idx < K; idx += blockDim.x * gridDim.x / M / N) {
            threadSum = axpy(threadSum, (oT)conj(A[idx * lda + m]), (oT)__ldg(B + idx * ldb + n));
        }
    } else {
#pragma unroll 4
        for (size_t idx = tidx / M / N; idx < K; idx += blockDim.x * gridDim.x / M / N) {
            threadSum = axpy(threadSum, (oT)A[idx * lda + m], (oT)__ldg(B + idx * ldb + n));
        }
    }

    __syncthreads();
    blockStorage[threadIdx.x] = threadSum;
    __syncthreads();

    if (threadIdx.x < M * N) {
        oT blockSum;
        zero(blockSum);
        for (int i = threadIdx.x; i < BLOCKSIZE; i += M * N) {
            blockSum = accu(blockSum, blockStorage[i]);
        }
        out[blockIdx.x * N * M + n * M + m] = blockSum;
    }
}

template<typename T, typename oT, bool conjv>
void ghost_tsmttsm_cu_rm_fallback(oT *const __restrict__ C, const T *const __restrict__ A,
    const T *const __restrict__ B, const oT alpha, const oT beta, ghost_lidx K, ghost_lidx ldc,
    ghost_lidx lda, ghost_lidx ldb, const int M, const int N)
{

    ghost_error err = GHOST_SUCCESS;
    const int threadsPerBlock = 256;
    ghost_cu_deviceprop prop;
    ghost_cu_deviceprop_get(&prop);

    int numBlocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &numBlocks, varBlockProductKernel<T, oT, conjv, threadsPerBlock>, threadsPerBlock, 0);
    int blockCount = prop.multiProcessorCount * numBlocks;

    blockCount = min(K * M * N  / threadsPerBlock / 20 + 1, blockCount);


    void *d_temp_storage = NULL;
    size_t temp_storage_size = M * N * blockCount;

    ghost_cu_temp_buffer_malloc(&d_temp_storage, sizeof(oT) * temp_storage_size);

    //    CUDA_CALL(cudaMemset(d_temp_storage, 0, temp_storage_size * sizeof(oT)), err);
    if (err != GHOST_SUCCESS) return;

    varBlockProductKernel<T, oT, conjv, threadsPerBlock>
        <<<blockCount, threadsPerBlock>>>(A, B, (oT *)d_temp_storage, K, lda, ldb, ldc, M, N);

    deviceReduce<<<M * N / 256 + 1, 256>>>(
        (oT *)d_temp_storage, C, alpha, beta, blockCount, lda, ldb, ldc, M, N);
    ghost_cu_temp_buffer_free(d_temp_storage);
}
} // namespace

ghost_error ghost_tsmttsm__u_cuda_x_x_x_1_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta, int conjv)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL);
    ghost_error ret = GHOST_SUCCESS;

    if (x->traits.datatype & GHOST_DT_COMPLEX) {
        if (conjv) {
            if (x->traits.datatype & GHOST_DT_DOUBLE) {
                ghost_tsmttsm_cu_rm_fallback<cuDoubleComplex, cuDoubleComplex, true>(
                    (cuDoubleComplex *)x->cu_val, (const cuDoubleComplex *)v->cu_val,
                    (const cuDoubleComplex *)w->cu_val, *(cuDoubleComplex *)alpha, *(cuDoubleComplex *)beta,
                    DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);
            } else {
                ghost_tsmttsm_cu_rm_fallback<cuFloatComplex, cuFloatComplex, true>(
                    (cuFloatComplex *)x->cu_val, (const cuFloatComplex *)v->cu_val,
                    (const cuFloatComplex *)w->cu_val, *(cuFloatComplex *)alpha, *(cuFloatComplex *)beta,
                    DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);
            }
        } else {
            if (x->traits.datatype & GHOST_DT_DOUBLE) {
                ghost_tsmttsm_cu_rm_fallback<cuDoubleComplex, cuDoubleComplex, false>(
                    (cuDoubleComplex *)x->cu_val, (const cuDoubleComplex *)v->cu_val,
                    (const cuDoubleComplex *)w->cu_val, *(cuDoubleComplex *)alpha, *(cuDoubleComplex *)beta,
                    DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);
            } else {
                ghost_tsmttsm_cu_rm_fallback<cuFloatComplex, cuFloatComplex, false>(
                    (cuFloatComplex *)x->cu_val, (const cuFloatComplex *)v->cu_val,
                    (const cuFloatComplex *)w->cu_val, *(cuFloatComplex *)alpha, *(cuFloatComplex *)beta,
                    DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);
            }
        }
    } else {
        if (x->traits.datatype & GHOST_DT_DOUBLE && w->traits.datatype & GHOST_DT_DOUBLE) {
            ghost_tsmttsm_cu_rm_fallback<double, double, false>((double *)x->cu_val,
                (const double *)v->cu_val, (const double *)w->cu_val, *(double *)alpha, *(double *)beta,
                DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);

        } else if (x->traits.datatype & GHOST_DT_FLOAT && w->traits.datatype & GHOST_DT_FLOAT) {
            ghost_tsmttsm_cu_rm_fallback<float, float, false>((float *)x->cu_val,
                (const float *)v->cu_val, (const float *)w->cu_val, *(float *)alpha, *(float *)beta,
                DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);
        } else if (x->traits.datatype & GHOST_DT_DOUBLE && w->traits.datatype & GHOST_DT_FLOAT) {
            ghost_tsmttsm_cu_rm_fallback<float, double, false>((double *)x->cu_val,
                (const float *)v->cu_val, (const float *)w->cu_val, *(double *)alpha, *(double *)beta,
                DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols, x->traits.ncols);
        }
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL);
    CUDA_CALL_RETURN(cudaGetLastError());
    return ret;
}
