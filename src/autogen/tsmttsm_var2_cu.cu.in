/*!GHOST_AUTOGEN_TSMTTSM *,*,1 */
#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/util.h"
#include "ghost/densemat.h"
#include "ghost/log.h"
#include "ghost/timing.h"
#include "ghost/locality.h"
#include "ghost/instr.h"
#include "ghost/rand.h"
#include "ghost/tsmttsm_var2_cu_gen.h"

#include <cuda_runtime.h>
#include <stdio.h>
#include <cublas_v2.h>
#include <curand.h>
#include <sys/types.h>
#include <unistd.h>
#include <complex.h>

#include "ghost/cu_complex.h"

namespace {

void *temp_storage = NULL;
size_t temp_storage_bytes = 0;

namespace GENV4 {
template <typename T>
__global__ void deviceReduce(T *blockResults, T *result, T alpha, T beta,
                             int blockCount, size_t lda, size_t ldb, size_t ldc,
                             int M, int N) {
  size_t tidx = blockDim.x * blockIdx.x + threadIdx.x;

  if (tidx >= M * N) return;

  int n = tidx / M;
  int m = tidx % M;

  T sum;
  zero(sum);

  for (int i = 0; i < blockCount; i++) {
    sum = accu (sum,blockResults[i * N * ldc + n * ldc + m]);
  }

  result[n * ldc + m] = axpby(sum,result[n * ldc + m],alpha,beta);
}

template <typename T, bool conjv, int BLOCKSIZE, bool TRANSPOSE>
__global__ void varBlockProductKernel(const T *A, const T *B, T *out, size_t K,
                                      size_t lda, size_t ldb, size_t ldc, int M,
                                      int N) {
  size_t tidx = blockDim.x * blockIdx.x + threadIdx.x;

  __shared__ T blockStorage[BLOCKSIZE];

  zero(blockStorage[threadIdx.x]);

  int m = tidx % M;
  int n = (tidx / M) % N;

  if (blockDim.x * gridDim.x / M / N == tidx / M / N) return;

  T threadSum;
  zero(threadSum);

  if (conjv) {
#pragma unroll 4
      for (size_t idx = tidx / M / N; idx < K;
           idx += blockDim.x * gridDim.x / M / N) {
        threadSum = axpy(threadSum, conj(A[idx * lda + m]), __ldg( B + idx * ldb + n));
      }
  } else {
#pragma unroll 4
      for (size_t idx = tidx / M / N; idx < K;
           idx += blockDim.x * gridDim.x / M / N) {
        threadSum = axpy(threadSum, A[idx * lda + m], __ldg( B + idx * ldb + n));
      }
  }

  __syncthreads();
  blockStorage[threadIdx.x] = threadSum;
  __syncthreads();

  if (threadIdx.x < M * N) {
    T blockSum;
    zero(blockSum);
    for (int i = threadIdx.x; i < BLOCKSIZE; i += M * N) {
      blockSum = accu(blockSum,blockStorage[i]);
    }
    out[blockIdx.x * N * ldc + n * ldc + m] = blockSum;
  }
}
}

template <typename T, bool conjv>
void ghost_tsmttsm_cu_rm_fallback(T *const __restrict__ C,
                                  const T *const __restrict__ A,
                                  const T *const __restrict__ B, const T alpha,
                                  const T beta, ghost_lidx K, ghost_lidx ldc,
                                  ghost_lidx lda, ghost_lidx ldb, const int M,
                                  const int N) {
  const int threadsPerBlock = 256;
  int deviceUsed;
  cudaGetDevice(&deviceUsed);
  cudaDeviceProp prop;
  cudaGetDeviceProperties(&prop, deviceUsed);
  int numBlocks;
  cudaOccupancyMaxActiveBlocksPerMultiprocessor(
      &numBlocks, GENV4::varBlockProductKernel<T, conjv, threadsPerBlock, true>,
      threadsPerBlock, 0);
  int blockCount = prop.multiProcessorCount * numBlocks;

  if (temp_storage_bytes == 0 || temp_storage == NULL) {
    temp_storage_bytes = blockCount * sizeof(T) * N * ldc;
    ghost_cu_malloc(&temp_storage, temp_storage_bytes);
    if (temp_storage == NULL) temp_storage_bytes = 0;
  }
  ghost_error err = GHOST_SUCCESS;
  CUDA_CALL(cudaMemset(temp_storage, 0, temp_storage_bytes), err);
  if (err != GHOST_SUCCESS) return;

  GENV4::varBlockProductKernel<T, conjv, threadsPerBlock,
                               false><<<blockCount, threadsPerBlock>>>(
      A, B, (T *)temp_storage, K, lda, ldb, ldc, M, N);

  GENV4::deviceReduce<<<M * N / 256 + 1, 256>>>(
      (T *)temp_storage, C, alpha, beta, blockCount, lda, ldb, ldc, M, N);

  ghost_cu_free(temp_storage);
  temp_storage = NULL;
  temp_storage_bytes = 0;
}
}

ghost_error ghost_tsmttsm__u_cuda_x_x_x_1_rm(ghost_densemat *x,ghost_densemat *v,ghost_densemat *w, void *alpha,void *beta, int conjv)
{
  GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL);
  ghost_error ret = GHOST_SUCCESS;
  
  if (x->traits.datatype & GHOST_DT_COMPLEX) {
      if (conjv) {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
          ghost_tsmttsm_cu_rm_fallback<cuDoubleComplex,true>(
              (cuDoubleComplex *)x->cu_val, (const cuDoubleComplex *)v->cu_val,
              (const cuDoubleComplex *)w->cu_val, *(cuDoubleComplex *)alpha,
              *(cuDoubleComplex *)beta, DM_NROWS(v), x->stride, v->stride,
              w->stride, v->traits.ncols, x->traits.ncols);
        } else {
          ghost_tsmttsm_cu_rm_fallback<cuFloatComplex,true>(
              (cuFloatComplex *)x->cu_val, (const cuFloatComplex *)v->cu_val,
              (const cuFloatComplex *)w->cu_val, *(cuFloatComplex *)alpha,
              *(cuFloatComplex *)beta, DM_NROWS(v), x->stride, v->stride,
              w->stride, v->traits.ncols, x->traits.ncols);
        }
      } else {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
          ghost_tsmttsm_cu_rm_fallback<cuDoubleComplex,false>(
              (cuDoubleComplex *)x->cu_val, (const cuDoubleComplex *)v->cu_val,
              (const cuDoubleComplex *)w->cu_val, *(cuDoubleComplex *)alpha,
              *(cuDoubleComplex *)beta, DM_NROWS(v), x->stride, v->stride,
              w->stride, v->traits.ncols, x->traits.ncols);
        } else {
          ghost_tsmttsm_cu_rm_fallback<cuFloatComplex,false>(
              (cuFloatComplex *)x->cu_val, (const cuFloatComplex *)v->cu_val,
              (const cuFloatComplex *)w->cu_val, *(cuFloatComplex *)alpha,
              *(cuFloatComplex *)beta, DM_NROWS(v), x->stride, v->stride,
              w->stride, v->traits.ncols, x->traits.ncols);
        }
      }
  } else {
    if (x->traits.datatype & GHOST_DT_DOUBLE) {
      ghost_tsmttsm_cu_rm_fallback<double,false>(
          (double *)x->cu_val, (const double *)v->cu_val,
          (const double *)w->cu_val, *(double *)alpha, *(double *)beta,
          DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols,
          x->traits.ncols);
    } else {
      ghost_tsmttsm_cu_rm_fallback<float,false>(
          (float *)x->cu_val, (const float *)v->cu_val,
          (const float *)w->cu_val, *(float *)alpha, *(float *)beta,
          DM_NROWS(v), x->stride, v->stride, w->stride, v->traits.ncols,
          x->traits.ncols);
    }
  }

  GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL);
  CUDA_CALL_RETURN(cudaGetLastError());
  return ret;
}
