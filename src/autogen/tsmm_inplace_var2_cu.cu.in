/*!GHOST_AUTOGEN_TSMM-INPLACE *,* */
#include "ghost/config.h"
#include "ghost/densemat.h"
#include "ghost/instr.h"
#include "ghost/locality.h"
#include "ghost/log.h"
#include "ghost/rand.h"
#include "ghost/timing.h"
#include "ghost/tsmm_inplace_var2_cu_gen.h"
#include "ghost/types.h"
#include "ghost/util.h"

#include <complex.h>
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <curand.h>
#include <stdio.h>
#include <sys/types.h>
#include <unistd.h>
#include <iostream>

#include "ghost/cu_complex.h"

namespace {

template<typename T>
bool eq(const T lhs, const T rhs)
{
    return lhs == rhs;
};

template<>
bool eq<cuDoubleComplex>(const cuDoubleComplex lhs, const cuDoubleComplex rhs)
{
    return lhs.x == rhs.x && lhs.y == rhs.y;
}

template<>
bool eq<cuFloatComplex>(const cuFloatComplex lhs, const cuFloatComplex rhs)
{
    return lhs.x == rhs.x && lhs.y == rhs.y;
}

template<typename T, typename iT, bool BETAISZERO>
static __global__ void tsmm_varip2_kernel(T *A, const iT *B, T *out, const int M, const int N,
    const int K, const int lda, const int ldb, const int ldc, iT alpha, iT beta)
{
    int tidx = blockIdx.x * blockDim.x + threadIdx.x;
    int n = tidx % N;

    const int unrollFactor = 8;
    iT sums[unrollFactor];
    for (int i = 0; i < unrollFactor; i++) {
        zero(sums[i]);
    }

    for (int m = 0; m < M; m++) {
#pragma unroll(unrollFactor)
        for (int i = 0; i < unrollFactor; i++) {
            int row = tidx / N + i * (blockDim.x * gridDim.x / N);
            sums[i] = axpy(sums[i], (iT)__ldg(A + row * lda + m), __ldg(B + n * lda + m));
        }
    }

    __syncthreads();
#pragma unroll(unrollFactor)
    for (int i = 0; i < unrollFactor; i++) {
        int row = tidx / N + i * (blockDim.x * gridDim.x / N);
        if (BETAISZERO) {
            A[row * ldc + n] = (T)scale(alpha, sums[i]);
        } else {
            A[row * ldc + n] = (T)axpby(sums[i], (iT)A[row * ldc + n], alpha, beta);
        }
    }
}

template<typename T, typename iT>
static __global__ void tsmm_remainder_kernel(T *A, const iT *B, T *out, const int M, const int N,
    const int K, const int lda, const int ldb, const int ldc, iT alpha, iT beta, int startIdx)
{
    int tidx = blockIdx.x * blockDim.x + threadIdx.x;
    int n = tidx % N;

    for (int row = startIdx + tidx / N; row < K; row += gridDim.x * blockDim.x) {
        iT sum;
        zero(sum);
        for (int m = 0; m < M; m++) {
            sum = axpy(sum, (iT)__ldg(A + row * lda + m), B[n * ldb + m]);
        }
        __syncthreads();
        A[row * ldc + n] = (T)axpby(sum, (iT)__ldg(A + row * ldc + n), alpha, beta);
    }
}


template<typename T, typename iT>
void tsmm_dispatch(ghost_densemat *x, ghost_densemat *w, void *alpha, void *beta)
{
    ghost_lidx M = DM_NROWS(w);
    ghost_lidx N = w->traits.ncols;
    ghost_lidx K = DM_NROWS(x);
    T *A = (T *)x->cu_val;
    ghost_lidx lda = x->stride;
    iT *B = (iT *)w->cu_val;
    ghost_lidx ldb = w->stride;
    iT dalpha = *(iT *)alpha;
    iT dbeta = *(iT *)beta;

    const int threadsPerBlock = (512 / N) * N;
    int blockCount = K / (threadsPerBlock * 8 / N);

    if (blockCount != 0) {
        iT Tzero;
        zero(Tzero);
        if (eq(dbeta, Tzero)) {
            tsmm_varip2_kernel<T, iT, true>
                <<<blockCount, threadsPerBlock>>>(A, B, A, M, N, K, lda, ldb, lda, dalpha, dbeta);
        } else {
            tsmm_varip2_kernel<T, iT, false>
                <<<blockCount, threadsPerBlock>>>(A, B, A, M, N, K, lda, ldb, lda, dalpha, dbeta);
        }
    }
    tsmm_remainder_kernel<T, iT><<<10, (1024 / N) * N>>>(
        A, B, A, M, N, K, lda, ldb, lda, dalpha, dbeta, blockCount * (threadsPerBlock / N * 8));
}
}

ghost_error ghost_tsmm_inplace__u_cuda_x_x_x(ghost_densemat *x, ghost_densemat *w, void *alpha, void *beta)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL);
    ghost_error ret = GHOST_SUCCESS;


    if (x->traits.datatype & GHOST_DT_COMPLEX) {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
            tsmm_dispatch<cuDoubleComplex, cuDoubleComplex>(x, w, alpha, beta);
        } else {
            tsmm_dispatch<cuFloatComplex, cuFloatComplex>(x, w, alpha, beta);
        }
    } else {
        if (x->traits.datatype & GHOST_DT_DOUBLE && w->traits.datatype & GHOST_DT_DOUBLE) {
            tsmm_dispatch<double, double>(x, w, alpha, beta);
        } else if (x->traits.datatype & GHOST_DT_FLOAT && w->traits.datatype & GHOST_DT_FLOAT) {
            tsmm_dispatch<float, float>(x, w, alpha, beta);
        } else if (x->traits.datatype & GHOST_DT_FLOAT && w->traits.datatype & GHOST_DT_DOUBLE) {
            tsmm_dispatch<float, double>(x, w, alpha, beta);
        }
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH | GHOST_FUNCTYPE_KERNEL);
    CUDA_CALL_RETURN(cudaGetLastError());
    return ret;
}
