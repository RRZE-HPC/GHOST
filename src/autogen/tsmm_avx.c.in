/*!GHOST_AUTOGEN_TSMM K,N,UNROLL */
#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/densemat.h"
#include "ghost/instr.h"
#include "ghost/util.h"
#include "ghost/tsmm_avx_gen.h"

#include <immintrin.h>
#include <math.h>
#include <float.h>

#define KREMAINDER

#GHOST_SUBST INCOLS ${K}
#GHOST_SUBST OUTCOLS ${N}
#GHOST_SUBST OUTERUNROLL ${UNROLL}

// general complex multiplication
#define complex_mul(a,b) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(a,a,0),b),_mm256_mul_pd(_mm256_shuffle_pd(a,a,0xF),_mm256_shuffle_pd(b,b,5)))

// complex multiplication with the second operand already stored as [imag0,real0,imag1,real1]
#define complex_mul_bshuf(a,b) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(a,a,0),b),_mm256_mul_pd(_mm256_shuffle_pd(a,a,0xF),b))

#define complex_mul_asplit_bflip(are,aim,b,bflip) _mm256_addsub_pd(_mm256_mul_pd(are,b),_mm256_mul_pd(aim,bflip))
#define complex_mul_aflip(a,aflip,b) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(b,b,0),a),_mm256_mul_pd(_mm256_shuffle_pd(b,b,0xF),aflip))


ghost_error ghost_tsmm__u_avx_z_OUTCOLS_INCOLS_OUTERUNROLL_1_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
#ifdef GHOST_BUILD_AVX
#if OUTCOLS%4
    __m256i mask, halfmask;
#if OUTCOLS%4 == 1
    mask = _mm256_set_epi64x(0,0,0,~0);
    halfmask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#if OUTCOLS%4 == 2
    mask = _mm256_set_epi64x(0,0,~0,~0);
    halfmask = _mm256_set_epi64x(~0,~0,~0,~0);
#endif
#if OUTCOLS%4 == 3
    mask = _mm256_set_epi64x(0,~0,~0,~0);
    halfmask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#endif
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    ghost_error ret = GHOST_SUCCESS;

    ghost_lidx n = DM_NROWS(v);

    INFO_LOG("In AVX TSMM with two fixed block sizes [OUTCOLS][INCOLS] %"PRLIDX"x%"PRLIDX" <- %"PRLIDX"x%"PRLIDX" * %"PRLIDX"x%"PRLIDX,n,x->traits.ncols,n,v->traits.ncols,v->traits.ncols,x->traits.ncols);
    
    if (n%OUTERUNROLL) {
        n+=(OUTERUNROLL-n%OUTERUNROLL);
        INFO_LOG("Padding large dimension to %d\n",n);
    }

    const double complex * const restrict vval = (const double complex*) v->val;
    const double complex * const restrict wval = (const double complex*) w->val;
    double complex * const restrict xval = (double complex*) x->val;

    const ghost_lidx ldv = v->stride;
    const ghost_lidx ldw = w->stride;
    const ghost_lidx ldx = x->stride;

    const double complex dalpha = *(double complex *)alpha;
    const double complex dbeta = *(double complex *)beta;
    __m256d alphare, alphaim;
    alphare = _mm256_set1_pd(creal(dalpha));
    alphaim = _mm256_set1_pd(cimag(dalpha));

    __m256d betavec;
    betavec = _mm256_broadcast_pd(beta);
    ghost_lidx i,m,k,j,s;

    double * restrict wre = NULL, * restrict wim = NULL;
    ghost_malloc_align((void **)&wre,sizeof(double)*DM_NROWS(w)*w->traits.ncols,32);
    ghost_malloc_align((void **)&wim,sizeof(double)*DM_NROWS(w)*w->traits.ncols,32);
    for (s=0; s<w->traits.ncols; s++) {
        for (j=0; j<DM_NROWS(w); j++) {
            wre[j*w->traits.ncols+s] = creal(wval[s*ldw+j]);
            wim[j*w->traits.ncols+s] = cimag(wval[s*ldw+j]);
        }
    }

    if (fabs(creal(dalpha)-1.) > DBL_MIN || fabs(cimag(dalpha)) > DBL_MIN || fabs(creal(dbeta)) > DBL_MIN || fabs(cimag(dbeta)) > DBL_MIN) { // general case: X = b*X + a*V*W
#pragma omp parallel private(k,m)
        {
            #GHOST_UNROLL#__m256d vrev@,vimv@,tmpre@,tmpim@,tmpref@,tmpimf@,res0@,res1@;#OUTERUNROLL
#pragma omp for schedule(runtime)
            for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
#pragma unroll_and_jam
                for (k=0; k<OUTCOLS/4; k++) {
                    #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                    for (m=0; m<INCOLS; m++) {
                        #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                        #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                        #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_loadu_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vimv@,_mm256_loadu_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                        #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_loadu_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vrev@,_mm256_loadu_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                    }

                    #GHOST_UNROLL#tmpref@ = _mm256_sub_pd(_mm256_mul_pd(alphare,tmpre@),_mm256_mul_pd(alphaim,tmpim@));#OUTERUNROLL
                    #GHOST_UNROLL#tmpimf@ = _mm256_add_pd(_mm256_mul_pd(alphare,tmpim@),_mm256_mul_pd(alphaim,tmpre@));#OUTERUNROLL

                    #GHOST_UNROLL#tmpimf@ = _mm256_shuffle_pd(tmpimf@,tmpimf@,0b0101);#OUTERUNROLL
                    #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x20);#OUTERUNROLL
                    #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x31);#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_storeu_pd((double *)&xval[(i+@)*ldx+(4*k)],_mm256_add_pd(complex_mul(betavec,_mm256_loadu_pd((double *)&xval[(i+@)*ldx+(4*k)])),res0@));#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_storeu_pd((double *)&xval[(i+@)*ldx+(4*k)+2],_mm256_add_pd(complex_mul(betavec,_mm256_loadu_pd((double *)&xval[(i+@)*ldx+(4*k)+2])),res1@));#OUTERUNROLL
                }
#if OUTCOLS%4
                #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                for (m=0; m<INCOLS; m++) {
                    #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                    #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                    #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                }

                #GHOST_UNROLL#tmpref@ = _mm256_sub_pd(_mm256_mul_pd(alphare,tmpre@),_mm256_mul_pd(alphaim,tmpim@));#OUTERUNROLL
                #GHOST_UNROLL#tmpimf@ = _mm256_add_pd(_mm256_mul_pd(alphare,tmpim@),_mm256_mul_pd(alphaim,tmpre@));#OUTERUNROLL

                #GHOST_UNROLL#tmpimf@ = _mm256_shuffle_pd(tmpimf@,tmpimf@,0b0101);#OUTERUNROLL
                #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x20);#OUTERUNROLL
#if OUTCOLS%4 > 2
                #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x31);#OUTERUNROLL
#endif
#if OUTCOLS%4 <= 2
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)],halfmask,_mm256_add_pd(complex_mul(betavec,_mm256_maskload_pd((double *)&xval[(i+@)*ldx+(4*k)],halfmask)),res0@));#OUTERUNROLL
#else
                #GHOST_UNROLL#_mm256_store_pd((double *)&xval[(i+@)*ldx+(4*k)],_mm256_add_pd(complex_mul(betavec,_mm256_load_pd((double *)&xval[(i+@)*ldx+(4*k)])),res0@));#OUTERUNROLL
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)+2],halfmask,_mm256_add_pd(complex_mul(betavec,_mm256_maskload_pd((double *)&xval[(i+@)*ldx+(4*k)+2],halfmask)),res1@));#OUTERUNROLL
#endif
#endif

            }
        }
    } else { // common case: X = V*W
        INFO_LOG("Fast case: alpha=1 and beta=0");
#pragma omp parallel private(k,m)
        {
            #GHOST_UNROLL#__m256d vrev@,vimv@,tmpre@,tmpim@,res0@,res1@;#OUTERUNROLL
#pragma omp for schedule(runtime)
            for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
                for (k=0; k<OUTCOLS/4; k++) {
                    #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                    for (m=0; m<INCOLS; m++) {
                        #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                        #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                        #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_loadu_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vimv@,_mm256_loadu_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                        #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_loadu_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vrev@,_mm256_loadu_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                    }
                    #GHOST_UNROLL#tmpim@ = _mm256_shuffle_pd(tmpim@,tmpim@,0b0101);#OUTERUNROLL
                    #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x20);#OUTERUNROLL
                    #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x31);#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_storeu_pd((double *)&xval[(i+@)*ldx+(4*k)],res0@);#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_storeu_pd((double *)&xval[(i+@)*ldx+(4*k)+2],res1@);#OUTERUNROLL
                }
#if OUTCOLS%4
                #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                for (m=0; m<INCOLS; m++) {
                    #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                    #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                    #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                }

                #GHOST_UNROLL#tmpim@ = _mm256_shuffle_pd(tmpim@,tmpim@,0b0101);#OUTERUNROLL
                #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x20);#OUTERUNROLL
#if OUTCOLS%4 > 2
                #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x31);#OUTERUNROLL
#endif
#if OUTCOLS%4 <= 2
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)],halfmask,res0@);#OUTERUNROLL
#else
                #GHOST_UNROLL#_mm256_store_pd((double *)&xval[(i+@)*ldx+(4*k)],res0@);#OUTERUNROLL
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)+2],halfmask,res1@);#OUTERUNROLL
#endif
#endif
            }
        }
    }

    free(wre);
    free(wim);
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH)
    return ret;
#else
    UNUSED(x);
    UNUSED(v);
    UNUSED(w);
    UNUSED(alpha);
    UNUSED(beta);
    ERROR_LOG("AVX not available!");
    return GHOST_ERR_UNKNOWN;
#endif
}

ghost_error ghost_tsmm__a_avx_z_OUTCOLS_INCOLS_OUTERUNROLL_1_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
#ifdef GHOST_BUILD_AVX
#if OUTCOLS%4
    __m256i mask;
#if OUTCOLS%4 == 1
    __m256i halfmask;
    mask = _mm256_set_epi64x(0,0,0,~0);
    halfmask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#if OUTCOLS%4 == 2
    mask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#if OUTCOLS%4 == 3
    __m256i halfmask;
    mask = _mm256_set_epi64x(0,~0,~0,~0);
    halfmask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#endif
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    ghost_error ret = GHOST_SUCCESS;

    ghost_lidx n = DM_NROWS(v);

    INFO_LOG("In AVX TSMM with two fixed block sizes [OUTCOLS][INCOLS] %"PRLIDX"x%"PRLIDX" <- %"PRLIDX"x%"PRLIDX" * %"PRLIDX"x%"PRLIDX,n,x->traits.ncols,n,v->traits.ncols,v->traits.ncols,x->traits.ncols);
    
    if (n%OUTERUNROLL) {
        n+=(OUTERUNROLL-n%OUTERUNROLL);
        INFO_LOG("Padding large dimension to %d\n",n);
    }

    const double complex * const restrict vval = (const double complex*) v->val;
    const double complex * const restrict wval = (const double complex*) w->val;
    double complex * const restrict xval = (double complex*) x->val;

    const ghost_lidx ldv = v->stride;
    const ghost_lidx ldw = w->stride;
    const ghost_lidx ldx = x->stride;

    const double complex dalpha = *(double complex *)alpha;
    const double complex dbeta = *(double complex *)beta;
    __m256d alphare, alphaim;
    alphare = _mm256_set1_pd(creal(dalpha));
    alphaim = _mm256_set1_pd(cimag(dalpha));

    __m256d betavec;
    betavec = _mm256_broadcast_pd(beta);
    ghost_lidx i,m,k,j,s;

    double * restrict wre = NULL, * restrict wim = NULL;
    ghost_malloc_align((void **)&wre,sizeof(double)*DM_NROWS(w)*w->traits.ncols,32);
    ghost_malloc_align((void **)&wim,sizeof(double)*DM_NROWS(w)*w->traits.ncols,32);
    for (s=0; s<w->traits.ncols; s++) {
        for (j=0; j<DM_NROWS(w); j++) {
            wre[j*w->traits.ncols+s] = creal(wval[s*ldw+j]);
            wim[j*w->traits.ncols+s] = cimag(wval[s*ldw+j]);
        }
    }

    if (fabs(creal(dalpha)-1.) > DBL_MIN || fabs(cimag(dalpha)) > DBL_MIN || fabs(creal(dbeta)) > DBL_MIN || fabs(cimag(dbeta)) > DBL_MIN) { // general case: X = b*X + a*V*W
#pragma omp parallel private(k,m)
        {
            #GHOST_UNROLL#__m256d vrev@,vimv@,tmpre@,tmpim@,tmpref@,tmpimf@,res0@,res1@;#OUTERUNROLL
#pragma omp for schedule(runtime)
            for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
                for (k=0; k<OUTCOLS/4; k++) {
                    #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                    for (m=0; m<INCOLS; m++) {
                        #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                        #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                        #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_load_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vimv@,_mm256_load_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                        #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_load_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vrev@,_mm256_load_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                    }

                    #GHOST_UNROLL#tmpref@ = _mm256_sub_pd(_mm256_mul_pd(alphare,tmpre@),_mm256_mul_pd(alphaim,tmpim@));#OUTERUNROLL
                    #GHOST_UNROLL#tmpimf@ = _mm256_add_pd(_mm256_mul_pd(alphare,tmpim@),_mm256_mul_pd(alphaim,tmpre@));#OUTERUNROLL

                    #GHOST_UNROLL#tmpimf@ = _mm256_shuffle_pd(tmpimf@,tmpimf@,0b0101);#OUTERUNROLL
                    #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x20);#OUTERUNROLL
                    #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x31);#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_store_pd((double *)&xval[(i+@)*ldx+(4*k)],_mm256_add_pd(complex_mul(betavec,_mm256_load_pd((double *)&xval[(i+@)*ldx+(4*k)])),res0@));#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_store_pd((double *)&xval[(i+@)*ldx+(4*k)+2],_mm256_add_pd(complex_mul(betavec,_mm256_load_pd((double *)&xval[(i+@)*ldx+(4*k)+2])),res1@));#OUTERUNROLL
                }
#if OUTCOLS%4
                #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                for (m=0; m<INCOLS; m++) {
                    #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                    #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                    #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                }

                #GHOST_UNROLL#tmpref@ = _mm256_sub_pd(_mm256_mul_pd(alphare,tmpre@),_mm256_mul_pd(alphaim,tmpim@));#OUTERUNROLL
                #GHOST_UNROLL#tmpimf@ = _mm256_add_pd(_mm256_mul_pd(alphare,tmpim@),_mm256_mul_pd(alphaim,tmpre@));#OUTERUNROLL

                #GHOST_UNROLL#tmpimf@ = _mm256_shuffle_pd(tmpimf@,tmpimf@,0b0101);#OUTERUNROLL
                #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x20);#OUTERUNROLL
#if OUTCOLS%4 > 2
                #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpref@,tmpimf@,0b0101),_mm256_blend_pd(tmpref@,tmpimf@,0b0101),0b0101),0x31);#OUTERUNROLL
#endif
#if OUTCOLS%4 == 1
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)],halfmask,_mm256_add_pd(complex_mul(betavec,_mm256_maskload_pd((double *)&xval[(i+@)*ldx+(4*k)],halfmask)),res0@));#OUTERUNROLL
#endif
#if OUTCOLS%4 == 2
                #GHOST_UNROLL#_mm256_store_pd((double *)&xval[(i+@)*ldx+(4*k)],_mm256_add_pd(complex_mul(betavec,_mm256_load_pd((double *)&xval[(i+@)*ldx+(4*k)])),res0@));#OUTERUNROLL
#endif
#if OUTCOLS%4 == 3
                #GHOST_UNROLL#_mm256_store_pd((double *)&xval[(i+@)*ldx+(4*k)],_mm256_add_pd(complex_mul(betavec,_mm256_load_pd((double *)&xval[(i+@)*ldx+(4*k)])),res0@));#OUTERUNROLL
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)+2],halfmask,_mm256_add_pd(complex_mul(betavec,_mm256_maskload_pd((double *)&xval[(i+@)*ldx+(4*k)+2],halfmask)),res1@));#OUTERUNROLL
#endif
#endif
            }
        }
    } else { // common case: X = V*W
        INFO_LOG("Fast case: alpha=1 and beta=0");
#pragma omp parallel private(k,m)
        {
            #GHOST_UNROLL#__m256d vrev@,vimv@,tmpre@,tmpim@,res0@,res1@;#OUTERUNROLL
#pragma omp for schedule(runtime)
            for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
                for (k=0; k<OUTCOLS/4; k++) {
                    #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                    for (m=0; m<INCOLS; m++) {
                        #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                        #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                        #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_load_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vimv@,_mm256_load_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                        #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_load_pd(&wre[m*OUTCOLS+(k*4)])),_mm256_mul_pd(vrev@,_mm256_load_pd(&wim[m*OUTCOLS+(k*4)]))));#OUTERUNROLL
                    }
                    #GHOST_UNROLL#tmpim@ = _mm256_shuffle_pd(tmpim@,tmpim@,0b0101);#OUTERUNROLL
                    #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x20);#OUTERUNROLL
                    #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x31);#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_stream_pd((double *)&xval[(i+@)*ldx+(4*k)],res0@);#OUTERUNROLL
                    #GHOST_UNROLL#_mm256_stream_pd((double *)&xval[(i+@)*ldx+(4*k)+2],res1@);#OUTERUNROLL
                }
#if OUTCOLS%4
                #GHOST_UNROLL#tmpre@ = _mm256_setzero_pd();#OUTERUNROLL
                #GHOST_UNROLL#tmpim@ = _mm256_setzero_pd();#OUTERUNROLL
#pragma unroll(INCOLS)
                for (m=0; m<INCOLS; m++) {
                    #GHOST_UNROLL#vrev@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)]);#OUTERUNROLL
                    #GHOST_UNROLL#vimv@ = _mm256_broadcast_sd(&((double *)vval)[2*((i+@)*ldv+m)+1]);#OUTERUNROLL
                    #GHOST_UNROLL#tmpre@ = _mm256_add_pd(tmpre@,_mm256_sub_pd(_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                    #GHOST_UNROLL#tmpim@ = _mm256_add_pd(tmpim@,_mm256_add_pd(_mm256_mul_pd(vimv@,_mm256_maskload_pd(&wre[m*OUTCOLS+(k*4)],mask)),_mm256_mul_pd(vrev@,_mm256_maskload_pd(&wim[m*OUTCOLS+(k*4)],mask))));#OUTERUNROLL
                }

                #GHOST_UNROLL#tmpim@ = _mm256_shuffle_pd(tmpim@,tmpim@,0b0101);#OUTERUNROLL
                #GHOST_UNROLL#res0@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x20);#OUTERUNROLL
#if OUTCOLS%4 > 2
                #GHOST_UNROLL#res1@ = _mm256_permute2f128_pd(_mm256_blend_pd(tmpre@,tmpim@,0b1010),_mm256_shuffle_pd(_mm256_blend_pd(tmpre@,tmpim@,0b0101),_mm256_blend_pd(tmpre@,tmpim@,0b0101),0b0101),0x31);#OUTERUNROLL
#endif
#if OUTCOLS%4 == 1
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)],halfmask,res0@);#OUTERUNROLL
#endif
#if OUTCOLS%4 == 2
                #GHOST_UNROLL#_mm256_stream_pd((double *)&xval[(i+@)*ldx+(4*k)],res0@);#OUTERUNROLL
#endif
#if OUTCOLS%4 == 3
                #GHOST_UNROLL#_mm256_stream_pd((double *)&xval[(i+@)*ldx+(4*k)],res0@);#OUTERUNROLL
                #GHOST_UNROLL#_mm256_maskstore_pd((double *)&xval[(i+@)*ldx+(4*k)+2],halfmask,res1@);#OUTERUNROLL
#endif
#endif
            }
        }
    }

    free(wre);
    free(wim);
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH)
    return ret;
#else
    UNUSED(x);
    UNUSED(v);
    UNUSED(w);
    UNUSED(alpha);
    UNUSED(beta);
    ERROR_LOG("AVX not available!");
    return GHOST_ERR_UNKNOWN;
#endif
}

ghost_error ghost_tsmm__a_avx_d_OUTCOLS_INCOLS_OUTERUNROLL_4_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
#ifdef GHOST_BUILD_AVX
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    ghost_error ret = GHOST_SUCCESS;

    ghost_lidx n = DM_NROWS(v);

    INFO_LOG("In AVX TSMM with two fixed block sizes [OUTCOLS][INCOLS] %"PRLIDX"x%"PRLIDX" <- %"PRLIDX"x%"PRLIDX" * %"PRLIDX"x%"PRLIDX,n,x->traits.ncols,n,v->traits.ncols,v->traits.ncols,x->traits.ncols);
    
    if (n%OUTERUNROLL) {
        n+=(OUTERUNROLL-n%OUTERUNROLL);
        INFO_LOG("Padding large dimension to %d\n",n);
    }


    const double * const restrict vval = (const double *) v->val;
    const double * const restrict wval = (const double *) w->val;
    double * const restrict xval = (double *) x->val;

    const ghost_lidx ldv = v->stride;
    const ghost_lidx ldw = w->stride;
    const ghost_lidx ldx = x->stride;

    const double dalpha = *(double *)alpha;
    const double dbeta = *(double *)beta;
    __m256d betavec;
   
    betavec = _mm256_broadcast_sd(beta);
    ghost_lidx i,m,k,j,s;

    double * restrict wnonpad = NULL;
    ghost_lidx ncols_wnonpad = PAD(w->traits.ncols,4);
    ghost_lidx nrows_wnonpad = PAD(DM_NROWS(w),4);
    ghost_malloc_align((void **)&wnonpad,sizeof(double)*nrows_wnonpad*ncols_wnonpad,32);
    memset(wnonpad,0,sizeof(double)*nrows_wnonpad*ncols_wnonpad);
    for (s=0; s<w->traits.ncols; s++) {
        for (j=0; j<DM_NROWS(w); j++) {
            wnonpad[s*nrows_wnonpad+j] = dalpha*wval[s*ldw+j];
        }
    }

    if ( fabs(dbeta) > DBL_MIN) { // general case: X = b*X + a*V*W
#pragma omp parallel private(k,m)
        {
            #GHOST_UNROLL#__m256d tmp@;#4*OUTERUNROLL
            #GHOST_UNROLL#__m256d vrow@;#OUTERUNROLL
#pragma omp for schedule(runtime)
            for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
                for (k=0; k<OUTCOLS/4; k++) {
                    #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#4*OUTERUNROLL

                    for (m=0; m<INCOLS/4; m++) {
                        #GHOST_UNROLL#vrow@ = _mm256_load_pd(&vval[(i+@)*ldv+m*4]);#OUTERUNROLL
                        #GHOST_UNROLL#tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_load_pd(&wnonpad[(k*4+(@%4))*INCOLS+m*4])));#4*OUTERUNROLL
                    }
                    #GHOST_UNROLL#_mm256_store_pd(&xval[(i+@)*ldx+k*4],_mm256_add_pd(_mm256_mul_pd(betavec,_mm256_load_pd(&xval[(i+@)*ldx+k*4])),_mm256_hadd_pd(_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x20),_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x31)),_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x20),_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x31)))));#OUTERUNROLL

                }
            }
        }
    } else { // common case: X = V*W
        INFO_LOG("Fast case: beta=0");
#pragma omp parallel private(k,m)
        {
            #GHOST_UNROLL#__m256d tmp@;#4*OUTERUNROLL
            #GHOST_UNROLL#__m256d vrow@;#OUTERUNROLL
#pragma omp for schedule(runtime)
            for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
                for (k=0; k<OUTCOLS/4; k++) {
                    #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#4*OUTERUNROLL
                    for (m=0; m<INCOLS/4; m++) {
                        #GHOST_UNROLL#vrow@ = _mm256_load_pd(&vval[(i+@)*ldv+m*4]);#OUTERUNROLL
                        #GHOST_UNROLL#tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_load_pd(&wnonpad[(k*4+(@%4))*INCOLS+m*4])));#4*OUTERUNROLL
                    }
                    #GHOST_UNROLL#_mm256_stream_pd(&xval[(i+@)*ldx+k*4],_mm256_hadd_pd(_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x20),_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x31)),_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x20),_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x31))));#OUTERUNROLL
                }
            }
        }
    }

    free(wnonpad);
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH)
    return ret;
#else
    UNUSED(x);
    UNUSED(v);
    UNUSED(w);
    UNUSED(alpha);
    UNUSED(beta);
    ERROR_LOG("AVX not available!");
    return GHOST_ERR_UNKNOWN;
#endif
}

ghost_error ghost_tsmm__u_avx_d_OUTCOLS_INCOLS_OUTERUNROLL_1_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta)
{
#ifdef GHOST_BUILD_AVX
#if INCOLS%4
    __m256i kmask;
#if INCOLS%4 == 1
    kmask = _mm256_set_epi64x(0,0,0,~0);
#endif
#if INCOLS%4 == 2
    kmask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#if INCOLS%4 == 3
    kmask = _mm256_set_epi64x(0,~0,~0,~0);
#endif
#endif
#if OUTCOLS%4
    __m256i nmask;
#if OUTCOLS%4 == 1
    nmask = _mm256_set_epi64x(0,0,0,~0);
#endif
#if OUTCOLS%4 == 2
    nmask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#if OUTCOLS%4 == 3
    nmask = _mm256_set_epi64x(0,~0,~0,~0);
#endif
#endif
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    ghost_error ret = GHOST_SUCCESS;

    ghost_lidx n = DM_NROWS(v);

    INFO_LOG("In AVX TSMM with two fixed block sizes [OUTCOLS][INCOLS] %"PRLIDX"x%"PRLIDX" <- %"PRLIDX"x%"PRLIDX" * %"PRLIDX"x%"PRLIDX,n,x->traits.ncols,n,v->traits.ncols,v->traits.ncols,x->traits.ncols);
    
    if (n%OUTERUNROLL) {
        n+=(OUTERUNROLL-n%OUTERUNROLL);
        INFO_LOG("Padding large dimension to %d\n",n);
    }


    const double * const restrict vval = (const double *) v->val;
    const double * const restrict wval = (const double *) w->val;
    double * const restrict xval = (double *) x->val;

    const ghost_lidx ldv = v->stride;
    const ghost_lidx ldw = w->stride;
    const ghost_lidx ldx = x->stride;

    const double dalpha = *(double *)alpha;
    __m256d betavec;
   
    betavec = _mm256_broadcast_sd(beta);
    ghost_lidx i,m,k,j,s;

    double * restrict wnonpad = NULL;
    ghost_lidx ncols_wnonpad = w->traits.ncols;
#ifdef KREMAINDER
    ghost_lidx nrows_wnonpad = DM_NROWS(w);
#else
    ghost_lidx nrows_wnonpad = PAD(DM_NROWS(w),4);
#endif
    ghost_malloc_align((void **)&wnonpad,sizeof(double)*nrows_wnonpad*ncols_wnonpad,32);
    memset(wnonpad,0,sizeof(double)*nrows_wnonpad*ncols_wnonpad);
    for (s=0; s<w->traits.ncols; s++) {
        for (j=0; j<DM_NROWS(w); j++) {
            wnonpad[s*nrows_wnonpad+j] = dalpha*wval[s*ldw+j];
        }
    }

#pragma omp parallel private(k,m)
    {
        #GHOST_UNROLL#__m256d tmp@;#4*OUTERUNROLL
        #GHOST_UNROLL#__m256d vrow@;#OUTERUNROLL
#pragma omp for schedule(runtime)
        for (i=0; i<n-(OUTERUNROLL-1); i+=OUTERUNROLL) {
            for (k=0; k<OUTCOLS/4; k++) {
                #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#4*OUTERUNROLL

#ifdef KREMAINDER
#pragma unroll(INCOLS/4)
                for (m=0; m<INCOLS/4; m++) {
                    #GHOST_UNROLL#vrow@ = _mm256_loadu_pd(&vval[(i+@)*ldv+m*4]);#OUTERUNROLL
                    #GHOST_UNROLL#tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_loadu_pd(&wnonpad[(k*4+(@%4))*nrows_wnonpad+m*4])));#4*OUTERUNROLL
                }
#if INCOLS%4
                #GHOST_UNROLL#vrow@ = _mm256_maskload_pd(&vval[(i+@)*ldv+m*4],kmask);#OUTERUNROLL
                #GHOST_UNROLL#tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_maskload_pd(&wnonpad[(k*4+(@%4))*nrows_wnonpad+m*4],kmask)));#4*OUTERUNROLL
#endif
#else
#pragma unroll((INCOLS+3)/4)
                for (m=0; m<(INCOLS+3)/4; m++) {
                    #GHOST_UNROLL#vrow@ = _mm256_loadu_pd(&vval[(i+@)*ldv+m*4]);#OUTERUNROLL
                    #GHOST_UNROLL#tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_loadu_pd(&wnonpad[(k*4+(@%4))*nrows_wnonpad+m*4])));#4*OUTERUNROLL
                }
#endif

                #GHOST_UNROLL#_mm256_storeu_pd(&xval[(i+@)*ldx+k*4],_mm256_add_pd(_mm256_mul_pd(betavec,_mm256_loadu_pd(&xval[(i+@)*ldx+k*4])),_mm256_hadd_pd(_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x20),_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x31)),_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x20),_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x31)))));#OUTERUNROLL

            }
#if OUTCOLS%4
            #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#4*OUTERUNROLL

            for (m=0; m<INCOLS/4; m++) {
                #GHOST_UNROLL#vrow@ = _mm256_loadu_pd(&vval[(i+@)*ldv+m*4]);#OUTERUNROLL
                #GHOST_UNROLL#if(@%4 < OUTCOLS%4) { tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_loadu_pd(&wnonpad[(k*4+(@%4))*nrows_wnonpad+m*4])));}#4*OUTERUNROLL
            }
#if INCOLS%4
            #GHOST_UNROLL#vrow@ = _mm256_maskload_pd(&vval[(i+@)*ldv+m*4],kmask);#OUTERUNROLL
            #GHOST_UNROLL#if(@%4 < OUTCOLS%4) {tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(vrow~@/4~,_mm256_maskload_pd(&wnonpad[(k*4+(@%4))*nrows_wnonpad+m*4],kmask)));}#4*OUTERUNROLL
#endif

            #GHOST_UNROLL#_mm256_maskstore_pd(&xval[(i+@)*ldx+k*4],nmask,_mm256_add_pd(_mm256_mul_pd(betavec,_mm256_maskload_pd(&xval[(i+@)*ldx+k*4],nmask)),_mm256_hadd_pd(_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x20),_mm256_permute2f128_pd(tmp~4*@~,tmp~4*@+2~,0x31)),_mm256_hadd_pd(_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x20),_mm256_permute2f128_pd(tmp~4*@+1~,tmp~4*@+3~,0x31)))));#OUTERUNROLL
#endif
        }
    }

    free(wnonpad);
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH)
    return ret;
#else
    UNUSED(x);
    UNUSED(v);
    UNUSED(w);
    UNUSED(alpha);
    UNUSED(beta);
    ERROR_LOG("AVX not available!");
    return GHOST_ERR_UNKNOWN;
#endif
}

