#include "ghost/config.h"
#undef GHOST_HAVE_MPI
#undef GHOST_HAVE_INSTR_LIKWID
#include "ghost/types.h"
#include "ghost/sell.h"
#include "ghost/complex.h"
#include "ghost/instr.h"
#include "ghost/log.h"
#include "ghost/error.h"
#include "ghost/util.h"
#include "ghost/math.h"

#include <cuComplex.h>
#include <stdio.h>
#include <cuda_runtime.h>
#include <cuda.h>
#include <complex.h>
#include <cub/cub.cuh>

#include "ghost/cu_complex.h"
#include "ghost/cu_sell_kernel.h"

#define MAX_COLS_PER_BLOCK 32
#define SELL_CUDA_THREADSPERBLOCK 1024
#define LOCALDOT_ONTHEFLY

#define CALL(func,dt1,dt2,dt2_b,C,ncols,b1,b2,b3,b4,b5,b6,...){\
    func<dt1,dt2,dt2_b,C,ncols,b1,b2,b3,b4,b5,b6><<<__VA_ARGS__>>>((dt2 *)lhsval,*(int *)(lhs->stride),(dt2 *)rhsval,*(int *)rhs->stride,flags,mat->nrows,SELL(mat)->cumat->rowLen,SELL(mat)->cumat->col,(dt1 *)SELL(mat)->cumat->val,SELL(mat)->cumat->chunkStart,(dt2 *)cu_shift,(dt2)scale,(dt2)beta,(dt2 *)cu_localdot);\
}\

static char boolstr[256];

#define SELL_C_VARIANTS(kernel,dt1,dt2,dt2_b,grid,block,smem)\
${SELL_C_NCOLS_SWITCHCASCADE}\
if (do_fallback) {\
    if (rhs->traits.storage == GHOST_DENSEMAT_ROWMAJOR) {\
        block.x*=block.y;/* ignore special thread mapping for nvecs=32 */\
        block.y= 1;\
        block.x/=2; /* Otherwise, too many registers per thread! */\
        grid.x*=2;\
        ghost_cu_free(cu_localdot);\
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)*rhs->traits.ncols*3*grid.x));\
        WARNING_LOG("Executing row-major fallback kernel with grid %dx%d and block %dx%d smem %zu",grid.x,grid.y,block.x,block.y,smem);\
        SELL_kernel_rm_fallback_tmpl<dt1,dt2,dt2_b><<<grid,block,smem>>>((dt2 *)lhsval,*(int *)(lhs->stride),(dt2 *)rhsval,*(int *)rhs->stride,flags,mat->nrows,SELL(mat)->cumat->rowLen,SELL(mat)->cumat->col,(dt1 *)SELL(mat)->cumat->val,SELL(mat)->cumat->chunkStart,(dt2 *)cu_shift,(dt2)scale,(dt2)beta,(dt2 *)cu_localdot,SELL(mat)->chunkHeight,rhs->traits.ncols,flags&(GHOST_SPMV_AXPBY|GHOST_SPMV_AXPY),flags&GHOST_SPMV_SCALE,flags&(GHOST_SPMV_VSHIFT|GHOST_SPMV_SHIFT),flags&GHOST_SPMV_DOT_ANY);\
    } else {\
        WARNING_LOG("Executing col-major fallback kernel with grid %dx%d and block %dx%d smem %zu",grid.x,grid.y,block.x,block.y,smem);\
        SELL_kernel_cm_fallback_tmpl<dt1,dt2,dt2_b><<<grid,block,smem>>>((dt2 *)lhsval,*(int *)(lhs->stride),(dt2 *)rhsval,*(int *)rhs->stride,flags,mat->nrows,SELL(mat)->cumat->rowLen,SELL(mat)->cumat->col,(dt1 *)SELL(mat)->cumat->val,SELL(mat)->cumat->chunkStart,(dt2 *)cu_shift,(dt2)scale,(dt2)beta,(dt2 *)cu_localdot,SELL(mat)->chunkHeight,rhs->traits.ncols,flags&(GHOST_SPMV_AXPBY|GHOST_SPMV_AXPY),flags&GHOST_SPMV_SCALE,flags&(GHOST_SPMV_VSHIFT|GHOST_SPMV_SHIFT),flags&GHOST_SPMV_DOT_ANY);\
    }\
}

#ifdef LOCALDOT_ONTHEFLY
#define PROCESS_LOCALDOT(dt2,dt2_host)\
    GHOST_INSTR_START("spmv_cuda_dot_reduction")\
int col;\
for (col=0; col<rhs->traits.ncols; col++) {\
    deviceReduce3<dt2>(&cu_localdot[grid.x*col], &cu_localdot[col], rhs->traits.ncols*grid.x, grid.x);\
}\
if (flags & GHOST_SPMV_DOT_YY) {\
GHOST_CALL_RETURN(ghost_cu_download(localdot,cu_localdot,rhs->traits.ncols*sizeof(dt2_host)));\
}\
if (flags & GHOST_SPMV_DOT_XY) {\
GHOST_CALL_RETURN(ghost_cu_download(&localdot[rhs->traits.ncols],&cu_localdot[rhs->traits.ncols*grid.x],rhs->traits.ncols*sizeof(dt2_host)));\
}\
if (flags & GHOST_SPMV_DOT_XX) {\
GHOST_CALL_RETURN(ghost_cu_download(&localdot[2*rhs->traits.ncols],&cu_localdot[2*rhs->traits.ncols*grid.x],rhs->traits.ncols*sizeof(dt2_host)));\
}\
GHOST_INSTR_STOP("spmv_cuda_dot_reduction")
#else
#define PROCESS_LOCALDOT(dt2,dt2_host)\
    GHOST_INSTR_START("spmv_cuda_dot")\
INFO_LOG("Not doing the local dot product on-the-fly!");\
memset(localdot,0,rhs->traits.ncols*3*sizeof(dt2_host));\
lhs->dot(lhs,&localdot[0],lhs);\
lhs->dot(lhs,&localdot[rhs->traits.ncols],rhs);\
lhs->dot(rhs,&localdot[2*rhs->traits.ncols],rhs);\
GHOST_INSTR_STOP("spmv_cuda_dot")
#endif

#define CHOOSE_KERNEL(dt1,dt2,dt2_b,dt2_host) {\
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);\
    cudaGetLastError(); /* Remove previous error */\
    ghost_error_t ret = GHOST_SUCCESS;\
    void *lhsval, *rhsval;\
    ghost_densemat_t *lhscompact, *rhscompact;\
    if (lhs->traits.flags & GHOST_DENSEMAT_SCATTERED) {\
        INFO_LOG("Cloning (and compressing) lhs before operation");\
        GHOST_CALL_RETURN(lhs->clone(lhs,&lhscompact,lhs->traits.nrows,0,lhs->traits.ncols,0));\
    } else {\
        lhscompact = lhs;\
    }\
    if (rhs->traits.flags & GHOST_DENSEMAT_SCATTERED) {\
        INFO_LOG("Cloning (and compressing) rhs before operation");\
        GHOST_CALL_RETURN(rhs->clone(rhs,&rhscompact,rhs->traits.nrows,0,rhs->traits.ncols,0));\
    } else {\
        rhscompact = rhs;\
    }\
    GHOST_CALL_RETURN(ghost_densemat_cu_valptr(lhscompact,&lhsval));\
    GHOST_CALL_RETURN(ghost_densemat_cu_valptr(rhscompact,&rhsval));\
    int cu_device;\
    GHOST_CALL_RETURN(ghost_cu_device(&cu_device));\
    dt2 *cu_localdot = NULL;\
    dt2 *cu_shift = NULL;\
    dt2_host *localdot = NULL;\
    dt2 *shift, scale, beta;\
    dim3 block, grid;\
    GHOST_SPMV_PARSE_ARGS(flags,argp,scale,beta,shift,localdot,dt2_host,dt2);\
    if (flags & GHOST_SPMV_AXPY) {\
        dt2_host hbeta = 1.;\
        beta = *((dt2 *)&hbeta);\
    }\
    strcpy(boolstr,"\0");\
    if (flags & GHOST_SPMV_AXPBY || flags & GHOST_SPMV_AXPY) {\
        strcat(boolstr,"true,");\
    } else {\
        strcat(boolstr,"false,");\
    }\
    if (flags & GHOST_SPMV_SCALE) {\
        strcat(boolstr,"true,");\
    } else {\
        strcat(boolstr,"false,");\
    }\
    if (flags & GHOST_SPMV_VSHIFT || flags & GHOST_SPMV_SHIFT) {\
        strcat(boolstr,"true,");\
    } else {\
        strcat(boolstr,"false,");\
    }\
    if (flags & GHOST_SPMV_DOT_YY) {\
        strcat(boolstr,"true,");\
    } else {\
        strcat(boolstr,"false,");\
    }\
    if (flags & GHOST_SPMV_DOT_XY) {\
        strcat(boolstr,"true,");\
    } else {\
        strcat(boolstr,"false,");\
    }\
    if (flags & GHOST_SPMV_DOT_XX) {\
        strcat(boolstr,"true");\
    } else {\
        strcat(boolstr,"false");\
    }\
    size_t shiftsize = sizeof(dt2)*(flags & (GHOST_SPMV_VSHIFT|GHOST_SPMV_SHIFT)?rhs->traits.ncols:0);\
    GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_shift,shiftsize));\
    if (flags & GHOST_SPMV_SHIFT) {\
        INFO_LOG("scatter shift %zu bytes",shiftsize);\
        ghost_lidx_t c;\
        for (c=0; c<rhs->traits.ncols; c++) {\
            GHOST_CALL_RETURN(ghost_cu_upload(&cu_shift[c],shift,sizeof(dt2)));\
        }\
    } else {\
        GHOST_CALL_RETURN(ghost_cu_upload(cu_shift,shift,shiftsize));\
    }\
    struct cudaDeviceProp prop;\
    CUDA_CALL_RETURN(cudaGetDeviceProperties(&prop,cu_device));\
    GHOST_INSTR_START("spmv_cuda")\
    if (rhs->traits.storage == GHOST_DENSEMAT_COLMAJOR) {\
        block.x = SELL_CUDA_THREADSPERBLOCK/MIN(MAX_COLS_PER_BLOCK,rhs->traits.ncols);\
        block.y = MIN(MAX_COLS_PER_BLOCK,rhs->traits.ncols);\
        grid.x = (int)ceil(mat->nrowsPadded/(double)block.x);\
        grid.y = (int)(ceil(rhs->traits.ncols/(double)MAX_COLS_PER_BLOCK));\
        size_t reqSmem = 0;\
        if (flags & GHOST_SPMV_DOT_ANY) {\
            reqSmem = sizeof(dt2)*32*block.y;\
            WARNING_LOG("Maybe too little shared mem allocated!");\
        }\
        if (prop.sharedMemPerBlock < reqSmem) {\
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");\
        }\
        INFO_LOG("grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);\
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)*rhs->traits.ncols*3*grid.x));\
        SELL_C_VARIANTS(SELL_kernel_CU_tmpl,dt1,dt2,dt2_b,grid,block,reqSmem)\
    } else if (rhs->traits.ncols == 32) {\
        INFO_LOG("Experimental row-major CUDA SELL-SpMMV");\
        block.x = 32;\
        block.y = SELL_CUDA_THREADSPERBLOCK/block.x;\
        grid.x = (int)ceil(mat->nrows/((double)block.y*block.x/rhs->traits.ncols));\
        grid.y = (int)(ceil(rhs->traits.ncols/(double)MAX_COLS_PER_BLOCK));\
        size_t reqSmem = 0;\
        if (prop.sharedMemPerBlock < reqSmem) {\
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");\
        }\
        INFO_LOG("grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);\
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)*rhs->traits.ncols*3*grid.x));\
        SELL_C_VARIANTS(SELL_kernel_CU_rm_tmpl,dt1,dt2,dt2_b,grid,block,reqSmem)\
        /*INFO_LOG("Very experimental row-major CUDA SELL-SpMMV");\
        block.x = 32;\
        block.y = 32;\
        grid.x = (int)ceil(mat->nrows/block.x);\
        grid.y = 1;\
        INFO_LOG("grid %dx%d block %dx%d",grid.x,grid.y,block.x,block.y);\
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)*rhs->traits.ncols*3*grid.x));\
        SELL_C_VARIANTS(SELL_kernel_CU_rm_tmpl_v2,dt1,dt2,grid,block,0)*/\
    } else {\
        INFO_LOG("Experimental row-major CUDA SELL-SpMMV with ncols != 32");\
        /*block.x = MIN(MAX_COLS_PER_BLOCK,rhs->traits.ncols);*/\
        block.x = SELL_CUDA_THREADSPERBLOCK;\
        block.y = 1;\
        grid.x = (int)ceil(mat->nrows/((double)block.y*block.x/(double)rhs->traits.ncols));\
        grid.y = 1;\
        size_t reqSmem = 0;\
        if ((flags & GHOST_SPMV_DOT_ANY) && (SELL_CUDA_THREADSPERBLOCK/rhs->traits.ncols > 32)) {\
            reqSmem = sizeof(dt2)*32;\
        }\
        if (prop.sharedMemPerBlock < reqSmem) {\
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");\
        }\
        INFO_LOG("grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);\
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)*rhs->traits.ncols*3*grid.x));\
        SELL_C_VARIANTS(SELL_kernel_CU_rm_1dblock_tmpl,dt1,dt2,dt2_b,grid,block,reqSmem)\
    }\
    CUDA_CALL_RETURN(cudaGetLastError());\
    if (lhscompact != lhs) {\
        INFO_LOG("Transform lhs back");\
        GHOST_CALL_RETURN(lhs->fromVec(lhs,lhscompact,0,0));\
        lhscompact->destroy(lhscompact);\
    }\
    if (rhscompact != rhs) {\
        INFO_LOG("Transform rhs back");\
        GHOST_CALL_RETURN(rhs->fromVec(rhs,rhscompact,0,0));\
        rhscompact->destroy(rhscompact);\
    }\
    cudaDeviceSynchronize();\
    GHOST_INSTR_STOP("spmv_cuda")\
    if (flags & GHOST_SPMV_DOT_ANY) {\
        PROCESS_LOCALDOT(dt2,dt2_host)\
    }\
    GHOST_CALL_RETURN(ghost_cu_free(cu_localdot));\
    GHOST_CALL_RETURN(ghost_cu_free(cu_shift));\
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);\
    return ret;\
}
# if 0
    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_localdot>  
__global__ void SELL_kernel_CU_rm_tmpl_v2(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot)
{
    int row = blockDim.x*blockIdx.x+threadIdx.x;
    int col = threadIdx.y;

    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
    }

}
    template<>  
__global__ void SELL_kernel_CU_rm_tmpl_v2<double,double,double,32,32,false,false,false,false>(double * const __restrict__ lhs, const int lhs_lda, const double * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const double * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const double * const __restrict__ shift, const double alpha, const double beta, double * const __restrict__ localdot)
{
    int row = blockDim.x*blockIdx.x+threadIdx.x;
    int col = threadIdx.y;

    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/32];
        ric = row%32;
        int j;
        double tmp;

        zero<double>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp += rhs[rhs_lda*mcol[cs + ric + j*32]+col] * val[cs+ric+j*32];
        }

        lhs[lhs_lda*row+col] = tmp;
    }

}
#endif
    
    template<typename m_t, typename v_t, typename v_t_b>  
__global__ void SELL_kernel_rm_fallback_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot)
{
    int tid = blockDim.x*blockIdx.x+threadIdx.x;
    int row = tid/ncols;
    int col = tid%ncols;


    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);
    
        row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(blockDim.x/ncols);
        col = threadIdx.x/(blockDim.x/ncols);
    

        __syncthreads();
        if (row<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        }


        if (blockDim.x/ncols <= 32) {
            dot1 = ghost_partialWarpReduceSum(dot1,blockDim.x/ncols);
            __syncthreads();
            dot2 = ghost_partialWarpReduceSum(dot2,blockDim.x/ncols);
            __syncthreads();
            dot3 = ghost_partialWarpReduceSum(dot3,blockDim.x/ncols);
            __syncthreads();
            
            if (!(threadIdx.x%(blockDim.x/ncols))) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        } else {
            dot1 = ghost_partialBlockReduceSum(dot1,blockDim.x/ncols/warpSize);
            __syncthreads();
            dot2 = ghost_partialBlockReduceSum(dot2,blockDim.x/ncols/warpSize);
            __syncthreads();
            dot3 = ghost_partialBlockReduceSum(dot3,blockDim.x/ncols/warpSize);
            __syncthreads();
                
            
            col = threadIdx.x/(blockDim.x/ncols/warpSize);
            if (!(threadIdx.x%(blockDim.x/ncols/warpSize)) && col<ncols) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        }
    }

#endif
}

    template<typename m_t, typename v_t, typename v_t_b>  
__global__ void SELL_kernel_cm_fallback_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot)
{
    int i = threadIdx.x+blockIdx.x*blockDim.x;
    int col = blockDim.y*blockIdx.y+threadIdx.y;

    if (i<nrows) {
        int cs, tid;
        if (C == blockDim.x) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.x;
        } else {
            cs = chunkstart[i/C];
            tid = threadIdx.x%C;
        }
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[i]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*col+mcol[cs + tid + j*C]], val[cs+tid+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*col+i],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*col+i] = axpy<v_t,float>(scale<v_t>(lhs[lhs_lda*col+i],beta),tmp,1.f);
        } else {
            lhs[lhs_lda*col+i] = tmp;
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        if (i<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*col+i]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*col+i],lhs[lhs_lda*col+i]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*col+i]);
        }

        dot1 = ghost_blockReduceSum(dot1);
        __syncthreads();
        dot2 = ghost_blockReduceSum(dot2);
        __syncthreads();
        dot3 = ghost_blockReduceSum(dot3);
        __syncthreads();

        if (threadIdx.x==0) {
            fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x],dot1);
            localdot[1*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x],dot3);
        }
    }

#endif
}
    
    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx>  
__global__ void SELL_kernel_CU_rm_1dblock_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot)
{
    int tid = blockDim.x*blockIdx.x+threadIdx.x;
    int row = tid/ncols;
    int col = tid%ncols;


    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot_yy && do_dot_xy && do_dot_xx) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);
    
        row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
        col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
    

        __syncthreads();
        if (row<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        }


        if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
            dot1 = ghost_partialWarpReduceSum(dot1,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            dot2 = ghost_partialWarpReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            dot3 = ghost_partialWarpReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        } else {
            dot1 = ghost_partialBlockReduceSum(dot1,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
            dot2 = ghost_partialBlockReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
            dot3 = ghost_partialBlockReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
                
            
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        }
    } else if (do_dot_xy && do_dot_xx) {
        v_t_b dot3;
        v_t dot2;
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);
    
        row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
        col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
    

        __syncthreads();
        if (row<nrows) {
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        }


        if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
            dot2 = ghost_partialWarpReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            dot3 = ghost_partialWarpReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        } else {
            dot2 = ghost_partialBlockReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
            dot3 = ghost_partialBlockReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
                
            
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        }
    } else {
        if (do_dot_yy) {
            v_t_b dot;
            zero<v_t_b>(dot);
        
            row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
        

            __syncthreads();
            if (row<nrows) {
                dot = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            }


            if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
                dot = ghost_partialWarpReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols);
                __syncthreads();
                
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                    fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            } else {
                dot = ghost_partialBlockReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                __syncthreads();
                    
                col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                    fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            }
        }
        if (do_dot_xy) {
            v_t dot;
            zero<v_t>(dot);
        
            row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
        

            __syncthreads();
            if (row<nrows) {
                dot = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            }


            if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
                dot = ghost_partialWarpReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols);
                __syncthreads();
                
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                    localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot;
                }
            } else {
                dot = ghost_partialBlockReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                __syncthreads();
                
                col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                    localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot;
                }
            }
        }
        if (do_dot_xx) {
            v_t_b dot;
            zero<v_t_b>(dot);
        
            row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
        

            __syncthreads();
            if (row<nrows) {
                dot = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            }


            if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
                dot = ghost_partialWarpReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols);
                __syncthreads();
                
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                    fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            } else {
                dot = ghost_partialBlockReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                __syncthreads();
                    
                col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                    fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            }
        }
    }

#endif

}

    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx>  
__global__ void SELL_kernel_CU_rm_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot)
{
    int i = threadIdx.y+blockIdx.x*blockDim.y;
    int col = blockDim.x*blockIdx.y+threadIdx.x;

    if (i<nrows) {
        int cs, tid;
        if (C == blockDim.y) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.y;
        } else {
            cs = chunkstart[i/C];
            tid = threadIdx.y%C;
        }
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[i]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + tid + j*C]+col], val[cs+tid+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*i+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*i+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*i+col],beta);
        } else {
            lhs[lhs_lda*i+col] = tmp;
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot_yy && do_dot_xy && do_dot_xx) {
        v_t_b dot1,dot3; 
        v_t dot2; 
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        i = threadIdx.x+blockIdx.x*blockDim.y;
        col = blockDim.x*blockIdx.y+threadIdx.y;

        __syncthreads();
        if (i<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*i+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*i+col],lhs[lhs_lda*i+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
        }
        dot1 = ghost_warpReduceSum(dot1);
        dot2 = ghost_warpReduceSum(dot2);
        dot3 = ghost_warpReduceSum(dot3);

        if (threadIdx.x==0) {
            fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot1);
            localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot3);
        }
    } else if (do_dot_xy && do_dot_xx) {
        v_t_b dot3; 
        v_t dot2; 
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        i = threadIdx.x+blockIdx.x*blockDim.y;
        col = blockDim.x*blockIdx.y+threadIdx.y;

        __syncthreads();
        if (i<nrows) {
            dot2 = mulConj<v_t>(rhs[rhs_lda*i+col],lhs[lhs_lda*i+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
        }
        dot2 = ghost_warpReduceSum(dot2);
        dot3 = ghost_warpReduceSum(dot3);

        if (threadIdx.x==0) {
            localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot3);
        }
    } else {
        if (do_dot_yy) {
            v_t_b dot; 
            zero<v_t_b>(dot);

            i = threadIdx.x+blockIdx.x*blockDim.y;
            col = blockDim.x*blockIdx.y+threadIdx.y;

            __syncthreads();
            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
            }
            dot = ghost_warpReduceSum(dot);

            if (threadIdx.x==0) {
                fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot);
            }
        }
        if (do_dot_xy) {
            v_t dot; 
            zero<v_t>(dot);

            i = threadIdx.x+blockIdx.x*blockDim.y;
            col = blockDim.x*blockIdx.y+threadIdx.y;

            __syncthreads();
            if (i<nrows) {
                dot = mulConj<v_t>(lhs[lhs_lda*i+col],rhs[rhs_lda*i+col]);
            }
            dot = ghost_warpReduceSum(dot);

            if (threadIdx.x==0) {
                localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = *(v_t *)&dot;
            }
        }
        if (do_dot_xx) {
            v_t_b dot; 
            zero<v_t_b>(dot);

            i = threadIdx.x+blockIdx.x*blockDim.y;
            col = blockDim.x*blockIdx.y+threadIdx.y;

            __syncthreads();
            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
            }
            dot = ghost_warpReduceSum(dot);

            if (threadIdx.x==0) {
                localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = *(v_t *)&dot;
            }
        }
    }

#endif

}

template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx>  
__global__ void SELL_kernel_CU_tmpl(
        v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot)
{
    int i = threadIdx.x+blockIdx.x*blockDim.x;
    int col = blockDim.y*blockIdx.y+threadIdx.y;

    if (i<nrows) {
        int cs, tid;
        if (C == blockDim.x) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.x;
        } else {
            cs = chunkstart[i/C];
            tid = threadIdx.x%C;
        }
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[i]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*col+mcol[cs + tid + j*C]], val[cs+tid+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*col+i],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*col+i] = axpy<v_t,float>(scale<v_t>(lhs[lhs_lda*col+i],beta),tmp,1.f);
        } else {
            lhs[lhs_lda*col+i] = tmp;
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot_yy && do_dot_xy && do_dot_xx) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        if (i<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*col+i]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*col+i],lhs[lhs_lda*col+i]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*col+i]);
        }

        dot1 = ghost_blockReduceSum(dot1);
        __syncthreads();
        dot2 = ghost_blockReduceSum(dot2);
        __syncthreads();
        dot3 = ghost_blockReduceSum(dot3);
        __syncthreads();

        if (threadIdx.x==0) {
            fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x],dot1);
            localdot[1*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x],dot3);
        }
    } else {
        if (do_dot_yy) {
            v_t_b dot;
            zero<v_t_b>(dot);

            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*col+i]);
            }

            dot = ghost_blockReduceSum(dot);
            __syncthreads();

            if (threadIdx.x==0) {
                fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x],dot);
            }
        }
        if (do_dot_xy) {
            v_t dot;
            zero<v_t>(dot);

            if (i<nrows) {
                dot = mulConj<v_t>(rhs[rhs_lda*col+i],lhs[lhs_lda*col+i]);
            }

            dot = ghost_blockReduceSum(dot);
            __syncthreads();

            if (threadIdx.x==0) {
                localdot[1*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x] = dot;
            }
        }
        if (do_dot_xx) {
            v_t_b dot;
            zero<v_t_b>(dot);

            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*col+i]);
            }

            dot = ghost_blockReduceSum(dot);
            __syncthreads();

            if (threadIdx.x==0) {
                fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x],dot);
            }
        }
    }

#endif
}


#if 0
    template<>  
__global__ void SELL_kernel_CU_rm_tmpl<double,double,double,32,32,true,true,true,true>(double * const __restrict__ lhs, const int lhs_lda, const double * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const double * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const double * const __restrict__ shift, const double alpha, const double beta, double * const __restrict__ localdot)
{
    int i = threadIdx.y+blockIdx.x*blockDim.y;
    int col = blockDim.x*blockIdx.y+threadIdx.x;

    if (i<nrows) {
        int cs, tid;
        if (32 == blockDim.y) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.y;
        } else {
            cs = chunkstart[i/32];
            tid = threadIdx.y%32;
        }
        int j;
        double tmp = 0.;

        for (j=0; j<rowlen[i]; j++) {
            tmp += rhs[rhs_lda*mcol[cs + tid + j*32]+col] * val[cs+tid+j*32];
        }

        lhs[lhs_lda*i+col] = lhs[lhs_lda*i+col]*beta + alpha*(tmp-rhs[rhs_lda*i+col]*shift[col]);
    }
#ifdef LOCALDOT_ONTHEFLY 

    double3 dot;
    zero<double>(dot.x);
    zero<double>(dot.y);
    zero<double>(dot.z);

    i = threadIdx.x+blockIdx.x*blockDim.y;
    col = blockDim.x*blockIdx.y+threadIdx.y;

    __syncthreads();
    if (i<nrows) {
        dot.x = lhs[lhs_lda*i+col] * lhs[lhs_lda*i+col];
        dot.y = rhs[rhs_lda*i+col] * lhs[lhs_lda*i+col];
        dot.z = rhs[rhs_lda*i+col] * rhs[rhs_lda*i+col];
    }

    dot = ghost_warpReduceSum(dot);

    if (threadIdx.x==0) {
        localdot[0*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.x;
        localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.y;
        localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.z;
    }
#endif
}
    
    template<>  
__global__ void SELL_kernel_CU_rm_tmpl<double,double,double,1,32,true,true,true,true>(double * const __restrict__ lhs, const int lhs_lda, const double * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const double * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const double * const __restrict__ shift, const double alpha, const double beta, double * const __restrict__ localdot)
{
    int i = threadIdx.y+blockIdx.x*blockDim.y;
    int col = blockDim.x*blockIdx.y+threadIdx.x;

    if (i<nrows) {
        int cs, tid;
        if (1 == blockDim.y) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.y;
        } else {
            cs = chunkstart[i/1];
            tid = threadIdx.y%1;
        }
        int j;
        double tmp = 0.;

        for (j=0; j<rowlen[i]; j++) {
            tmp += rhs[rhs_lda*mcol[cs + tid + j*1]+col] * val[cs+tid+j*1];
        }

        lhs[lhs_lda*i+col] = lhs[lhs_lda*i+col]*beta + alpha*(tmp-rhs[rhs_lda*i+col]*shift[col]);
    }
#ifdef LOCALDOT_ONTHEFLY 

    double3 dot;
    zero<double>(dot.x);
    zero<double>(dot.y);
    zero<double>(dot.z);

    i = threadIdx.x+blockIdx.x*blockDim.y;
    col = blockDim.x*blockIdx.y+threadIdx.y;

    __syncthreads();
    if (i<nrows) {
        dot.x = lhs[lhs_lda*i+col] * lhs[lhs_lda*i+col];
        dot.y = rhs[rhs_lda*i+col] * lhs[lhs_lda*i+col];
        dot.z = rhs[rhs_lda*i+col] * rhs[rhs_lda*i+col];
    }

    dot = ghost_warpReduceSum(dot);

    if (threadIdx.x==0) {
        localdot[0*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.x;
        localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.y;
        localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.z;
    }
#endif
}



   template<>  
   __global__ void SELL_kernel_CU_rm_tmpl<double,double,4,true,true,true,true>(double * const __restrict__ lhs, const int lhs_lda, const double * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const double * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const double * const __restrict__ shift, const double alpha, const double beta, double * const __restrict__ localdot)
   {
   int col = (blockDim.x*blockIdx.y+threadIdx.x)%16;
   int rowinblock = (blockDim.x*blockIdx.y+threadIdx.x)/16;
   int nrowsinblock = blockDim.x*gridDim.y/16;
   int row = nrowsinblock*(blockIdx.x*blockDim.y+threadIdx.y)+rowinblock;

   if (row==0 && col == 0) {
   printf("nrowsinblock %d\n",nrowsinblock);
   }

   if (row<nrows) {
   int cs, rowinchunk;
   cs = chunkstart[row/4];
   rowinchunk = (threadIdx.y*nrowsinblock+rowinblock)%4;
   int j;
   double tmp = 0.;

//if (threadIdx.x == 0) {
printf("thr %d/%d block %d/%d idx %d/%d cs %d rowinchunk %d\n",threadIdx.y,threadIdx.x,blockIdx.x,blockIdx.y,row,col,cs,rowinchunk);
//}

for (j=0; j<rowlen[row]; j++) {
tmp += rhs[rhs_lda*mcol[cs + rowinchunk + j*4]+col] * val[cs + rowinchunk + j*4];
}

lhs[lhs_lda*row+col] = lhs[lhs_lda*row+col]*beta + alpha*(tmp-rhs[rhs_lda*row+col]*shift[col]);
}
#ifdef LOCALDOT_ONTHEFLY 

double3 dot;
zero<double>(dot.x);
zero<double>(dot.y);
zero<double>(dot.z);

row = threadIdx.x+blockIdx.x*blockDim.y;
col = blockDim.x/16*blockIdx.y+threadIdx.y;

__syncthreads();
if (row<nrows) {
dot.x = lhs[lhs_lda*row+col] * lhs[lhs_lda*row+col];
dot.y = rhs[rhs_lda*row+col] * lhs[lhs_lda*row+col];
dot.z = rhs[rhs_lda*row+col] * rhs[rhs_lda*row+col];
}

dot = ghost_warpReduceSum(dot);

if (threadIdx.x==0) {
printf("row %d col %d tid.x %d tid.y %d dot %f\n",row,col,threadIdx.x,threadIdx.y,dot.x);
localdot[0*gridDim.y*blockDim.x/16*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.x;
localdot[1*gridDim.y*blockDim.x/16*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.y;
localdot[2*gridDim.y*blockDim.x/16*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot.z;
}
#endif
}
    template<>
__global__ void SELL_kernel_CU_tmpl<double,double,double,32,32,true,true,true,true>(double * const __restrict__ lhs, const int lhs_lda, const double * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags_t flags, const int nrows, const ghost_lidx_t * const __restrict__ rowlen, const ghost_lidx_t * const __restrict__ mcol, const double * const __restrict__ val, const ghost_lidx_t * const __restrict__ chunkstart, const double * const __restrict__ shift, const double alpha, const double beta, double * const __restrict__ localdot)
{
    int i = threadIdx.x+blockIdx.x*blockDim.x;
    int col = blockDim.y*blockIdx.y+threadIdx.y;

    if (i<nrows) {
        int cs, tid;
        if (32 == blockDim.x) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.x;
        } else {
            cs = chunkstart[i/32];
            tid = threadIdx.x%32;
        }
        int j;
        double tmp = 0.;

        for (j=0; j<rowlen[i]; j++) {
            tmp += rhs[rhs_lda*col+mcol[cs + tid + j*32]] * val[cs+tid+j*32];
        }

        lhs[lhs_lda*col+i] = beta*lhs[lhs_lda*col+i] + alpha*(tmp-rhs[rhs_lda*col+i]*shift[col]);
    }
#ifdef LOCALDOT_ONTHEFLY 
    double3 dot;

    if (i<nrows) {
        dot.x = lhs[lhs_lda*col+i]*lhs[lhs_lda*col+i];
        dot.y = rhs[rhs_lda*col+i]*lhs[lhs_lda*col+i];
        dot.z = rhs[rhs_lda*col+i]*rhs[rhs_lda*col+i];
    } else {
        dot.x = 0.;
        dot.y = 0.;
        dot.z = 0.;
    }

    dot = ghost_blockReduceSum(dot);

    if (threadIdx.x==0) {
        localdot[0*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x] = dot.x;
        localdot[1*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x] = dot.y;
        localdot[2*gridDim.y*blockDim.y*gridDim.x + 3*col + blockIdx.x] = dot.z;
    }
#endif
}
#endif


extern "C" ghost_error_t dd_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{
    CHOOSE_KERNEL(double,double,double,double);
}

extern "C" ghost_error_t ds_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(double,float,float,float);
}

extern "C" ghost_error_t dc_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(double,cuFloatComplex,float,complex float);
}

extern "C" ghost_error_t dz_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(double,cuDoubleComplex,double,complex double);
}

extern "C" ghost_error_t sd_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(float,double,double,double);
}

extern "C" ghost_error_t ss_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(float,float,float,float);
}

extern "C" ghost_error_t sc_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(float,cuFloatComplex,float,complex float);
}

extern "C" ghost_error_t sz_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(float,cuDoubleComplex,double,complex double);
}

extern "C" ghost_error_t zd_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuDoubleComplex,double,double,double);
}

extern "C" ghost_error_t zs_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuDoubleComplex,float,float,float);
}

extern "C" ghost_error_t zc_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuDoubleComplex,cuFloatComplex,float,complex float);
}

extern "C" ghost_error_t zz_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuDoubleComplex,cuDoubleComplex,double,complex double);
}

extern "C" ghost_error_t cd_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuFloatComplex,double,double,double);
}

extern "C" ghost_error_t cs_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuFloatComplex,float,float,float);
}

extern "C" ghost_error_t cc_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuFloatComplex,cuFloatComplex,float,complex float);
}

extern "C" ghost_error_t cz_SELL_kernel_CU(ghost_sparsemat_t *mat, ghost_densemat_t *lhs, ghost_densemat_t *rhs, ghost_spmv_flags_t flags, va_list argp)
{ 
    CHOOSE_KERNEL(cuFloatComplex,cuDoubleComplex,double,complex double);
}

