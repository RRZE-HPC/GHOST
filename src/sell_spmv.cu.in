/*!GHOST_AUTOGEN  */
#include "ghost/config.h"
//#undef GHOST_HAVE_MPI
#include "ghost/types.h"
#include "ghost/sell.h"
#include "ghost/complex.h"
#include "ghost/instr.h"
#include "ghost/log.h"
#include "ghost/error.h"
#include "ghost/util.h"
#include "ghost/math.h"

#include <cuComplex.h>
#include <cuda_runtime.h>
#include <cuda.h>
#include <complex.h>

#include "ghost/cu_complex.h"
#include "ghost/cu_sell_kernel.h"

#define MAX_COLS_PER_BLOCK 16
#define MAX_COLS_PER_BLOCK_COLMAJOR 16
#define SELL_CUDA_THREADSPERBLOCK 512

/*
   comment this out if the dot products should not be computed on the fly but 
   _after_ the SpMV
   */
#define LOCALDOT_ONTHEFLY

/**
 * @brief Holds a string with comma-separated configuration bools
 */
static char boolstr[256];

/**
 * @brief Select and call the CUDA SELL SpMV kernel.
 *
 * This macro gets expanded by cmake at configure step by the 
 * SELL_CU_SPMV_SWITCHCASCADE.
 * There, all auto-generated versions are checked for a match with the given 
 * kernel.
 * If no kernel is found, the variable do_fallback is set to false and a 
 * fallback kernel is being executed.
 *
 * @param kernel The kernel to execute.
 * @param dt1 The matrix data type.
 * @param dt2 The vector data type.
 * @param dt2_b The base data type of the vector data type (e.g., double for
 * cuDoubleComplex).
 * @param grid The CUDA grid.

 * @param block The CUDA block.
 * @param smem Amount of shared mem.
 */
#define SELL_CU_SPMV_SELECTOR(kernel,dt1,dt2,dt2_b,grid,block,smem)\
${SELL_CU_SPMV_SWITCHCASCADE}\
if (do_fallback) {\
    if (rhs->stride > 1 && rhs->traits.storage == GHOST_DENSEMAT_ROWMAJOR) {\
        PERFWARNING_LOG("Executing row-major fallback kernel with grid %dx%d "\
                "and block %dx%d smem %zu",grid.x,grid.y,block.x,block.y,smem);\
        SELL_kernel_rm_fallback_tmpl<dt1,dt2,dt2_b><<<grid,block,smem>>>(\
                (dt2 *)lhsval,(int)(lhs->stride),(dt2 *)rhsval,\
                (int)rhs->stride,traits.flags,mat->nrows,SELL(mat)->cumat->rowLen,\
                SELL(mat)->cumat->col,(dt1 *)SELL(mat)->cumat->val,\
                SELL(mat)->cumat->chunkStart,(dt2 *)cu_shift,(dt2)scale,\
                (dt2)beta,(dt2 *)cu_localdot,mat->traits.C,\
                rhs->traits.ncols,(dt2 *)zval,zstride,sdelta,seta,traits.flags&(GHOST_SPMV_AXPBY|GHOST_SPMV_AXPY),\
                traits.flags&GHOST_SPMV_SCALE,\
                traits.flags&(GHOST_SPMV_VSHIFT|GHOST_SPMV_SHIFT),\
                traits.flags&GHOST_SPMV_DOT,\
                traits.flags&GHOST_SPMV_CHAIN_AXPBY);\
    } else {\
        ghost_cu_free(cu_localdot);\
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)\
                    *rhs->traits.ncols*3*grid.x));\
        PERFWARNING_LOG("Executing col-major fallback kernel with grid %dx%d "\
                "and block %dx%d smem %zu",grid.x,grid.y,block.x,block.y,smem);\
        SELL_kernel_cm_fallback_tmpl<dt1,dt2,dt2_b><<<grid,block,smem>>>(\
                (dt2 *)lhsval,(int)(lhs->stride),(dt2 *)rhsval,\
                (int)rhs->stride,traits.flags,mat->nrows,SELL(mat)->cumat->rowLen,\
                SELL(mat)->cumat->col,(dt1 *)SELL(mat)->cumat->val,\
                SELL(mat)->cumat->chunkStart,(dt2 *)cu_shift,(dt2)scale,\
                (dt2)beta,(dt2 *)cu_localdot,mat->traits.C,\
                rhs->traits.ncols,(dt2 *)zval,zstride,sdelta,seta,traits.flags&(GHOST_SPMV_AXPBY|GHOST_SPMV_AXPY),\
                traits.flags&GHOST_SPMV_SCALE,\
                traits.flags&(GHOST_SPMV_VSHIFT|GHOST_SPMV_SHIFT),\
                traits.flags&GHOST_SPMV_DOT,\
                traits.flags&GHOST_SPMV_CHAIN_AXPBY);\
    }\
}

# if 0
    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_localdot>  
__global__ void SELL_kernel_CU_rm_tmpl_v2(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot)
{
    int row = blockDim.x*blockIdx.x+threadIdx.x;
    int col = threadIdx.y;

    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
    }

}
    template<>  
__global__ void SELL_kernel_CU_rm_tmpl_v2<double,double,double,32,32,false,false,false,false>(double * const __restrict__ lhs, const int lhs_lda, const double * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const double * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const double * const __restrict__ shift, const double alpha, const double beta, double * const __restrict__ localdot)
{
    int row = blockDim.x*blockIdx.x+threadIdx.x;
    int col = threadIdx.y;

    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/32];
        ric = row%32;
        int j;
        double tmp;

        zero<double>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp += rhs[rhs_lda*mcol[cs + ric + j*32]+col] * val[cs+ric+j*32];
        }

        lhs[lhs_lda*row+col] = tmp;
    }

}
#endif
    
    template<typename m_t, typename v_t, typename v_t_b>  
__global__ void SELL_kernel_rm_fallback_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, int C, int ncols, v_t * const __restrict__ z, const int z_lda, const v_t delta, const v_t eta, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot, bool do_chain_axpby)
{
    int row = threadIdx.y+blockIdx.x*blockDim.y;
    int col = threadIdx.x+blockIdx.y*blockDim.x;

    if (row<nrows && col<ncols) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
        if (do_chain_axpby) {
            z[z_lda*row+col] = axpby<v_t>(lhs[lhs_lda*row+col],z[z_lda*row+col],eta,delta);
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    row = threadIdx.x+blockIdx.x*blockDim.x;
    col = threadIdx.y+blockIdx.y*blockDim.y;
    if (col < ncols && (do_dot)) {
        v_t_b dot1, dot3;
        v_t dot2;
  
        __syncthreads();
        if (row<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        } else {
            zero<v_t_b>(dot1);
            zero<v_t>(dot2);
            zero<v_t_b>(dot3);
        }

        dot1 = ghost_partialWarpReduceSum(dot1,16);
        dot2 = ghost_partialWarpReduceSum(dot2,16);
        dot3 = ghost_partialWarpReduceSum(dot3,16);
        
        if (threadIdx.x == 0) {
            fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
            localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
        }
    }

#endif
}

    template<typename m_t, typename v_t, typename v_t_b>  
__global__ void SELL_kernel_cm_fallback_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, int C, int ncols, v_t * const __restrict__ z, const int z_lda, const v_t delta, const v_t eta, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot, bool do_chain_axpby)
{
    int i = threadIdx.x+blockIdx.x*blockDim.x;
    int col = blockDim.y*blockIdx.y+threadIdx.y;

    if (i<nrows && col<ncols) {
        int cs, tid;
        if (C == blockDim.x) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.x;
        } else {
            cs = chunkstart[i/C];
            tid = i%C;
        }
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[i]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*col+mcol[cs + tid + j*C]], val[cs+tid+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*col+i],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*col+i] = axpy<v_t,float>(scale<v_t>(lhs[lhs_lda*col+i],beta),tmp,1.f);
        } else {
            lhs[lhs_lda*col+i] = tmp;
        }
        if (do_chain_axpby) {
            z[z_lda*col+i] = axpby<v_t>(lhs[lhs_lda*col+i],z[z_lda*col+i],eta,delta);
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        if (i<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*col+i]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*col+i],lhs[lhs_lda*col+i]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*col+i]);
        }

        dot1 = ghost_blockReduceSum(dot1);
        __syncthreads();
        dot2 = ghost_blockReduceSum(dot2);
        __syncthreads();
        dot3 = ghost_blockReduceSum(dot3);

        if (threadIdx.x==0) {
            fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
            localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
        }
    }

#endif
}
    
    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx, bool do_chain_axpby>  
__global__ void SELL_kernel_CU_rm_new_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, v_t * const __restrict__ z, const int z_lda, const v_t delta, const v_t eta)
{
    int row = threadIdx.y+blockIdx.x*blockDim.y;
    int col = threadIdx.x+blockIdx.y*blockDim.x;

    if (row<nrows && col<ncols) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
        if (do_chain_axpby) {
            z[z_lda*row+col] = axpby<v_t>(lhs[lhs_lda*row+col],z[z_lda*row+col],eta,delta);
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    row = threadIdx.x+blockIdx.x*blockDim.x;
    col = threadIdx.y+blockIdx.y*blockDim.y;
    if (col < ncols && (do_dot_yy || do_dot_xy || do_dot_xx)) {
        v_t_b dot1, dot3;
        v_t dot2;
  
        __syncthreads();
        if (row<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        } else {
            zero<v_t_b>(dot1);
            zero<v_t>(dot2);
            zero<v_t_b>(dot3);
        }

        dot1 = ghost_partialWarpReduceSum(dot1,16);
        dot2 = ghost_partialWarpReduceSum(dot2,16);
        dot3 = ghost_partialWarpReduceSum(dot3,16);
        
        if (threadIdx.x == 0) {
            fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
            localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
        }
    }

#endif

}
    
    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx, bool do_chain_axpby>  
__global__ void SELL_kernel_CU_rm_1dblock_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, v_t * const __restrict__ z, const int z_lda, const v_t delta, const v_t eta)
{
    int tid = blockDim.x*blockIdx.x+threadIdx.x;
    int row = tid/ncols;
    int col = tid%ncols;


    if (row<nrows) {
        int cs, ric; // chunkstart and row in chunk
        cs = chunkstart[row/C];
        ric = row%C;
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[row]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + ric + j*C]+col], val[cs+ric+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*row+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*row+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*row+col],beta);
        } else {
            lhs[lhs_lda*row+col] = tmp;
        }
        if (do_chain_axpby) {
            z[z_lda*row+col] = axpby<v_t>(lhs[lhs_lda*row+col],z[z_lda*row+col],eta,delta);
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot_yy && do_dot_xy && do_dot_xx) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);
    
        row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
        col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
    

        __syncthreads();
        if (row<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        }


        if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
            dot1 = ghost_partialWarpReduceSum(dot1,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            dot2 = ghost_partialWarpReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            dot3 = ghost_partialWarpReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        } else {
            dot1 = ghost_partialBlockReduceSum(dot1,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
            dot2 = ghost_partialBlockReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
            dot3 = ghost_partialBlockReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
                
            
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        }
    } else if (do_dot_xy && do_dot_xx) {
        v_t_b dot3;
        v_t dot2;
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);
    
        row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
        col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
    

        __syncthreads();
        if (row<nrows) {
            dot2 = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*row+col]);
        }


        if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
            dot2 = ghost_partialWarpReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            dot3 = ghost_partialWarpReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols);
            __syncthreads();
            
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        } else {
            dot2 = ghost_partialBlockReduceSum(dot2,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
            dot3 = ghost_partialBlockReduceSum(dot3,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            __syncthreads();
                
            
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
            if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
            }
        }
    } else {
        if (do_dot_yy) {
            v_t_b dot;
            zero<v_t_b>(dot);
        
            row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
        

            __syncthreads();
            if (row<nrows) {
                dot = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            }


            if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
                dot = ghost_partialWarpReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols);
                __syncthreads();
                
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                    fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            } else {
                dot = ghost_partialBlockReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                __syncthreads();
                    
                col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                    fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            }
        }
        if (do_dot_xy) {
            v_t dot;
            zero<v_t>(dot);
        
            row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
        

            __syncthreads();
            if (row<nrows) {
                dot = mulConj<v_t>(rhs[rhs_lda*row+col],lhs[lhs_lda*row+col]);
            }


            if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
                dot = ghost_partialWarpReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols);
                __syncthreads();
                
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                    localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot;
                }
            } else {
                dot = ghost_partialBlockReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                __syncthreads();
                
                col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                    localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot;
                }
            }
        }
        if (do_dot_xx) {
            v_t_b dot;
            zero<v_t_b>(dot);
        
            row = blockDim.x/ncols*blockIdx.x+threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols);
            col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols);
        

            __syncthreads();
            if (row<nrows) {
                dot = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*row+col]);
            }


            if (SELL_CUDA_THREADSPERBLOCK/ncols <= 32) {
                dot = ghost_partialWarpReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols);
                __syncthreads();
                
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols))) {
                    fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            } else {
                dot = ghost_partialBlockReduceSum(dot,SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                __syncthreads();
                    
                col = threadIdx.x/(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize);
                if (!(threadIdx.x%(SELL_CUDA_THREADSPERBLOCK/ncols/warpSize)) && col<ncols) {
                    fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
                }
            }
        }
    }

#endif

}

    template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx, bool do_chain_axpby>  
__global__ void SELL_kernel_CU_rm_tmpl(v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, v_t * const __restrict__ z, const int z_lda, const v_t delta, const v_t eta)
{
    int i = threadIdx.y+blockIdx.x*blockDim.y;
    int col = blockDim.x*blockIdx.y+threadIdx.x;

    if (i<nrows) {
        int cs, tid;
        if (C == blockDim.y) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.y;
        } else {
            cs = chunkstart[i/C];
            tid = threadIdx.y%C;
        }
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[i]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*mcol[cs + tid + j*C]+col], val[cs+tid+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*i+col],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*i+col] = axpy<v_t,v_t>(tmp,lhs[lhs_lda*i+col],beta);
        } else {
            lhs[lhs_lda*i+col] = tmp;
        }
        if (do_chain_axpby) {
            z[z_lda*i+col] = axpby<v_t>(lhs[lhs_lda*i+col],z[z_lda*i+col],eta,delta);
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot_yy && do_dot_xy && do_dot_xx) {
        v_t_b dot1,dot3; 
        v_t dot2; 
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        i = threadIdx.x+blockIdx.x*blockDim.y;
        col = blockDim.x*blockIdx.y+threadIdx.y;

        __syncthreads();
        if (i<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*i+col]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*i+col],lhs[lhs_lda*i+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
        }
        dot1 = ghost_blockReduceSum(dot1);
        __syncthreads();
        dot2 = ghost_blockReduceSum(dot2);
        __syncthreads();
        dot3 = ghost_blockReduceSum(dot3);
        __syncthreads();

        if (threadIdx.x==0) {
            fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot1);
            localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot3);
        }
    } else if (do_dot_xy && do_dot_xx) {
        v_t_b dot3; 
        v_t dot2; 
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        i = threadIdx.x+blockIdx.x*blockDim.y;
        col = blockDim.x*blockIdx.y+threadIdx.y;

        __syncthreads();
        if (i<nrows) {
            dot2 = mulConj<v_t>(rhs[rhs_lda*i+col],lhs[lhs_lda*i+col]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
        }
        dot2 = ghost_warpReduceSum(dot2);
        dot3 = ghost_warpReduceSum(dot3);

        if (threadIdx.x==0) {
            localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot3);
        }
    } else {
        if (do_dot_yy) {
            v_t_b dot; 
            zero<v_t_b>(dot);

            i = threadIdx.x+blockIdx.x*blockDim.y;
            col = blockDim.x*blockIdx.y+threadIdx.y;

            __syncthreads();
            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
            }
            dot = ghost_warpReduceSum(dot);

            if (threadIdx.x==0) {
                fromReal<v_t,v_t_b>(localdot[0*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x], dot);
            }
        }
        if (do_dot_xy) {
            v_t dot; 
            zero<v_t>(dot);

            i = threadIdx.x+blockIdx.x*blockDim.y;
            col = blockDim.x*blockIdx.y+threadIdx.y;

            __syncthreads();
            if (i<nrows) {
                dot = mulConj<v_t>(lhs[lhs_lda*i+col],rhs[rhs_lda*i+col]);
            }
            dot = ghost_warpReduceSum(dot);

            if (threadIdx.x==0) {
                localdot[1*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = *(v_t *)&dot;
            }
        }
        if (do_dot_xx) {
            v_t_b dot; 
            zero<v_t_b>(dot);

            i = threadIdx.x+blockIdx.x*blockDim.y;
            col = blockDim.x*blockIdx.y+threadIdx.y;

            __syncthreads();
            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*i+col]);
            }
            dot = ghost_warpReduceSum(dot);

            if (threadIdx.x==0) {
                localdot[2*gridDim.y*blockDim.x*gridDim.x + gridDim.x*threadIdx.y + blockIdx.x] = *(v_t *)&dot;
            }
        }
    }

#endif

}

template<typename m_t, typename v_t, typename v_t_b, int C, int ncols, bool do_axpby, bool do_scale, bool do_vshift, bool do_dot_yy, bool do_dot_xy, bool do_dot_xx, bool do_chain_axpby>  
__global__ void SELL_kernel_CU_tmpl(
        v_t * const __restrict__ lhs, const int lhs_lda, const v_t * const __restrict__ rhs, const int rhs_lda, const ghost_spmv_flags flags, const int nrows, const ghost_lidx * const __restrict__ rowlen, const ghost_lidx * const __restrict__ mcol, const m_t * const __restrict__ val, const ghost_lidx * const __restrict__ chunkstart, const v_t * const __restrict__ shift, const v_t alpha, const v_t beta, v_t * const __restrict__ localdot, v_t * const __restrict__ z, const int z_lda, const v_t delta, const v_t eta)
{
    int i = threadIdx.x+blockIdx.x*blockDim.x;
    int col = blockDim.y*blockIdx.y+threadIdx.y;

    if (i<nrows && col<ncols) {
        int cs, tid;
        if (C == blockDim.x) {
            cs = chunkstart[blockIdx.x];
            tid = threadIdx.x;
        } else {
            cs = chunkstart[i/C];
            tid = threadIdx.x%C;
        }
        int j;
        v_t tmp;

        zero<v_t>(tmp);

        for (j=0; j<rowlen[i]; j++) {
            tmp = axpy<v_t,m_t>(tmp, rhs[rhs_lda*col+mcol[cs + tid + j*C]], val[cs+tid+j*C]);
        }

        if (do_vshift) {
            tmp = axpy<v_t,v_t>(tmp,rhs[rhs_lda*col+i],scale2<v_t,float>(shift[col],-1.f));
        }
        if (do_scale) {
            tmp = scale<v_t>(alpha,tmp);
        }
        if (do_axpby) {
            lhs[lhs_lda*col+i] = axpy<v_t,float>(scale<v_t>(lhs[lhs_lda*col+i],beta),tmp,1.f);
        } else {
            lhs[lhs_lda*col+i] = tmp;
        }
        if (do_chain_axpby) {
            z[z_lda*col+i] = axpby<v_t>(lhs[lhs_lda*col+i],z[z_lda*col+i],eta,delta);
        }
    }
#ifdef LOCALDOT_ONTHEFLY 
    if (do_dot_yy && do_dot_xy && do_dot_xx) {
        v_t_b dot1, dot3;
        v_t dot2;
        zero<v_t_b>(dot1);
        zero<v_t>(dot2);
        zero<v_t_b>(dot3);

        if (i<nrows) {
            dot1 = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*col+i]);
            dot2 = mulConj<v_t>(rhs[rhs_lda*col+i],lhs[lhs_lda*col+i]);
            dot3 = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*col+i]);
        }

        dot1 = ghost_blockReduceSum(dot1);
        __syncthreads();
        dot2 = ghost_blockReduceSum(dot2);
        __syncthreads();
        dot3 = ghost_blockReduceSum(dot3);

        if (threadIdx.x==0) {
            fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot1);
            localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot2;
            fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot3);
        }
    } else {
        if (do_dot_yy) {
            v_t_b dot;
            zero<v_t_b>(dot);

            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(lhs[lhs_lda*col+i]);
            }

            dot = ghost_blockReduceSum(dot);
            __syncthreads();

            if (threadIdx.x==0) {
                fromReal<v_t,v_t_b>(localdot[0*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
            }
        }
        if (do_dot_xy) {
            v_t dot;
            zero<v_t>(dot);

            if (i<nrows) {
                dot = mulConj<v_t>(rhs[rhs_lda*col+i],lhs[lhs_lda*col+i]);
            }

            dot = ghost_blockReduceSum(dot);
            __syncthreads();

            if (threadIdx.x==0) {
                localdot[1*ncols*gridDim.x + gridDim.x*col + blockIdx.x] = dot;
            }
        }
        if (do_dot_xx) {
            v_t_b dot;
            zero<v_t_b>(dot);

            if (i<nrows) {
                dot = mulConjSame<v_t,v_t_b>(rhs[rhs_lda*col+i]);
            }

            dot = ghost_blockReduceSum(dot);
            __syncthreads();

            if (threadIdx.x==0) {
                fromReal<v_t,v_t_b>(localdot[2*ncols*gridDim.x + gridDim.x*col + blockIdx.x],dot);
            }
        }
    }

#endif
}

template<typename m_dt, typename v_dt_host, typename v_dt_device, typename v_dt_base>
static ghost_error ghost_cu_sell_spmv_tmpl(ghost_sparsemat *mat, ghost_densemat *lhs, ghost_densemat *rhs, ghost_spmv_traits traits)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    cudaGetLastError(); /* Remove previous error */
    ghost_error ret = GHOST_SUCCESS;
    void *lhsval, *rhsval, *zval;
    int zstride;
    ghost_densemat *lhscompact, *rhscompact, *zcompact;
    if (lhs->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        PERFWARNING_LOG("Cloning (and compressing) lhs before operation");
        GHOST_CALL_RETURN(lhs->clone(lhs,&lhscompact,lhs->traits.nrows,0,lhs->traits.ncols,0));
    } else {
        lhscompact = lhs;
    }
    if (rhs->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        PERFWARNING_LOG("Cloning (and compressing) rhs before operation");
        GHOST_CALL_RETURN(rhs->clone(rhs,&rhscompact,rhs->traits.nrows,0,rhs->traits.ncols,0));
    } else {
        rhscompact = rhs;
    }
    GHOST_CALL_RETURN(ghost_densemat_cu_valptr(lhscompact,&lhsval));
    GHOST_CALL_RETURN(ghost_densemat_cu_valptr(rhscompact,&rhsval));
    int cu_device;
    GHOST_CALL_RETURN(ghost_cu_device(&cu_device));
    v_dt_device *cu_localdot = NULL;
    v_dt_device *cu_shift = NULL;
    v_dt_host *localdot = NULL;
    v_dt_device *shift, scale, beta, sdelta,seta;
    ghost_densemat *z = NULL;
    dim3 block, grid;
    GHOST_SPMV_PARSE_TRAITS(traits,scale,beta,shift,localdot,z,sdelta,seta,v_dt_host,v_dt_device);
    if (z && (z->traits.flags & GHOST_DENSEMAT_SCATTERED)) {
        PERFWARNING_LOG("Cloning (and compressing) z before operation");
        GHOST_CALL_RETURN(z->clone(z,&zcompact,z->traits.nrows,0,z->traits.ncols,0));
    } else {
        zcompact = z;
    }
    if (z) {
        zstride = z->stride;
        GHOST_CALL_RETURN(ghost_densemat_cu_valptr(zcompact,&zval));
    } else {
        zstride = 0;
    }
    if (traits.flags & GHOST_SPMV_AXPY) {
        v_dt_host hbeta = 1.;
        beta = *((v_dt_device *)&hbeta);
    }
    strcpy(boolstr,"\0");
    if (traits.flags & GHOST_SPMV_AXPBY || traits.flags & GHOST_SPMV_AXPY) {
        strcat(boolstr,"true,");
    } else {
        strcat(boolstr,"false,");
    }
    if (traits.flags & GHOST_SPMV_SCALE) {
        strcat(boolstr,"true,");
    } else {
        strcat(boolstr,"false,");
    }
    if (traits.flags & GHOST_SPMV_VSHIFT || traits.flags & GHOST_SPMV_SHIFT) {
        strcat(boolstr,"true,");
    } else {
        strcat(boolstr,"false,");
    }
    if (traits.flags & GHOST_SPMV_DOT_YY) {
        strcat(boolstr,"true,");
    } else {
        strcat(boolstr,"false,");
    }
    if (traits.flags & GHOST_SPMV_DOT_XY) {
        strcat(boolstr,"true,");
    } else {
        strcat(boolstr,"false,");
    }
    if (traits.flags & GHOST_SPMV_DOT_XX) {
        strcat(boolstr,"true,");
    } else {
        strcat(boolstr,"false,");
    }
    if (traits.flags & GHOST_SPMV_CHAIN_AXPBY) {
        strcat(boolstr,"true");
    } else {
        strcat(boolstr,"false");
    }
    if (traits.flags & (GHOST_SPMV_SHIFT|GHOST_SPMV_VSHIFT)) {
        size_t shiftsize = sizeof(v_dt_device)*(traits.flags & (GHOST_SPMV_VSHIFT|GHOST_SPMV_SHIFT)?rhs->traits.ncols:0);
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_shift,shiftsize));
        if (traits.flags & GHOST_SPMV_SHIFT) {
            ghost_lidx c;
            for (c=0; c<rhs->traits.ncols; c++) {
                GHOST_CALL_RETURN(ghost_cu_upload(&cu_shift[c],shift,sizeof(v_dt_device)));
            }
        } else {
            GHOST_CALL_RETURN(ghost_cu_upload(cu_shift,shift,shiftsize));
        }
    }

    ghost_cu_deviceprop prop;
    GHOST_CALL_RETURN(ghost_cu_deviceprop_get(&prop));
        
       
    GHOST_INSTR_START("spmv_cuda")
    if (rhs->traits.storage == GHOST_DENSEMAT_COLMAJOR || (rhs->stride == 1)) {
        block.x = PAD(CEILDIV(SELL_CUDA_THREADSPERBLOCK,MIN(MAX_COLS_PER_BLOCK_COLMAJOR,rhs->traits.ncols)),32);
        block.y = MIN(MAX_COLS_PER_BLOCK_COLMAJOR,rhs->traits.ncols);
        while(block.x*block.y > SELL_CUDA_THREADSPERBLOCK && block.x > 32) {
            block.x -= 32;
        }

        grid.x = CEILDIV(mat->nrowsPadded,block.x);
        grid.y = CEILDIV(rhs->traits.ncols,MAX_COLS_PER_BLOCK_COLMAJOR);
        if (traits.flags & GHOST_SPMV_DOT) {
            GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(v_dt_device)*rhs->traits.ncols*3*grid.x));
        }
        size_t reqSmem = 0;
        if (traits.flags & GHOST_SPMV_DOT) {
            reqSmem = sizeof(v_dt_device)*32*block.y;
        }
        if (prop.sharedMemPerBlock < reqSmem) {
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");
        }
        DEBUG_LOG(1,"grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);
        SELL_CU_SPMV_SELECTOR(SELL_kernel_CU_tmpl,m_dt,v_dt_device,v_dt_base,grid,block,reqSmem)
    /*} else if (rhs->traits.ncols == 32) {
        DEBUG_LOG(1,"Experimental row-major CUDA SELL-SpMMV");
        block.x = 32;
        block.y = 16;
        grid.x = (int)ceil(mat->nrows/((double)block.y*block.x/rhs->traits.ncols));
        grid.y = (int)(ceil(rhs->traits.ncols/(double)MAX_COLS_PER_BLOCK));
        if (traits.flags & GHOST_SPMV_DOT) {
            GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(v_dt_device)*rhs->traits.ncols*3*grid.x));
        }
        size_t reqSmem = 0;
        if (prop.sharedMemPerBlock < reqSmem) {
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");
        }
        ERROR_LOG("grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);
        SELL_CU_SPMV_SELECTOR(SELL_kernel_CU_rm_tmpl,m_dt,v_dt_device,v_dt_base,grid,block,reqSmem)*/
        /*INFO_LOG("Very experimental row-major CUDA SELL-SpMMV");
        block.x = 32;
        block.y = 32;
        grid.x = (int)ceil(mat->nrows/block.x);
        grid.y = 1;
        INFO_LOG("grid %dx%d block %dx%d",grid.x,grid.y,block.x,block.y);
        GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(dt2)*rhs->traits.ncols*3*grid.x));
        SELL_CU_SPMV_SELECTOR(SELL_kernel_CU_rm_tmpl_v2,dt1,dt2,grid,block,0)*/
    } else {
        block.x = 16;
        block.y = 16;
        grid.y = CEILDIV(rhs->traits.ncols,block.x);
        grid.x = CEILDIV(mat->nrows,block.y);
        if (traits.flags & GHOST_SPMV_DOT) {
            GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(v_dt_device)*rhs->traits.ncols*3*grid.x));
        }
        size_t reqSmem = 0;
        /*if ((traits.flags & GHOST_SPMV_DOT) && (SELL_CUDA_THREADSPERBLOCK/rhs->traits.ncols > 32)) {
            reqSmem = sizeof(v_dt_device)*32;
        }
        if (prop.sharedMemPerBlock < reqSmem) {
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");
        }*/
        DEBUG_LOG(1,"grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);
        SELL_CU_SPMV_SELECTOR(SELL_kernel_CU_rm_new_tmpl,m_dt,v_dt_device,v_dt_base,grid,block,reqSmem)
#if 0
        DEBUG_LOG(1,"Experimental row-major CUDA SELL-SpMMV with ncols != 32");
        /*block.x = MIN(MAX_COLS_PER_BLOCK,rhs->traits.ncols);*/
        block.x = SELL_CUDA_THREADSPERBLOCK;
        block.y = 1;
        grid.x = (int)ceil(mat->nrows/((double)block.y*block.x/(double)rhs->traits.ncols));
        grid.y = 1;
        if (traits.flags & GHOST_SPMV_DOT) {
            GHOST_CALL_RETURN(ghost_cu_malloc((void **)&cu_localdot,sizeof(v_dt_device)*rhs->traits.ncols*3*grid.x));
        }
        size_t reqSmem = 0;
        if ((traits.flags & GHOST_SPMV_DOT) && (SELL_CUDA_THREADSPERBLOCK/rhs->traits.ncols > 32)) {
            reqSmem = sizeof(v_dt_device)*32;
        }
        if (prop.sharedMemPerBlock < reqSmem) {
            WARNING_LOG("Not enough shared memory available! CUDA kernel will not execute!");
        }
        ERROR_LOG("grid %dx%d block %dx%d shmem %zu",grid.x,grid.y,block.x,block.y,reqSmem);
        SELL_CU_SPMV_SELECTOR(SELL_kernel_CU_rm_1dblock_tmpl,m_dt,v_dt_device,v_dt_base,grid,block,reqSmem)
#endif
    }
    CUDA_CALL_RETURN(cudaGetLastError());
    if (lhscompact != lhs) {
        DEBUG_LOG(1,"Transform lhs back");
        GHOST_CALL_RETURN(lhs->fromVec(lhs,lhscompact,0,0));
        lhscompact->destroy(lhscompact);
    }
    if (rhscompact != rhs) {
        DEBUG_LOG(1,"Transform rhs back");
        GHOST_CALL_RETURN(rhs->fromVec(rhs,rhscompact,0,0));
        rhscompact->destroy(rhscompact);
    }
    cudaDeviceSynchronize();
    GHOST_INSTR_STOP("spmv_cuda")
    if (traits.flags & GHOST_SPMV_DOT) {
#ifdef LOCALDOT_ONTHEFLY
        GHOST_INSTR_START("spmv_cuda_dot_reduction")
        ghost_lidx col;
        for (col=0; col<rhs->traits.ncols; col++) {
            deviceReduce3<v_dt_device>(&cu_localdot[grid.x*col], &cu_localdot[col], rhs->traits.ncols*grid.x, grid.x);
        }
        if (traits.flags & GHOST_SPMV_DOT_YY) {
            GHOST_CALL_RETURN(ghost_cu_download(localdot,cu_localdot,rhs->traits.ncols*sizeof(v_dt_host)));
        }
        if (traits.flags & GHOST_SPMV_DOT_XY) {
            GHOST_CALL_RETURN(ghost_cu_download(&localdot[rhs->traits.ncols],&cu_localdot[rhs->traits.ncols*grid.x],rhs->traits.ncols*sizeof(v_dt_host)));
        }
        if (traits.flags & GHOST_SPMV_DOT_XX) {
            GHOST_CALL_RETURN(ghost_cu_download(&localdot[2*rhs->traits.ncols],&cu_localdot[2*rhs->traits.ncols*grid.x],rhs->traits.ncols*sizeof(v_dt_host)));
        }
        GHOST_INSTR_STOP("spmv_cuda_dot_reduction")
#else
            GHOST_INSTR_START("spmv_cuda_dot")
        PERFWARNING_LOG("Not doing the local dot product on-the-fly!");
        memset(localdot,0,rhs->traits.ncols*3*sizeof(v_dt_host));
        lhs->localdot_vanilla(lhs,&localdot[0],lhs);
        lhs->localdot_vanilla(lhs,&localdot[rhs->traits.ncols],rhs);
        rhs->localdot_vanilla(rhs,&localdot[2*rhs->traits.ncols],rhs);
        GHOST_INSTR_STOP("spmv_cuda_dot")
#endif
    }
    //if (traits.flags & GHOST_SPMV_CHAIN_AXPBY) {
    //    PERFWARNING_LOG("AXPBY will not be done on-the-fly!");
    //    z->axpby(z,lhs,&seta,&sdelta);
   // }
    if (traits.flags & GHOST_SPMV_DOT) {
        GHOST_CALL_RETURN(ghost_cu_free(cu_localdot));
    }
    if (traits.flags & (GHOST_SPMV_SHIFT|GHOST_SPMV_VSHIFT)) {
        GHOST_CALL_RETURN(ghost_cu_free(cu_shift));
    }
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);
    return ret;
}

extern "C" ghost_error ghost_cu_sell_spmv_selector(ghost_densemat *lhs, ghost_sparsemat *mat, ghost_densemat *rhs, ghost_spmv_traits traits)
{
    ghost_error ret;

    if (mat->traits.C == 1) {
        INFO_LOG("Calling CUBLAS CRS SpMV for SELL-1 matrix");
        return ghost_cu_sell1_spmv_selector(lhs,mat,rhs,traits);
    } 
    SELECT_TMPL_4DATATYPES(mat->traits.datatype,rhs->traits.datatype,ghost_complex,ret,ghost_cu_sell_spmv_tmpl,mat,lhs,rhs,traits);

    return ret;
}

