#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/sell.h"
#include "ghost/util.h"
#include "ghost/instr.h"
#include "ghost/machine.h"
#include "ghost/omp.h"
#include "ghost/math.h"
#include "ghost/sell_spmv_avx_gen.h"
#include <immintrin.h>


#define complex_mul(a,b) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(b,b,0),a),_mm256_mul_pd(_mm256_shuffle_pd(b,b,0xF),_mm256_shuffle_pd(a,a,5)))
#define complex_mul_conj1(a,b) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(b,b,0),a),_mm256_mul_pd(_mm256_mul_pd(_mm256_shuffle_pd(b,b,0xF),_mm256_set1_pd(-1.)),_mm256_shuffle_pd(a,a,5)))

#GHOST_FUNC_BEGIN#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_d_d_cm_CHUNKHEIGHT_x(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX) && CHUNKHEIGHT>=4
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    INFO_LOG("In CM kernel with block size %d",res->traits.ncols);
    ghost_lidx_t i;
    double *lval = NULL, *rval = NULL;
    double *mval = (double *)SELL(mat)->val;
    double *local_dot_product = NULL;
    double *partsums = NULL;
    
    double sscale = 1., sbeta = 1.;
    double *sshift = NULL;
    __m256d shift, scale, beta;
    double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,double,double);
    scale = _mm256_broadcast_sd(&sscale);
    beta = _mm256_broadcast_sd(&sbeta);

    int nthreads = 1;
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(double))); 
        for (i=0; i<(3*invec->traits.ncols+padding)*nthreads; i++) {
            partsums[i] = 0.;
        }
    }

#pragma omp parallel shared(partsums)
    {
        ghost_lidx_t j,c,v;
        ghost_lidx_t offs;
        __m256d val;
        __m256d rhs;
        __m128d rhstmp;
        #GHOST_UNROLL#__m256d tmp@;#CHUNKHEIGHT/4
        int tid = ghost_omp_threadnum();
        __m256d dot1[invec->traits.ncols],dot2[invec->traits.ncols],dot3[invec->traits.ncols];
        for (v=0; v<invec->traits.ncols; v++) {
            dot1[v] = _mm256_setzero_pd();
            dot2[v] = _mm256_setzero_pd();
            dot3[v] = _mm256_setzero_pd();
        }

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrowsPadded/CHUNKHEIGHT; c++) 
        { // loop over chunks
            for (v=0; v<invec->traits.ncols; v++)
            {

                #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#CHUNKHEIGHT/4
                lval = (double *)res->val+v*res->stride;
                rval = (double *)invec->val+v*invec->stride;
                offs = SELL(mat)->chunkStart[c];

                for (j=0; j<SELL(mat)->chunkLenPadded[c]; j++) 
                { // loop inside chunk

                    #GHOST_UNROLL#val    = _mm256_load_pd(&mval[offs]);rhstmp = _mm_loadl_pd(rhstmp,&rval[(SELL(mat)->col[offs++])]);rhstmp = _mm_loadh_pd(rhstmp,&rval[(SELL(mat)->col[offs++])]);rhs    = _mm256_insertf128_pd(rhs,rhstmp,0);rhstmp = _mm_loadl_pd(rhstmp,&rval[(SELL(mat)->col[offs++])]);rhstmp = _mm_loadh_pd(rhstmp,&rval[(SELL(mat)->col[offs++])]);rhs    = _mm256_insertf128_pd(rhs,rhstmp,1);tmp@    = _mm256_add_pd(tmp@,_mm256_mul_pd(val,rhs));#CHUNKHEIGHT/4
                }

                if (spmvmOptions & (GHOST_SPMV_SHIFT | GHOST_SPMV_VSHIFT)) {
                    if (spmvmOptions & GHOST_SPMV_SHIFT) {
                        shift = _mm256_broadcast_sd(&sshift[0]);
                    } else {
                        shift = _mm256_broadcast_sd(&sshift[v]);
                    }
                    #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,_mm256_mul_pd(shift,_mm256_load_pd(&rval[c*CHUNKHEIGHT+4*@])));#CHUNKHEIGHT/4
                }
                if (spmvmOptions & GHOST_SPMV_SCALE) {
                    #GHOST_UNROLL#tmp@ = _mm256_mul_pd(scale,tmp@);#CHUNKHEIGHT/4
                }
                if (spmvmOptions & GHOST_SPMV_AXPY) {
                    #GHOST_UNROLL#_mm256_store_pd(&lval[c*CHUNKHEIGHT+4*@],_mm256_add_pd(tmp@,_mm256_load_pd(&lval[c*CHUNKHEIGHT+4*@])));#CHUNKHEIGHT/4
                } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                    #GHOST_UNROLL#_mm256_store_pd(&lval[c*CHUNKHEIGHT+4*@],_mm256_add_pd(tmp@,_mm256_mul_pd(beta,_mm256_load_pd(&lval[c*CHUNKHEIGHT+4*@]))));#CHUNKHEIGHT/4
                } else {
                    #GHOST_UNROLL#_mm256_stream_pd(&lval[c*CHUNKHEIGHT+4*@],tmp@);#CHUNKHEIGHT/4
                }

                if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot1[v] = _mm256_add_pd(dot1[v],_mm256_mul_pd(_mm256_load_pd(&lval[c*CHUNKHEIGHT+4*@]),_mm256_load_pd(&lval[c*CHUNKHEIGHT+4*@])));#CHUNKHEIGHT/4
                        #GHOST_UNROLL#dot2[v] = _mm256_add_pd(dot2[v],_mm256_mul_pd(_mm256_load_pd(&rval[c*CHUNKHEIGHT+4*@]),_mm256_load_pd(&lval[c*CHUNKHEIGHT+4*@])));#CHUNKHEIGHT/4
                        #GHOST_UNROLL#dot3[v] = _mm256_add_pd(dot3[v],_mm256_mul_pd(_mm256_load_pd(&rval[c*CHUNKHEIGHT+4*@]),_mm256_load_pd(&rval[c*CHUNKHEIGHT+4*@])));#CHUNKHEIGHT/4
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*invec->traits.ncols)*tid)+3*v+0] += lval[c*CHUNKHEIGHT+rem]*lval[c*CHUNKHEIGHT+rem];
                            partsums[((padding+3*invec->traits.ncols)*tid)+3*v+1] += lval[c*CHUNKHEIGHT+rem]*rval[c*CHUNKHEIGHT+rem];
                            partsums[((padding+3*invec->traits.ncols)*tid)+3*v+2] += rval[c*CHUNKHEIGHT+rem]*rval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
            }
        }

        if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
            __m256d sum12;
            __m128d sum12high;
            __m128d res12;
            for (v=0; v<invec->traits.ncols; v++) {

                sum12 = _mm256_hadd_pd(dot1[v],dot2[v]);
                sum12high = _mm256_extractf128_pd(sum12,1);
                res12 = _mm_add_pd(sum12high, _mm256_castpd256_pd128(sum12));

                partsums[((padding+3*invec->traits.ncols)*tid)+3*v+0] += ((double *)&res12)[0];
                partsums[((padding+3*invec->traits.ncols)*tid)+3*v+1] += ((double *)&res12)[1];

                sum12 = _mm256_hadd_pd(dot3[v],dot3[v]);
                sum12high = _mm256_extractf128_pd(sum12,1);
                res12 = _mm_add_pd(sum12high, _mm256_castpd256_pd128(sum12));
                partsums[((padding+3*invec->traits.ncols)*tid)+3*v+2] += ((double *)&res12)[0];
            }
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t v;
        for (v=0; v<invec->traits.ncols; v++) {
            local_dot_product[v                       ] = 0.; 
            local_dot_product[v  +   invec->traits.ncols] = 0.;
            local_dot_product[v  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[v                       ] += partsums[(padding+3*invec->traits.ncols)*i + 3*v + 0];
                local_dot_product[v  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*v + 1];
                local_dot_product[v  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*v + 2];
            }
        }
        free(partsums);
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        PERFWARNING_LOG("AXPBY will not be done on-the-fly!");
        z->axpby(z,res,&seta,&sdelta);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
#if CHUNKHEIGHT < 4
    ERROR_LOG("Invalid chunk height!");
#else
    ERROR_LOG("No AVX available");
#endif
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END

#GHOST_FUNC_BEGIN#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_z_z_cm_CHUNKHEIGHT_x(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX) && CHUNKHEIGHT>=2
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    ghost_lidx_t i;
    complex double *lval = NULL, *rval = NULL;
    complex double *mval = (complex double *)SELL(mat)->val;
    complex double *local_dot_product = NULL;
    complex double *partsums = NULL;
    
    complex double sscale = 1., sbeta = 1.;
    complex double *sshift = NULL;
    __m256d scale, beta, shift;
    complex double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,complex double,complex double);
    scale = _mm256_setzero_pd();
    scale = _mm256_insertf128_pd(scale,_mm_loadu_pd((double *)&sscale),0);
    scale = _mm256_insertf128_pd(scale,_mm_loadu_pd((double *)&sscale),1);
    beta = _mm256_setzero_pd();
    beta = _mm256_insertf128_pd(beta,_mm_loadu_pd((double *)&sbeta),0);
    beta = _mm256_insertf128_pd(beta,_mm_loadu_pd((double *)&sbeta),1);

    int nthreads = 1;
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(complex double);
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(complex double))); 
        for (i=0; i<(3*invec->traits.ncols+padding)*nthreads; i++) {
            partsums[i] = 0.;
        }
    }

#pragma omp parallel shared(partsums)
    {
        ghost_lidx_t j,c,v;
        ghost_lidx_t offs;
        __m256d val;
        __m256d rhs;
        __m128d rhstmp;
        #GHOST_UNROLL#__m256d tmp@;#CHUNKHEIGHT/2
        int tid = ghost_omp_threadnum();
        __m256d dot1[invec->traits.ncols],dot2[invec->traits.ncols],dot3[invec->traits.ncols];
        for (v=0; v<invec->traits.ncols; v++) {
            dot1[v] = _mm256_setzero_pd();
            dot2[v] = _mm256_setzero_pd();
            dot3[v] = _mm256_setzero_pd();
        }

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrowsPadded/CHUNKHEIGHT; c++) 
        { // loop over chunks
            for (v=0; v<invec->traits.ncols; v++)
            {

                #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#CHUNKHEIGHT/2
                lval = (complex double *)res->val+v*res->stride;
                rval = (complex double *)invec->val+v*invec->stride;
                offs = SELL(mat)->chunkStart[c];

                for (j=0; j<SELL(mat)->chunkLenPadded[c]; j++) 
                { // loop inside chunk

                    #GHOST_UNROLL#val    = _mm256_load_pd((double *)&mval[offs]);rhstmp = _mm_load_pd((double *)&rval[(SELL(mat)->col[offs++])]);rhs = _mm256_insertf128_pd(rhs,rhstmp,0);rhstmp = _mm_load_pd((double *)&rval[(SELL(mat)->col[offs++])]);rhs = _mm256_insertf128_pd(rhs,rhstmp,1);tmp@ = _mm256_add_pd(tmp@,complex_mul(val,rhs));#CHUNKHEIGHT/2
                }

                if (spmvmOptions & (GHOST_SPMV_SHIFT | GHOST_SPMV_VSHIFT)) {
                    if (spmvmOptions & GHOST_SPMV_SHIFT) {
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),0);
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),1);
                    } else {
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[v]),0);
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[v]),1);
                    }
                    #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,complex_mul(shift,_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                }
                if (spmvmOptions & GHOST_SPMV_SCALE) {
                    #GHOST_UNROLL#tmp@ = complex_mul(scale,tmp@);#CHUNKHEIGHT/2
                }
                if (spmvmOptions & GHOST_SPMV_AXPY) {
                    #GHOST_UNROLL#_mm256_store_pd((double *)&lval[c*CHUNKHEIGHT+2*@],_mm256_add_pd(tmp@,_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                    #GHOST_UNROLL#_mm256_store_pd((double *)&lval[c*CHUNKHEIGHT+2*@],_mm256_add_pd(tmp@,complex_mul(beta,_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@]))));#CHUNKHEIGHT/2
                } else {
                    #GHOST_UNROLL#_mm256_stream_pd((double *)&lval[c*CHUNKHEIGHT+2*@],tmp@);#CHUNKHEIGHT/2
                }

                if (spmvmOptions & GHOST_SPMV_DOT_YY) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot1[v] = _mm256_add_pd(dot1[v],complex_mul_conj1(_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@]),_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*invec->traits.ncols)*tid)+3*v+0] += conj(lval[c*CHUNKHEIGHT+rem])*lval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
                if (spmvmOptions & GHOST_SPMV_DOT_XY) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot2[v] = _mm256_add_pd(dot2[v],complex_mul_conj1(_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@]),_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*invec->traits.ncols)*tid)+3*v+1] += conj(lval[c*CHUNKHEIGHT+rem])*rval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
                if (spmvmOptions & GHOST_SPMV_DOT_XX) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot3[v] = _mm256_add_pd(dot3[v],complex_mul_conj1(_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@]),_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*invec->traits.ncols)*tid)+3*v+2] += conj(rval[c*CHUNKHEIGHT+rem])*rval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
            }
        }

        if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
            for (v=0; v<invec->traits.ncols; v++) {
                partsums[((padding+3*invec->traits.ncols)*tid)+3*v+0] += ((complex double *)dot1)[2*v] + ((complex double *)dot1)[2*v+1];
                partsums[((padding+3*invec->traits.ncols)*tid)+3*v+1] += ((complex double *)dot2)[2*v] + ((complex double *)dot2)[2*v+1];
                partsums[((padding+3*invec->traits.ncols)*tid)+3*v+2] += ((complex double *)dot3)[2*v] + ((complex double *)dot3)[2*v+1];
            }
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t v;
        for (v=0; v<invec->traits.ncols; v++) {
            local_dot_product[v                       ] = 0.; 
            local_dot_product[v  +   invec->traits.ncols] = 0.;
            local_dot_product[v  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[v                       ] += partsums[(padding+3*invec->traits.ncols)*i + 3*v + 0];
                local_dot_product[v  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*v + 1];
                local_dot_product[v  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*v + 2];
            }
        }
        free(partsums);
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        PERFWARNING_LOG("AXPBY will not be done on-the-fly!");
        z->axpby(z,res,&seta,&sdelta);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
#if CHUNKHEIGHT < 2
    ERROR_LOG("Invalid chunk height!");
#else
    ERROR_LOG("No AVX available");
#endif
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END

#GHOST_FUNC_BEGIN#NVECS=${CFG_BLOCKVECTOR_SIZES}#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_z_z_cm_CHUNKHEIGHT_NVECS(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX) && CHUNKHEIGHT>=2
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    ghost_lidx_t i;
    complex double *lval = NULL, *rval = NULL;
    complex double *mval = (complex double *)SELL(mat)->val;
    complex double *local_dot_product = NULL;
    complex double *partsums = NULL;
    
    complex double sscale = 1., sbeta = 1.;
    complex double *sshift = NULL;
    __m256d scale, beta, shift;
    complex double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,complex double,complex double);
    scale = _mm256_setzero_pd();
    scale = _mm256_insertf128_pd(scale,_mm_loadu_pd((double *)&sscale),0);
    scale = _mm256_insertf128_pd(scale,_mm_loadu_pd((double *)&sscale),1);
    beta = _mm256_setzero_pd();
    beta = _mm256_insertf128_pd(beta,_mm_loadu_pd((double *)&sbeta),0);
    beta = _mm256_insertf128_pd(beta,_mm_loadu_pd((double *)&sbeta),1);

    int nthreads = 1;
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(complex double);
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*NVECS+padding)*nthreads*sizeof(complex double))); 
        for (i=0; i<(3*NVECS+padding)*nthreads; i++) {
            partsums[i] = 0.;
        }
    }

#pragma omp parallel shared(partsums)
    {
        ghost_lidx_t j,c,v;
        ghost_lidx_t offs;
        __m256d val;
        __m256d rhs;
        __m128d rhstmp;
        #GHOST_UNROLL#__m256d tmp@;#CHUNKHEIGHT/2
        int tid = ghost_omp_threadnum();
        __m256d dot1[NVECS],dot2[NVECS],dot3[NVECS];
        for (v=0; v<NVECS; v++) {
            dot1[v] = _mm256_setzero_pd();
            dot2[v] = _mm256_setzero_pd();
            dot3[v] = _mm256_setzero_pd();
        }

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrowsPadded/CHUNKHEIGHT; c++) 
        { // loop over chunks
            for (v=0; v<NVECS; v++)
            {

                #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#CHUNKHEIGHT/2
                lval = (complex double *)res->val+v*res->stride;
                rval = (complex double *)invec->val+v*invec->stride;
                offs = SELL(mat)->chunkStart[c];

                for (j=0; j<SELL(mat)->chunkLenPadded[c]; j++) 
                { // loop inside chunk

                    #GHOST_UNROLL#val    = _mm256_load_pd((double *)&mval[offs]);rhstmp = _mm_load_pd((double *)&rval[(SELL(mat)->col[offs++])]);rhs = _mm256_insertf128_pd(rhs,rhstmp,0);rhstmp = _mm_load_pd((double *)&rval[(SELL(mat)->col[offs++])]);rhs = _mm256_insertf128_pd(rhs,rhstmp,1);tmp@ = _mm256_add_pd(tmp@,complex_mul(val,rhs));#CHUNKHEIGHT/2
                }

                if (spmvmOptions & (GHOST_SPMV_SHIFT | GHOST_SPMV_VSHIFT)) {
                    if (spmvmOptions & GHOST_SPMV_SHIFT) {
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),0);
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),1);
                    } else {
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[v]),0);
                        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[v]),1);
                    }
                    #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,complex_mul(shift,_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                }
                if (spmvmOptions & GHOST_SPMV_SCALE) {
                    #GHOST_UNROLL#tmp@ = complex_mul(scale,tmp@);#CHUNKHEIGHT/2
                }
                if (spmvmOptions & GHOST_SPMV_AXPY) {
                    #GHOST_UNROLL#_mm256_store_pd((double *)&lval[c*CHUNKHEIGHT+2*@],_mm256_add_pd(tmp@,_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                    #GHOST_UNROLL#_mm256_store_pd((double *)&lval[c*CHUNKHEIGHT+2*@],_mm256_add_pd(tmp@,complex_mul(beta,_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@]))));#CHUNKHEIGHT/2
                } else {
                    #GHOST_UNROLL#_mm256_stream_pd((double *)&lval[c*CHUNKHEIGHT+2*@],tmp@);#CHUNKHEIGHT/2
                }

                if (spmvmOptions & GHOST_SPMV_DOT_YY) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot1[v] = _mm256_add_pd(dot1[v],complex_mul_conj1(_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@]),_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*NVECS)*tid)+3*v+0] += conj(lval[c*CHUNKHEIGHT+rem])*lval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
                if (spmvmOptions & GHOST_SPMV_DOT_XY) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot2[v] = _mm256_add_pd(dot2[v],complex_mul_conj1(_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@]),_mm256_load_pd((double *)&lval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*NVECS)*tid)+3*v+1] += conj(lval[c*CHUNKHEIGHT+rem])*rval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
                if (spmvmOptions & GHOST_SPMV_DOT_XX) {
                    if ((c+1)*CHUNKHEIGHT <= mat->nrows) {
                        #GHOST_UNROLL#dot3[v] = _mm256_add_pd(dot3[v],complex_mul_conj1(_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@]),_mm256_load_pd((double *)&rval[c*CHUNKHEIGHT+2*@])));#CHUNKHEIGHT/2
                    } else {
                        ghost_lidx_t rem;
                        for (rem=0; rem<mat->nrows-c*CHUNKHEIGHT; rem++) {
                            partsums[((padding+3*NVECS)*tid)+3*v+2] += conj(rval[c*CHUNKHEIGHT+rem])*rval[c*CHUNKHEIGHT+rem];
                        }
                    }
                }
            }
        }

        if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
            for (v=0; v<NVECS; v++) {
                partsums[((padding+3*NVECS)*tid)+3*v+0] += ((complex double *)dot1)[2*v] + ((complex double *)dot1)[2*v+1];
                partsums[((padding+3*NVECS)*tid)+3*v+1] += ((complex double *)dot2)[2*v] + ((complex double *)dot2)[2*v+1];
                partsums[((padding+3*NVECS)*tid)+3*v+2] += ((complex double *)dot3)[2*v] + ((complex double *)dot3)[2*v+1];
            }
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t v;
        for (v=0; v<NVECS; v++) {
            local_dot_product[v                       ] = 0.; 
            local_dot_product[v  +   NVECS] = 0.;
            local_dot_product[v  + 2*NVECS] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[v                       ] += partsums[(padding+3*NVECS)*i + 3*v + 0];
                local_dot_product[v  +   NVECS] += partsums[(padding+3*NVECS)*i + 3*v + 1];
                local_dot_product[v  + 2*NVECS] += partsums[(padding+3*NVECS)*i + 3*v + 2];
            }
        }
        free(partsums);
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        PERFWARNING_LOG("AXPBY will not be done on-the-fly!");
        z->axpby(z,res,&seta,&sdelta);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
#if CHUNKHEIGHT < 2
    ERROR_LOG("Invalid chunk height!");
#else
    ERROR_LOG("No AVX available");
#endif
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END

#GHOST_FUNC_BEGIN#NVECS=${CFG_BLOCKVECTOR_SIZES}#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_d_d_rm_CHUNKHEIGHT_NVECS(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX) && NVECS>=4
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);

#if NVECS%4
    WARNING_LOG("!!! Augmented functions net yet implemented for remainder !!!");
    __m256i mask;
#if NVECS%4 == 1
    mask = _mm256_set_epi64x(0,0,0,~0);
#endif
#if NVECS%4 == 2
    mask = _mm256_set_epi64x(0,0,~0,~0);
#endif
#if NVECS%4 == 3
    mask = _mm256_set_epi64x(0,~0,~0,~0);
#endif
#endif

    INFO_LOG("In RM kernel with block size NVECS (remainder %d)",NVECS%4);
    double *mval = (double *)SELL(mat)->val;
    double *local_dot_product = NULL;
    double *partsums = NULL;
    int nthreads = 1, i;
    
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);

    UNUSED(argp);
    
    double sscale = 1., sbeta = 1.;
    double *sshift = NULL;
    __m256d scale, beta;
    double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,double,double);
    
    double *sshiftpadded = NULL;
    int sshiftcopied = 0;
    if ((spmvmOptions & GHOST_SPMV_VSHIFT) ) {
        INFO_LOG("Copy to padded VSHIFT");
        GHOST_CALL_RETURN(ghost_malloc_align((void **)&sshiftpadded,PAD(invec->traits.ncols,4)*sizeof(double),256));
        memset(sshiftpadded,0,PAD(invec->traits.ncols,4)*sizeof(double));
        memcpy(sshiftpadded,sshift,invec->traits.ncols*sizeof(double));
        sshiftcopied = 1;
    }
    scale = _mm256_broadcast_sd(&sscale);
    beta = _mm256_broadcast_sd(&sbeta);
    
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(double))); 
        ghost_lidx_t col;
        for (col=0; col<(3*invec->traits.ncols+padding)*nthreads; col++) {
            partsums[col] = 0.;
        }
    }

#pragma omp parallel shared (partsums)
    {
        ghost_lidx_t j,c;
        ghost_lidx_t offs;
        __m256d matval;
        ghost_lidx_t matcol;
        int tid = ghost_omp_threadnum();
        double *lval, *rval, *rptr;
        rptr = (double *)invec->val;
        #GHOST_UNROLL#__m256d tmp@;#(NVECS+3)/4
        #GHOST_UNROLL#__m256d dot1_@ = _mm256_setzero_pd();#NVECS/4
        #GHOST_UNROLL#__m256d dot2_@ = _mm256_setzero_pd();#NVECS/4
        #GHOST_UNROLL#__m256d dot3_@ = _mm256_setzero_pd();#NVECS/4

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrows; c++) 
        { // loop over chunks
            lval = ((double *)(res->val))+res->stride*c;
            rval = &rptr[invec->stride*c];
            offs = SELL(mat)->chunkStart[c/CHUNKHEIGHT]+c%CHUNKHEIGHT;

            #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#(NVECS+3)/4
            for (j=0; j<SELL(mat)->rowLen[c]; j++) { // loop inside chunk
                matval = _mm256_broadcast_sd(&mval[offs+j*CHUNKHEIGHT]);
                matcol = SELL(mat)->col[offs+j*CHUNKHEIGHT];

                #GHOST_UNROLL#tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(matval,_mm256_load_pd(((double *)(invec->val))+invec->stride*(matcol)+@*4)));#NVECS/4
#if NVECS%4
                tmpNVECS/4 = _mm256_add_pd(tmpNVECS/4,_mm256_mul_pd(_mm256_broadcast_sd(&mval[offs+j*CHUNKHEIGHT]),_mm256_maskload_pd(((double *)(invec->val))+invec->stride*(SELL(mat)->col[offs+j*CHUNKHEIGHT])+(NVECS/4)*4,mask)));
#endif
            }
            if (spmvmOptions & GHOST_SPMV_SHIFT) {
                #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,_mm256_mul_pd(_mm256_broadcast_sd(&sshift[0]),_mm256_load_pd(rval+@*4)));#NVECS/4
            } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
                #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,_mm256_mul_pd(_mm256_load_pd(&sshiftpadded[(@%(NVECS/4))*4]),_mm256_load_pd(rval+@*4)));#NVECS/4
            }
            if (spmvmOptions & GHOST_SPMV_SCALE) {
                #GHOST_UNROLL#tmp@ = _mm256_mul_pd(scale,tmp@);#NVECS/4
            }
            if (spmvmOptions & GHOST_SPMV_AXPY) {
                #GHOST_UNROLL#_mm256_store_pd(lval+@*4,_mm256_add_pd(tmp@,_mm256_load_pd(lval+@*4)));#NVECS/4
            } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                #GHOST_UNROLL#_mm256_store_pd(lval+@*4,_mm256_add_pd(tmp@,_mm256_mul_pd(_mm256_load_pd(lval+@*4),beta)));#NVECS/4
            } else {
                #GHOST_UNROLL#_mm256_stream_pd(lval+@*4,tmp@);#NVECS/4
#if NVECS%4
                _mm256_maskstore_pd(lval+(NVECS/4)*4,mask,tmpNVECS/4);
#endif
            }

            if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
                #GHOST_UNROLL#dot1_@ = _mm256_add_pd(dot1_@,_mm256_mul_pd(_mm256_load_pd(lval+@*4),_mm256_load_pd(lval+@*4)));#NVECS/4
                #GHOST_UNROLL#dot2_@ = _mm256_add_pd(dot2_@,_mm256_mul_pd(_mm256_load_pd(rval+@*4),_mm256_load_pd(lval+@*4)));#NVECS/4
                #GHOST_UNROLL#dot3_@ = _mm256_add_pd(dot3_@,_mm256_mul_pd(_mm256_load_pd(rval+@*4),_mm256_load_pd(rval+@*4)));#NVECS/4
            }
        }
        if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
            __m128d part;
            #GHOST_UNROLL#part=_mm256_extractf128_pd(dot1_@,0);_mm_storel_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+0],part);_mm_storeh_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+3+0],part);part=_mm256_extractf128_pd(dot1_@,1);_mm_storel_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+6+0],part);_mm_storeh_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+9+0],part);#NVECS/4
            #GHOST_UNROLL#part=_mm256_extractf128_pd(dot2_@,0);_mm_storel_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+1],part);_mm_storeh_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+3+1],part);part=_mm256_extractf128_pd(dot2_@,1);_mm_storel_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+6+1],part);_mm_storeh_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+9+1],part);#NVECS/4
            #GHOST_UNROLL#part=_mm256_extractf128_pd(dot3_@,0);_mm_storel_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+2],part);_mm_storeh_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+3+2],part);part=_mm256_extractf128_pd(dot3_@,1);_mm_storel_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+6+2],part);_mm_storeh_pd(&partsums[((padding+3*invec->traits.ncols)*tid)+12*@+9+2],part);#NVECS/4
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t col;
        for (col=0; col<invec->traits.ncols; col++) {
            local_dot_product[col                       ] = 0.; 
            local_dot_product[col  +   invec->traits.ncols] = 0.;
            local_dot_product[col  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[col                         ] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 0];
                local_dot_product[col  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 1];
                local_dot_product[col  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 2];
            }
        }
        free(partsums);
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        PERFWARNING_LOG("AXPBY will not be done on-the-fly!");
        z->axpby(z,res,&seta,&sdelta);
    }
    if (sshiftcopied) {
        free(sshiftpadded);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
#if NVECS < 4
    ERROR_LOG("Invalid nVecs");
#else
    ERROR_LOG("No AVX available");
#endif
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END

#GHOST_FUNC_BEGIN#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_d_d_rm_CHUNKHEIGHT_x(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX)
    INFO_LOG("In RM kernel fixed arbitrary block size and fixed chunk height");
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    const double * const restrict mval = (double *)SELL(mat)->val;
    double *local_dot_product = NULL;
    double *partsums = NULL;
    int nthreads = 1, i;
    ghost_lidx_t ncolspadded = PAD(invec->traits.ncols,4);
    ghost_lidx_t donecols = 0;

    const bool resultisview = res->traits.flags & GHOST_DENSEMAT_VIEW;
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);

    UNUSED(argp);
    
    double sscale = 1., sbeta = 1.;
    double *sshift = NULL;
    double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;
    __m256d scale, beta, shift, delta, eta;
    __m256d vshift[ncolspadded/4];

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,double,double);
    
    double *sshiftpadded = NULL;
    int sshiftcopied = 0;
    if ((spmvmOptions & GHOST_SPMV_VSHIFT)) {
        INFO_LOG("Copy to padded VSHIFT");
        GHOST_CALL_RETURN(ghost_malloc_align((void **)&sshiftpadded,PAD(invec->traits.ncols,4)*sizeof(double),256));
        memset(sshiftpadded,0,PAD(invec->traits.ncols,4)*sizeof(double));
        memcpy(sshiftpadded,sshift,invec->traits.ncols*sizeof(double));
        sshiftcopied = 1;
    }
        
    scale = _mm256_set1_pd(sscale);
    beta = _mm256_set1_pd(sbeta);

    if (spmvmOptions & GHOST_SPMV_SHIFT) {
        shift = _mm256_setzero_pd();
        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),0);
        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),1);
    } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
        for (donecols=0; donecols < ncolspadded; donecols+=4) {
            vshift[donecols/4] = _mm256_load_pd((double *)&(sshiftpadded[donecols]));
        }
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        delta = _mm256_setzero_pd();
        delta = _mm256_insertf128_pd(delta,_mm_load_pd((double *)&sdelta),0);
        delta = _mm256_insertf128_pd(delta,_mm_load_pd((double *)&sdelta),1);
        eta = _mm256_setzero_pd();
        eta = _mm256_insertf128_pd(eta,_mm_load_pd((double *)&seta),0);
        eta = _mm256_insertf128_pd(eta,_mm_load_pd((double *)&seta),1);
    }

    
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(double))); 
        ghost_lidx_t col;
        for (col=0; col<(3*invec->traits.ncols+padding)*nthreads; col++) {
            partsums[col] = 0.;
        }
    }

#pragma omp parallel shared (partsums) private(donecols)
    {
        ghost_lidx_t j,c;
        ghost_lidx_t offs;
        __m256d rhs;
        __m256d matval;
        int tid = ghost_omp_threadnum();
        __m256d tmp[ncolspadded/4];
        __m256i mask = _mm256_set_epi64x(~0,~0,~0,~0); 
        if (res->traits.ncols%4 == 1) {
            mask = _mm256_set_epi64x(0,0,0,~0);
        }
        if (res->traits.ncols%4 == 2) {
            mask = _mm256_set_epi64x(0,0,~0,~0);
        }
        if (res->traits.ncols%4 == 3) {
            mask = _mm256_set_epi64x(0,~0,~0,~0);
        }
        __m256d dot1[ncolspadded/4];
        __m256d dot2[ncolspadded/4];
        __m256d dot3[ncolspadded/4];
        for (donecols=0; donecols < ncolspadded; donecols+=4) {
            dot1[donecols/4] = _mm256_setzero_pd();
            dot2[donecols/4] = _mm256_setzero_pd();
            dot3[donecols/4] = _mm256_setzero_pd();
        }

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrows; c++) 
        { // loop over chunks
            double *lval = ((double *)(res->val))+res->stride*c;
            double *rval = ((double *)(invec->val))+invec->stride*c;
            double *zval = NULL;
            if (z) {
               zval = ((double *)(z->val))+z->stride*c;
            }
            offs = SELL(mat)->chunkStart[c/CHUNKHEIGHT]+c%CHUNKHEIGHT;

            for (donecols=0; donecols < ncolspadded; donecols+=4) {
                tmp[donecols/4] = _mm256_setzero_pd();
            
                for (j=0; j<SELL(mat)->rowLen[c]; j++) { // loop inside chunk
                    matval=_mm256_set1_pd(mval[offs+j*CHUNKHEIGHT]);
                    ghost_lidx_t matcol = SELL(mat)->col[offs+j*CHUNKHEIGHT];

                    rhs = _mm256_load_pd((double *)((double *)(invec->val)+invec->stride*(matcol)+donecols));
                    tmp[donecols/4] = _mm256_add_pd(tmp[donecols/4],_mm256_mul_pd(rhs,matval));
                }
                
                if (spmvmOptions & GHOST_SPMV_SHIFT) {
                    tmp[donecols/4] = _mm256_sub_pd(tmp[donecols/4],_mm256_mul_pd(shift,_mm256_load_pd((double *)(rval+donecols))));
                } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
                    tmp[donecols/4] = _mm256_sub_pd(tmp[donecols/4],_mm256_mul_pd(vshift[donecols/4],_mm256_load_pd((double *)(rval+donecols))));
                }
                if (spmvmOptions & GHOST_SPMV_SCALE) {
                    tmp[donecols/4] = _mm256_mul_pd(scale,tmp[donecols/4]);
                }
                if (!resultisview || (resultisview && donecols<ncolspadded-4)) {
                    if (spmvmOptions & GHOST_SPMV_AXPY) {
                        _mm256_store_pd((double *)(lval+donecols),_mm256_add_pd(tmp[donecols/4],_mm256_load_pd((double *)(lval+donecols))));
                    } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                        _mm256_store_pd((double *)(lval+donecols),_mm256_add_pd(tmp[donecols/4],_mm256_mul_pd(_mm256_load_pd((double *)(lval+donecols)),beta)));
                    } else if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                        _mm256_store_pd((double *)(lval+donecols),tmp[donecols/4]);
                    } else {
                        _mm256_stream_pd((double *)(lval+donecols),tmp[donecols/4]);
                    }
                    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                        _mm256_store_pd((double *)(zval+donecols),_mm256_add_pd(_mm256_mul_pd(_mm256_load_pd((double *)(zval+donecols)),delta),_mm256_mul_pd(_mm256_load_pd((double *)(lval+donecols)),eta)));
                    }
                }
            }
            if (resultisview) {
                if (spmvmOptions & GHOST_SPMV_AXPY) {
                    _mm256_maskstore_pd((double *)(lval+donecols),mask,_mm256_add_pd(tmp[donecols/4],_mm256_load_pd((double *)(lval+donecols))));
                } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                    _mm256_maskstore_pd((double *)(lval+donecols),mask,_mm256_add_pd(tmp[donecols/4],_mm256_mul_pd(_mm256_load_pd((double *)(lval+donecols)),beta)));
                } else {
                    _mm256_maskstore_pd((double *)(lval+donecols),mask,tmp[donecols/4]);
                }
                if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                    _mm256_maskstore_pd((double *)(zval+donecols),mask,_mm256_add_pd(_mm256_mul_pd(_mm256_load_pd((double *)(zval+donecols)),delta),_mm256_mul_pd(_mm256_load_pd((double *)(lval+donecols)),eta)));
                }
            }

            for (donecols=0; donecols < ncolspadded; donecols+=4) {
                if (spmvmOptions & GHOST_SPMV_DOT_YY) {
                    dot1[donecols/4] = _mm256_add_pd(dot1[donecols/4],_mm256_mul_pd(_mm256_load_pd((const double*)(lval+donecols)),_mm256_load_pd((const double*)(lval+donecols))));
                }
                if (spmvmOptions & GHOST_SPMV_DOT_XY) {
                    dot2[donecols/4] = _mm256_add_pd(dot2[donecols/4],_mm256_mul_pd(_mm256_load_pd((const double*)(rval+donecols)),_mm256_load_pd((const double*)(lval+donecols))));
                }
                if (spmvmOptions & GHOST_SPMV_DOT_XX) {
                    dot3[donecols/4] = _mm256_add_pd(dot3[donecols/4],_mm256_mul_pd(_mm256_load_pd((const double*)(rval+donecols)),_mm256_load_pd((const double*)(rval+donecols))));
                }
            }
        }
        if (spmvmOptions & GHOST_SPMV_DOT_YY) {
            __m128d a,b;
            for (donecols=0; donecols < ncolspadded-4; donecols+=4) {
                a = _mm256_extractf128_pd(dot1[donecols/4],0); 
                b = _mm256_extractf128_pd(dot1[donecols/4],1); 
                _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+0+0],a); 
                _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+3+0],a); 
                _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+6+0],b); 
                _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+9+0],b); 
            }
            a = _mm256_extractf128_pd(dot1[donecols/4],0); 
            b = _mm256_extractf128_pd(dot1[donecols/4],1); 
            switch (invec->traits.ncols % 4) {
                case 0:
                    _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+9+0],b);
                case 3: 
                    _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+6+0],b);
                case 2:
                    _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+3+0],a); 
                case 1: 
                    _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+0+0],a);
            } 
        }
        if (spmvmOptions & GHOST_SPMV_DOT_XY) {
            __m128d a,b;
            for (donecols=0; donecols < ncolspadded-4; donecols+=4) {
                a = _mm256_extractf128_pd(dot2[donecols/4],0); 
                b = _mm256_extractf128_pd(dot2[donecols/4],1); 
                _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+0+1],a); 
                _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+3+1],a); 
                _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+6+1],b); 
                _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+9+1],b); 
            }
            a = _mm256_extractf128_pd(dot2[donecols/4],0); 
            b = _mm256_extractf128_pd(dot2[donecols/4],1); 
            switch (invec->traits.ncols % 4) {
                case 0:
                    _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+9+1],b);
                case 3: 
                    _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+6+1],b);
                case 2:
                    _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+3+1],a); 
                case 1: 
                    _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+0+1],a);
            } 
        }
        if (spmvmOptions & GHOST_SPMV_DOT_XX) {
            __m128d a,b;
            for (donecols=0; donecols < ncolspadded-4; donecols+=4) {
                a = _mm256_extractf128_pd(dot3[donecols/4],0); 
                b = _mm256_extractf128_pd(dot3[donecols/4],1); 
                _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+0+2],a); 
                _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+3+2],a); 
                _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+6+2],b); 
                _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+9+2],b); 
            }
            a = _mm256_extractf128_pd(dot3[donecols/4],0); 
            b = _mm256_extractf128_pd(dot3[donecols/4],1); 
            switch (invec->traits.ncols % 4) {
                case 0:
                    _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+9+2],b);
                case 3: 
                    _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+6+2],b);
                case 2:
                    _mm_storeh_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+3+2],a); 
                case 1: 
                    _mm_storel_pd(&((double *)partsums)[((padding+3*invec->traits.ncols)*tid)+12*(donecols/4)+0+2],a);
            } 
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t col;
        for (col=0; col<invec->traits.ncols; col++) {
            local_dot_product[col                       ] = 0.; 
            local_dot_product[col  +   invec->traits.ncols] = 0.;
            local_dot_product[col  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[col                         ] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 0];
                local_dot_product[col  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 1];
                local_dot_product[col  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 2];
            }
        }
        free(partsums);
    }

    if (sshiftcopied) {
        free(sshiftpadded);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
    ERROR_LOG("No AVX available");
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END


#GHOST_FUNC_BEGIN#NVECS=${CFG_BLOCKVECTOR_SIZES}#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_z_z_rm_CHUNKHEIGHT_NVECS(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX) && NVECS >= 2
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    INFO_LOG("In RM kernel fixed block size and chunk height");
    const double * const restrict mval = (double *)SELL(mat)->val;
    complex double *local_dot_product = NULL;
    complex double *partsums = NULL;
    int nthreads = 1, i;
    
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);

    UNUSED(argp);
    
    complex double sscale = 1., sbeta = 1.;
    complex double *sshift = NULL;
    complex double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;
    __m256d scale, beta, shift, delta, eta;
    #GHOST_UNROLL#__m256d vshift@;#NVECS/2

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,complex double,complex double);
    
    complex double *sshiftpadded = NULL;
    int sshiftcopied = 0;
    if ((spmvmOptions & GHOST_SPMV_VSHIFT)) {
        INFO_LOG("Copy to padded VSHIFT");
        GHOST_CALL_RETURN(ghost_malloc_align((void **)&sshiftpadded,PAD(invec->traits.ncols,4)*sizeof(complex double),256));
        memset(sshiftpadded,0,PAD(invec->traits.ncols,4)*sizeof(complex double));
        memcpy(sshiftpadded,sshift,invec->traits.ncols*sizeof(complex double));
        sshiftcopied = 1;
    }
        
    scale = _mm256_setzero_pd();
    scale = _mm256_insertf128_pd(scale,_mm_load_pd((double *)&sscale),0);
    scale = _mm256_insertf128_pd(scale,_mm_load_pd((double *)&sscale),1);
    beta = _mm256_setzero_pd();
    beta = _mm256_insertf128_pd(beta,_mm_load_pd((double *)&sbeta),0);
    beta = _mm256_insertf128_pd(beta,_mm_load_pd((double *)&sbeta),1);

    if (spmvmOptions & GHOST_SPMV_SHIFT) {
        shift = _mm256_setzero_pd();
        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),0);
        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),1);
    } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
        #GHOST_UNROLL#vshift@ = _mm256_load_pd(&((double *)sshiftpadded)[(@%(NVECS/2))*4]);#NVECS/2
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        delta = _mm256_setzero_pd();
        delta = _mm256_insertf128_pd(delta,_mm_load_pd((double *)&sdelta),0);
        delta = _mm256_insertf128_pd(delta,_mm_load_pd((double *)&sdelta),1);
        eta = _mm256_setzero_pd();
        eta = _mm256_insertf128_pd(eta,_mm_load_pd((double *)&seta),0);
        eta = _mm256_insertf128_pd(eta,_mm_load_pd((double *)&seta),1);
    }

    
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(complex double))); 
        ghost_lidx_t col;
        for (col=0; col<(3*invec->traits.ncols+padding)*nthreads; col++) {
            partsums[col] = 0.;
        }
    }

#pragma omp parallel shared (partsums)
    {
        ghost_lidx_t j,c;
        ghost_lidx_t offs;
        __m256d rhs;
        __m256d matval;
        int tid = ghost_omp_threadnum();
        #GHOST_UNROLL#__m256d tmp@;#NVECS/2
        #GHOST_UNROLL#__m256d dot1_@ = _mm256_setzero_pd();#NVECS/2
        #GHOST_UNROLL#__m256d dot2_@ = _mm256_setzero_pd();#NVECS/2
        #GHOST_UNROLL#__m256d dot3_@ = _mm256_setzero_pd();#NVECS/2

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrows; c++) 
        { // loop over chunks
            complex double *lval = ((complex double *)(res->val))+res->stride*c;
            complex double *rval = ((complex double *)(invec->val))+invec->stride*c;
            complex double *zval = NULL;
            if (z) {
               zval = ((complex double *)(z->val))+z->stride*c;
            }
            offs = SELL(mat)->chunkStart[c/CHUNKHEIGHT]+c%CHUNKHEIGHT;

            #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#NVECS/2
            
            for (j=0; j<SELL(mat)->rowLen[c]; j++) { // loop inside chunk
                    #GHOST_UNROLL#rhs = _mm256_load_pd(((double *)(invec->val))+invec->stride*(SELL(mat)->col[offs+j*CHUNKHEIGHT])*2+@*4);matval=_mm256_insertf128_pd(matval,_mm_load_pd(&mval[2*(offs+j*CHUNKHEIGHT)]),0);matval=_mm256_insertf128_pd(matval,_mm_load_pd(&mval[2*(offs+j*CHUNKHEIGHT)]),1);tmp@ = _mm256_add_pd(tmp@,complex_mul(rhs,matval));#NVECS/2
            }
            if (spmvmOptions & GHOST_SPMV_SHIFT) {
                #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,complex_mul(shift,_mm256_load_pd(((double *)rval)+@*4)));#NVECS/2
            } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
                #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,complex_mul(vshift@,_mm256_load_pd(((double *)rval)+@*4)));#NVECS/2
            }
            if (spmvmOptions & GHOST_SPMV_SCALE) {
                #GHOST_UNROLL#tmp@ = complex_mul(scale,tmp@);#NVECS/2
            }
            if (spmvmOptions & GHOST_SPMV_AXPY) {
                #GHOST_UNROLL#_mm256_store_pd(((double *)lval)+@*4,_mm256_add_pd(tmp@,_mm256_load_pd(((double *)lval)+@*4)));#NVECS/2
            } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                #GHOST_UNROLL#_mm256_store_pd(((double *)lval)+@*4,_mm256_add_pd(tmp@,complex_mul(_mm256_load_pd(((double *)lval)+@*4),beta)));#NVECS/2
            } else if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                #GHOST_UNROLL#_mm256_store_pd(((double *)lval)+@*4,tmp@);#NVECS/2
            } else {
                #GHOST_UNROLL#_mm256_stream_pd(((double *)lval)+@*4,tmp@);#NVECS/2
            }

            if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                #GHOST_UNROLL#_mm256_store_pd(((double *)zval)+@*4,_mm256_add_pd(complex_mul(_mm256_load_pd(((double *)zval)+@*4),delta),complex_mul(_mm256_load_pd(((double *)lval)+@*4),eta)));#NVECS/2
            }
            if (spmvmOptions & GHOST_SPMV_DOT_YY) {
                #GHOST_UNROLL#dot1_@ = _mm256_add_pd(dot1_@,complex_mul_conj1(_mm256_load_pd((const double*)lval+@*4),_mm256_load_pd((const double*)lval+@*4)));#NVECS/2
            }
            if (spmvmOptions & GHOST_SPMV_DOT_XY) {
                #GHOST_UNROLL#dot2_@ = _mm256_add_pd(dot2_@,complex_mul_conj1(_mm256_load_pd((const double*)rval+@*4),_mm256_load_pd((const double*)lval+@*4)));#NVECS/2
            }
            if (spmvmOptions & GHOST_SPMV_DOT_XX) {
                #GHOST_UNROLL#dot3_@ = _mm256_add_pd(dot3_@,complex_mul_conj1(_mm256_load_pd((const double*)rval+@*4),_mm256_load_pd((const double*)rval+@*4)));#NVECS/2
            }
        }
        if (spmvmOptions & GHOST_SPMV_DOT_YY) {
#ifdef __INTEL_COMPILER
            #GHOST_UNROLL#_mm256_storeu2_m128d(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+0)],&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+3+0)],dot1_@);#NVECS/2
#else
            __m128d v128;
            #GHOST_UNROLL#v128 = _mm256_castpd256_pd128(dot1_@); _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+0)],v128); v128 = _mm256_extractf128_pd(dot1_@,1); _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+3+0)],v128);#NVECS/2
#endif
        }
        if (spmvmOptions & GHOST_SPMV_DOT_XY) {
#ifdef __INTEL_COMPILER
            #GHOST_UNROLL#_mm256_storeu2_m128d(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+1)],&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+3+1)],dot2_@);#NVECS/2
#else
            __m128d v128;
            #GHOST_UNROLL#v128 = _mm256_castpd256_pd128(dot1_@); _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+1)],v128); v128 = _mm256_extractf128_pd(dot1_@,1); _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+3+1)],v128);#NVECS/2
#endif
        }
        if (spmvmOptions & GHOST_SPMV_DOT_XX) {
#ifdef __INTEL_COMPILER
            #GHOST_UNROLL#_mm256_storeu2_m128d(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+2)],&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+3+2)],dot3_@);#NVECS/2
#else
            __m128d v128;
            #GHOST_UNROLL#v128 = _mm256_castpd256_pd128(dot1_@); _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+2)],v128); v128 = _mm256_extractf128_pd(dot1_@,1); _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*@+3+2)],v128);#NVECS/2

#endif
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t col;
        for (col=0; col<invec->traits.ncols; col++) {
            local_dot_product[col                       ] = 0.; 
            local_dot_product[col  +   invec->traits.ncols] = 0.;
            local_dot_product[col  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[col                         ] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 0];
                local_dot_product[col  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 1];
                local_dot_product[col  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 2];
            }
        }
        free(partsums);
    }

    if (sshiftcopied) {
        free(sshiftpadded);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
#if NVECS < 2
    ERROR_LOG("Invalid nVecs");
#else
    ERROR_LOG("No AVX available");
#endif
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END

#GHOST_FUNC_BEGIN#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_z_z_rm_CHUNKHEIGHT_x(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX)
    INFO_LOG("In RM kernel fixed arbitrary block size and fixed chunk height");
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    const double * const restrict mval = (double *)SELL(mat)->val;
    complex double *local_dot_product = NULL;
    complex double *partsums = NULL;
    int nthreads = 1, i;
    ghost_lidx_t ncolspadded = PAD(invec->traits.ncols,2);
    ghost_lidx_t donecols = 0;

    
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);

    UNUSED(argp);
    
    complex double sscale = 1., sbeta = 1.;
    complex double *sshift = NULL;
    complex double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;
    __m256d scale, beta, shift, delta, eta;
    __m256d vshift[ncolspadded/2];

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,complex double,complex double);
    
    complex double *sshiftpadded = NULL;
    int sshiftcopied = 0;
    if ((spmvmOptions & GHOST_SPMV_VSHIFT)) {
        INFO_LOG("Copy to padded VSHIFT");
        GHOST_CALL_RETURN(ghost_malloc_align((void **)&sshiftpadded,PAD(invec->traits.ncols,4)*sizeof(complex double),256));
        memset(sshiftpadded,0,PAD(invec->traits.ncols,4)*sizeof(complex double));
        memcpy(sshiftpadded,sshift,invec->traits.ncols*sizeof(complex double));
        sshiftcopied = 1;
    }
        
    scale = _mm256_setzero_pd();
    scale = _mm256_insertf128_pd(scale,_mm_load_pd((double *)&sscale),0);
    scale = _mm256_insertf128_pd(scale,_mm_load_pd((double *)&sscale),1);
    beta = _mm256_setzero_pd();
    beta = _mm256_insertf128_pd(beta,_mm_load_pd((double *)&sbeta),0);
    beta = _mm256_insertf128_pd(beta,_mm_load_pd((double *)&sbeta),1);

    if (spmvmOptions & GHOST_SPMV_SHIFT) {
        shift = _mm256_setzero_pd();
        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),0);
        shift = _mm256_insertf128_pd(shift,_mm_load_pd((double *)&sshift[0]),1);
    } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
        for (donecols=0; donecols < ncolspadded; donecols+=2) {
            vshift[donecols/2] = _mm256_load_pd((double *)&(sshiftpadded[donecols]));
        }
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        delta = _mm256_setzero_pd();
        delta = _mm256_insertf128_pd(delta,_mm_load_pd((double *)&sdelta),0);
        delta = _mm256_insertf128_pd(delta,_mm_load_pd((double *)&sdelta),1);
        eta = _mm256_setzero_pd();
        eta = _mm256_insertf128_pd(eta,_mm_load_pd((double *)&seta),0);
        eta = _mm256_insertf128_pd(eta,_mm_load_pd((double *)&seta),1);
    }

    
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(complex double))); 
        ghost_lidx_t col;
        for (col=0; col<(3*invec->traits.ncols+padding)*nthreads; col++) {
            partsums[col] = 0.;
        }
    }

#pragma omp parallel shared (partsums) private(donecols)
    {
        ghost_lidx_t j,c;
        ghost_lidx_t offs;
        __m256d rhs;
        __m256d matval;
        int tid = ghost_omp_threadnum();
        __m256d tmp[ncolspadded/2];
        __m256i mask = _mm256_set_epi64x(~0,~0,~0,~0); 
        if (res->traits.ncols%2 == 1) {
            mask = _mm256_set_epi64x(0,0,~0,~0);
        }
        __m256d dot1[ncolspadded/2];
        __m256d dot2[ncolspadded/2];
        __m256d dot3[ncolspadded/2];
        for (donecols=0; donecols < ncolspadded; donecols+=2) {
            dot1[donecols/2] = _mm256_setzero_pd();
            dot2[donecols/2] = _mm256_setzero_pd();
            dot3[donecols/2] = _mm256_setzero_pd();
        }

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrows; c++) 
        { // loop over chunks
            complex double *lval = ((complex double *)(res->val))+res->stride*c;
            complex double *rval = ((complex double *)(invec->val))+invec->stride*c;
            complex double *zval = NULL;
            if (z) {
               zval = ((complex double *)(z->val))+z->stride*c;
            }
            offs = SELL(mat)->chunkStart[c/CHUNKHEIGHT]+c%CHUNKHEIGHT;

            for (donecols=0; donecols < ncolspadded; donecols+=2) {
                tmp[donecols/2] = _mm256_setzero_pd();
            }
            
            for (j=0; j<SELL(mat)->rowLen[c]; j++) { // loop inside chunk
                matval=_mm256_insertf128_pd(matval,_mm_load_pd(&mval[2*(offs+j*CHUNKHEIGHT)]),0);
                matval=_mm256_insertf128_pd(matval,_mm_load_pd(&mval[2*(offs+j*CHUNKHEIGHT)]),1);
                ghost_lidx_t matcol = SELL(mat)->col[offs+j*CHUNKHEIGHT];

                for (donecols=0; donecols < ncolspadded/2; donecols++) {
                    rhs = _mm256_load_pd((double *)((complex double *)(invec->val)+invec->stride*(matcol)+donecols*2));
                    tmp[donecols] = _mm256_add_pd(tmp[donecols],complex_mul(rhs,matval));
                }
            }
            
            if (spmvmOptions & GHOST_SPMV_SHIFT) {
                for (donecols=0; donecols < ncolspadded; donecols+=2) {
                    tmp[donecols/2] = _mm256_sub_pd(tmp[donecols/2],complex_mul(shift,_mm256_load_pd((double *)(rval+donecols))));
                }
            } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
                for (donecols=0; donecols < ncolspadded; donecols+=2) {
                    tmp[donecols/2] = _mm256_sub_pd(tmp[donecols/2],complex_mul(vshift[donecols/2],_mm256_load_pd((double *)(rval+donecols))));
                }
            }
            if (spmvmOptions & GHOST_SPMV_SCALE) {
                for (donecols=0; donecols < ncolspadded; donecols+=2) {
                    tmp[donecols/2] = complex_mul(scale,tmp[donecols/2]);
                }
            }
            if (spmvmOptions & GHOST_SPMV_AXPY) {
                for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                    _mm256_store_pd((double *)(lval+donecols),_mm256_add_pd(tmp[donecols/2],_mm256_load_pd((double *)(lval+donecols))));
                }
                _mm256_maskstore_pd((double *)(lval+donecols),mask,_mm256_add_pd(tmp[donecols/2],_mm256_load_pd((double *)(lval+donecols))));
            } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                    _mm256_store_pd((double *)(lval+donecols),_mm256_add_pd(tmp[donecols/2],complex_mul(_mm256_load_pd((double *)(lval+donecols)),beta)));
                }
                _mm256_maskstore_pd((double *)(lval+donecols),mask,_mm256_add_pd(tmp[donecols/2],complex_mul(_mm256_load_pd((double *)(lval+donecols)),beta)));
            } else if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                    _mm256_store_pd((double *)(lval+donecols),tmp[donecols/2]);
                }
                _mm256_maskstore_pd((double *)(lval+donecols),mask,tmp[donecols/2]);
            } else {
                for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                    _mm256_stream_pd((double *)(lval+donecols),tmp[donecols/2]);
                }
                if (!(res->traits.ncols%2) || !(res->traits.flags & GHOST_DENSEMAT_VIEW)) {
                    _mm256_stream_pd((double *)(lval+donecols),tmp[donecols/2]);
                } else {
                    _mm_stream_pd((double *)(lval+donecols),_mm256_extractf128_pd(tmp[donecols/2],0));
                }
            }
            if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
                for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                    _mm256_store_pd((double *)(zval+donecols),_mm256_add_pd(complex_mul(_mm256_load_pd((double *)(zval+donecols)),delta),complex_mul(_mm256_load_pd((double *)(lval+donecols)),eta)));
                }
                _mm256_maskstore_pd((double *)(zval+donecols),mask,_mm256_add_pd(complex_mul(_mm256_load_pd((double *)(zval+donecols)),delta),complex_mul(_mm256_load_pd((double *)(lval+donecols)),eta)));
            }

            if (spmvmOptions & GHOST_SPMV_DOT_YY) {
                for (donecols=0; donecols < ncolspadded; donecols+=2) {
                    dot1[donecols/2] = _mm256_add_pd(dot1[donecols/2],complex_mul_conj1(_mm256_load_pd((const double*)(lval+donecols)),_mm256_load_pd((const double*)(lval+donecols))));
                }
            }
            if (spmvmOptions & GHOST_SPMV_DOT_XY) {
                for (donecols=0; donecols < ncolspadded; donecols+=2) {
                    dot2[donecols/2] = _mm256_add_pd(dot2[donecols/2],complex_mul_conj1(_mm256_load_pd((const double*)(rval+donecols)),_mm256_load_pd((const double*)(lval+donecols))));
                }
            }
            if (spmvmOptions & GHOST_SPMV_DOT_XX) {
                for (donecols=0; donecols < ncolspadded; donecols+=2) {
                    dot3[donecols/2] = _mm256_add_pd(dot3[donecols/2],complex_mul_conj1(_mm256_load_pd((const double*)(rval+donecols)),_mm256_load_pd((const double*)(rval+donecols))));
                }
            }
        }
        if (spmvmOptions & GHOST_SPMV_DOT_YY) {
            __m128d v128;
            for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                v128 = _mm256_castpd256_pd128(dot1[donecols/2]); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+0)],v128); 
                v128 = _mm256_extractf128_pd(dot1[donecols/2],1); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+3+0)],v128);
            }
            v128 = _mm256_castpd256_pd128(dot1[donecols/2]); 
            _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+0)],v128);
            if (!(invec->traits.ncols % 2)) {
                v128 = _mm256_extractf128_pd(dot1[donecols/2],1); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+3+0)],v128);
            }
        }
        if (spmvmOptions & GHOST_SPMV_DOT_XY) {
            __m128d v128;
            for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                v128 = _mm256_castpd256_pd128(dot2[donecols/2]); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+1)],v128); 
                v128 = _mm256_extractf128_pd(dot2[donecols/2],1); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+3+1)],v128);
            }
            v128 = _mm256_castpd256_pd128(dot2[donecols/2]); 
            _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+1)],v128); 
            if (!(invec->traits.ncols % 2)) {
                v128 = _mm256_extractf128_pd(dot2[donecols/2],1); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+3+1)],v128);
            }
        }
        if (spmvmOptions & GHOST_SPMV_DOT_XX) {
            __m128d v128;
            for (donecols=0; donecols < ncolspadded-2; donecols+=2) {
                v128 = _mm256_castpd256_pd128(dot3[donecols/2]); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+2)],v128); 
                v128 = _mm256_extractf128_pd(dot3[donecols/2],1); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+3+2)],v128);
            }
            v128 = _mm256_castpd256_pd128(dot3[donecols/2]); 
            _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+2)],v128); 
            if (!(invec->traits.ncols % 2)) {
                v128 = _mm256_extractf128_pd(dot3[donecols/2],1); 
                _mm_storeu_pd(&((double *)partsums)[2*(((padding+3*invec->traits.ncols)*tid)+6*donecols/2+3+2)],v128);
            }
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t col;
        for (col=0; col<invec->traits.ncols; col++) {
            local_dot_product[col                       ] = 0.; 
            local_dot_product[col  +   invec->traits.ncols] = 0.;
            local_dot_product[col  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[col                         ] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 0];
                local_dot_product[col  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 1];
                local_dot_product[col  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 2];
            }
        }
        free(partsums);
    }

    if (sshiftcopied) {
        free(sshiftpadded);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    return GHOST_SUCCESS;
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
    ERROR_LOG("No AVX available");
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END
#if 0

#GHOST_FUNC_BEGIN#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv__a_avx_d_d_rm_CHUNKHEIGHT_x(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#ifdef GHOST_HAVE_AVX
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
    PERFWARNING_LOG("In variable-ncols kernel with %"PRLIDX" cols",invec->traits.ncols);
    ghost_lidx_t j,c,col;
    ghost_lidx_t offs;
    double *mval = (double *)SELL(mat)->val;
    double *local_dot_product = NULL;
    double *partsums = NULL;
    __m256d rhs;
    int nthreads = 1, i;
    int ncolspadded = PAD(invec->traits.ncols,4);
    
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);

    UNUSED(argp);
    
    double sscale = 1., sbeta = 1.;
    double *sshift = NULL;
    __m256d shift, scale, beta;
    double sdelta = 0., seta = 0.;
    ghost_densemat_t *z = NULL;

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,z,sdelta,seta,double,double);
    scale = _mm256_broadcast_sd(&sscale);
    beta = _mm256_broadcast_sd(&sbeta);
    int axpy = spmvmOptions & (GHOST_SPMV_AXPY | GHOST_SPMV_AXPBY);
    
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(double))); 
        for (col=0; col<(3*invec->traits.ncols+padding)*nthreads; col++) {
            partsums[col] = 0.;
        }
    }

#pragma omp parallel private(c,j,offs,rhs,col) shared (partsums)
    {
        int tid = ghost_omp_threadnum();
        #GHOST_UNROLL#__m256d tmp@;#CHUNKHEIGHT
        double *tmpresult = NULL;
        if (!axpy) {
            ghost_malloc((void **)&tmpresult,sizeof(double)*CHUNKHEIGHT*ncolspadded);
        }

        ghost_lidx_t donecols;

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrowsPadded/CHUNKHEIGHT; c++) 
        { // loop over chunks
            double *lval = ((double *)(res->val))+res->stride*c*CHUNKHEIGHT;
            double *rval = ((double *)(invec->val))+invec->stride*CHUNKHEIGHT;

            for (donecols = 0; donecols < ncolspadded; donecols+=4) {
                #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#CHUNKHEIGHT
                offs = SELL(mat)->chunkStart[c];

                for (j=0; j<SELL(mat)->chunkLen[c]; j++) { // loop inside chunk
                    #GHOST_UNROLL#rhs = _mm256_load_pd(((double *)(invec->val))+invec->stride*SELL(mat)->col[offs]+donecols);tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(_mm256_broadcast_sd(&mval[offs++]),rhs));#CHUNKHEIGHT
                }
              
                if (spmvmOptions & (GHOST_SPMV_SHIFT | GHOST_SPMV_VSHIFT)) {
                    if (spmvmOptions & GHOST_SPMV_SHIFT) {
                        shift = _mm256_broadcast_sd(&sshift[0]);
                    } else {
                        shift = _mm256_load_pd(&sshift[donecols]);
                    }
                    #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,_mm256_mul_pd(shift,_mm256_load_pd(((double *)(invec->val)) + res->stride*(c*CHUNKHEIGHT+@)+donecols)));#CHUNKHEIGHT
                }
                if (spmvmOptions & GHOST_SPMV_SCALE) {
                    #GHOST_UNROLL#tmp@ = _mm256_mul_pd(scale,tmp@);#CHUNKHEIGHT
                }
                if (axpy || ncolspadded<=4 || CHUNKHEIGHT == 1) {
                    if (spmvmOptions & GHOST_SPMV_AXPY) {
                        #GHOST_UNROLL#_mm256_store_pd(&lval[invec->traits.ncolspadded*@+donecols],_mm256_add_pd(tmp@,_mm256_load_pd(&lval[invec->traits.ncolspadded*@+donecols])));#CHUNKHEIGHT
                    } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                        #GHOST_UNROLL#_mm256_store_pd(&lval[invec->traits.ncolspadded*@+donecols],_mm256_add_pd(tmp@,_mm256_mul_pd(_mm256_load_pd(&lval[invec->traits.ncolspadded*@+donecols]),beta)));#CHUNKHEIGHT
                    } else {
                        #GHOST_UNROLL#_mm256_store_pd(&lval[invec->traits.ncolspadded*@+donecols],tmp@);#CHUNKHEIGHT
                    }
                    if ((c+1)*CHUNKHEIGHT < mat->nrows) {
                        if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
                            for (col = donecols; col<donecols+4; col++) {
                                #GHOST_UNROLL#partsums[((padding+3*invec->traits.ncols)*tid)+3*col+0] += lval[col+@*invec->traits.ncolspadded]*lval[col+@*invec->traits.ncolspadded];#CHUNKHEIGHT
                                #GHOST_UNROLL#partsums[((padding+3*invec->traits.ncols)*tid)+3*col+1] += lval[col+@*invec->traits.ncolspadded]*rval[col+@*invec->traits.ncolspadded];#CHUNKHEIGHT
                                #GHOST_UNROLL#partsums[((padding+3*invec->traits.ncols)*tid)+3*col+2] += rval[col+@*invec->traits.ncolspadded]*rval[col+@*invec->traits.ncolspadded];#CHUNKHEIGHT
                            }
                        }
                    } else {
                        ghost_lidx_t row = c*CHUNKHEIGHT;
                        ghost_lidx_t rowinchunk = 0;
                        for (; row<mat->nrows; row++) {
                            if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
                                for (col = donecols; col<donecols+4; col++) {
                                    partsums[((padding+3*invec->traits.ncols)*tid)+3*col+0] += lval[col+rowinchunk*invec->traits.ncolspadded]*lval[col+rowinchunk*invec->traits.ncolspadded];
                                    partsums[((padding+3*invec->traits.ncols)*tid)+3*col+1] += lval[col+rowinchunk*invec->traits.ncolspadded]*rval[col+rowinchunk*invec->traits.ncolspadded];
                                    partsums[((padding+3*invec->traits.ncols)*tid)+3*col+2] += rval[col+rowinchunk*invec->traits.ncolspadded]*rval[col+rowinchunk*invec->traits.ncolspadded];
                                }
                            }
                            rowinchunk++;
                        }
                    }
                } else { 
                    #GHOST_UNROLL#_mm256_store_pd(&tmpresult[@*ncolspadded+donecols],tmp@);#CHUNKHEIGHT
                    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
                        WARNING_LOG("local dot currently not working here!");
                    }
                    // TODO if non-AXPY cxompute DOT from tmp instead of lval
                }
            }
            if (!axpy && ncolspadded>4 && CHUNKHEIGHT != 1) {
                #GHOST_UNROLL#for (donecols = 0; donecols < ncolspadded; donecols+=4) {tmp@ = _mm256_load_pd(&tmpresult[@*ncolspadded+donecols]); _mm256_stream_pd(&lval[@*ncolspadded+donecols],tmp@);}#CHUNKHEIGHT
            }
            
        }
        free(tmpresult);
    }
    if (spmvmOptions & GHOST_SPMV_DOT_ANY) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        for (col=0; col<invec->traits.ncols; col++) {
            local_dot_product[col                       ] = 0.; 
            local_dot_product[col  +   invec->traits.ncols] = 0.;
            local_dot_product[col  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[col                         ] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 0];
                local_dot_product[col  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 1];
                local_dot_product[col  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 2];
            }
        }
        free(partsums);
    }
    if (spmvmOptions & GHOST_SPMV_CHAIN_AXPBY) {
        PERFWARNING_LOG("AXPBY will not be done on-the-fly!");
        z->axpby(z,res,&seta,&sdelta);
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL);
#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
    ERROR_LOG("No AVX available");
    return GHOST_ERR_UNKNOWN;
#endif
    return GHOST_SUCCESS;
}
#GHOST_FUNC_END

#GHOST_FUNC_BEGIN#NVECS=${CFG_BLOCKVECTOR_SIZES}#CHUNKHEIGHT=${CFG_SELL_CHUNKHEIGHTS}
ghost_error_t ghost_sellspmv_v2__avx_d_d_rm_CHUNKHEIGHT_NVECS(ghost_sparsemat_t *mat, ghost_densemat_t* res, ghost_densemat_t* invec, ghost_spmv_flags_t spmvmOptions,va_list argp)
{
#if defined(GHOST_HAVE_AVX) && NVECS>=4
    INFO_LOG("in RM kernel");
    double *mval = (double *)SELL(mat)->val;
    double *local_dot_product = NULL;
    double *partsums = NULL;
    int nthreads = 1, i;
    
    unsigned clsize;
    ghost_machine_cacheline_size(&clsize);
    int padding = (int)clsize/sizeof(double);

    UNUSED(argp);
    
    double sscale = 1., sbeta = 1.;
    double *sshift = NULL;
    __m256d scale, beta;

    GHOST_SPMV_PARSE_ARGS(spmvmOptions,argp,sscale,sbeta,sshift,local_dot_product,double,double);
    scale = _mm256_broadcast_sd(&sscale);
    beta = _mm256_broadcast_sd(&sbeta);
    
    if (spmvmOptions & GHOST_SPMV_DOT) {

#pragma omp parallel 
        {
#pragma omp single
            nthreads = ghost_omp_nthread();
        }

        GHOST_CALL_RETURN(ghost_malloc((void **)&partsums,(3*invec->traits.ncols+padding)*nthreads*sizeof(double))); 
        ghost_lidx_t col;
        for (col=0; col<(3*invec->traits.ncols+padding)*nthreads; col++) {
            partsums[col] = 0.;
        }
    }

#pragma omp parallel shared (partsums)
    {
        ghost_lidx_t j,c,col;
        ghost_lidx_t offs;
        __m256d rhs;
        int tid = ghost_omp_threadnum();
        #GHOST_UNROLL#__m256d tmp@;#CHUNKHEIGHT*NVECS/4

#pragma omp for schedule(runtime)
        for (c=0; c<mat->nrowsPadded/CHUNKHEIGHT; c++) 
        { // loop over chunks
            double *lval = (double *)res->val[c*CHUNKHEIGHT];
            double *rval = (double *)invec->val[c*CHUNKHEIGHT];

            #GHOST_UNROLL#tmp@ = _mm256_setzero_pd();#CHUNKHEIGHT*NVECS/4
            offs = SELL(mat)->chunkStart[c];

            for (j=0; j<SELL(mat)->chunkLen[c]; j++) { // loop inside chunk
                
                #GHOST_UNROLL#rhs = _mm256_load_pd((double *)invec->val[SELL(mat)->col[offs]]+(@%(NVECS/4))*4);tmp@ = _mm256_add_pd(tmp@,_mm256_mul_pd(_mm256_broadcast_sd(&mval[offs]),rhs));if(!((@+1)%(NVECS/4)))offs++;#CHUNKHEIGHT*NVECS/4
            }
            if (spmvmOptions & GHOST_SPMV_SHIFT) {
                #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,_mm256_mul_pd(_mm256_broadcast_sd(&sshift[0]),_mm256_load_pd(rval+@*4)));#CHUNKHEIGHT*NVECS/4
            } else if (spmvmOptions & GHOST_SPMV_VSHIFT) {
                #GHOST_UNROLL#tmp@ = _mm256_sub_pd(tmp@,_mm256_mul_pd(_mm256_load_pd(&sshift[(@%(NVECS/4))*4]),_mm256_load_pd(rval+@*4)));#CHUNKHEIGHT*NVECS/4
            }
            if (spmvmOptions & GHOST_SPMV_SCALE) {
                #GHOST_UNROLL#tmp@ = _mm256_mul_pd(scale,tmp@);#CHUNKHEIGHT*NVECS/4
            }
            if (spmvmOptions & GHOST_SPMV_AXPY) {
                #GHOST_UNROLL#_mm256_store_pd(lval+@*4,_mm256_add_pd(tmp@,_mm256_load_pd(lval+@*4)));#CHUNKHEIGHT*NVECS/4
            } else if (spmvmOptions & GHOST_SPMV_AXPBY) {
                #GHOST_UNROLL#_mm256_store_pd(lval+@*4,_mm256_add_pd(tmp@,_mm256_mul_pd(_mm256_load_pd(lval+@*4),beta)));#CHUNKHEIGHT*NVECS/4
            } else {
                #GHOST_UNROLL#_mm256_stream_pd(lval+@*4,tmp@);#CHUNKHEIGHT*NVECS/4
            }
            if (spmvmOptions & GHOST_SPMV_DOT) {
                for (col = 0; col<invec->traits.ncols; col++) {
                    #GHOST_UNROLL#partsums[((padding+3*invec->traits.ncols)*tid)+3*col+0] += lval[col+@*invec->traits.ncolspadded]*lval[col+@*invec->traits.ncolspadded];#CHUNKHEIGHT
                    #GHOST_UNROLL#partsums[((padding+3*invec->traits.ncols)*tid)+3*col+1] += lval[col+@*invec->traits.ncolspadded]*rval[col+@*invec->traits.ncolspadded];#CHUNKHEIGHT
                    #GHOST_UNROLL#partsums[((padding+3*invec->traits.ncols)*tid)+3*col+2] += rval[col+@*invec->traits.ncolspadded]*rval[col+@*invec->traits.ncolspadded];#CHUNKHEIGHT
                }
            }
        }
    }
    if (spmvmOptions & GHOST_SPMV_DOT) {
        if (!local_dot_product) {
            WARNING_LOG("The location of the local dot products is NULL. Will not compute them!");
            return GHOST_SUCCESS;
        }
        ghost_lidx_t col;
        for (col=0; col<invec->traits.ncols; col++) {
            local_dot_product[col                       ] = 0.; 
            local_dot_product[col  +   invec->traits.ncols] = 0.;
            local_dot_product[col  + 2*invec->traits.ncols] = 0.;
            for (i=0; i<nthreads; i++) {
                local_dot_product[col                         ] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 0];
                local_dot_product[col  +   invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 1];
                local_dot_product[col  + 2*invec->traits.ncols] += partsums[(padding+3*invec->traits.ncols)*i + 3*col + 2];
            }
        }
        free(partsums);
    }

#else
    UNUSED(mat);
    UNUSED(res);
    UNUSED(invec);
    UNUSED(spmvmOptions);
    UNUSED(argp);
#if NVECS < 4
    ERROR_LOG("Invalid nVecs");
#else
    ERROR_LOG("No AVX available");
#endif
    return GHOST_ERR_UNKNOWN;
#endif
    return GHOST_SUCCESS;
}
#GHOST_FUNC_END
#endif
