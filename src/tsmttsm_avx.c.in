#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/math.h"
#include "ghost/instr.h"
#include "ghost/locality.h"
#include "ghost/util.h"
#include "ghost/tsmttsm_avx_gen.h"
#include <immintrin.h>


#GHOST_FUNC_BEGIN#CFGK=${CFG_BLOCKVECTOR_SIZES}#CFGM=${CFG_BLOCKVECTOR_SIZES}
ghost_error_t ghost_tsmttsm__avx_d_CFGK_CFGM(ghost_densemat_t *x, ghost_densemat_t *v, ghost_densemat_t *w, void *alpha, void *beta, int conjv)
{
    UNUSED(conjv);
#ifdef GHOST_HAVE_AVX
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    ghost_error_t ret = GHOST_SUCCESS;

    int myrank=0;

    GHOST_CALL_GOTO(ghost_rank(&myrank,v->context->mpicomm),err,ret);

    ghost_lidx_t n = v->traits.nrows;
    INFO_LOG("In AVX TSMTTSM with two fixed block sizes [CFGK][CFGM] %dx%d <- %dx%d * %dx%d",CFGM,CFGK,CFGM,n,n,CFGK);
    
    double * const restrict vval = NULL, * const restrict wval = NULL, * restrict xval;
    ghost_lidx_t ldv, ldw, ldx;

    ldv = *v->stride;
    ldw = *w->stride;
    ldx = *x->stride;

    ghost_densemat_valptr(v,(void **)&vval);
    ghost_densemat_valptr(w,(void **)&wval);
    ghost_densemat_valptr(x,(void **)&xval);

    __m256d betavec, alphavec;
   
    betavec = _mm256_broadcast_sd(beta);
    alphavec = _mm256_broadcast_sd(alpha);
    
    double dalpha = *(double *)alpha;
    double dbeta = *(double *)beta;
    
    // make sure that the initial x only gets added up once
    if (myrank) {
        dbeta = 0.;
        betavec = _mm256_setzero_pd();
    }

    ghost_lidx_t i,j;

#ifdef GHOST_TSMTTSM_KAHAN
                if (CFGK%4 || CFGM%4) {
                    WARNING_LOG("Kahan TSMTTSM only implemented for K and M multiples of 4. "
                            "Fallback to normal summation for the remainder!");
                }
#endif
    
    if (x->traits.storage == GHOST_DENSEMAT_COLMAJOR) {
        ghost_lidx_t k;
        for (k=0; k<CFGK; k++) {
            for (j=0; j+4<=CFGM; j+=4) {
                _mm256_store_pd(&xval[k*ldx+j],_mm256_mul_pd(_mm256_load_pd(&xval[k*ldx+j]),betavec));
            }
            for (; j<CFGM; j++) {
                xval[k*ldx+j] = dbeta*xval[k*ldx+j];
            }
        }
#pragma omp parallel private(j,k)
        {
            j=0;
#ifdef GHOST_TSMTTSM_KAHAN
            #GHOST_UNROLL#__m256d c@ = _mm256_setzero_pd();#4
            #GHOST_UNROLL#__m256d y@;#4
            #GHOST_UNROLL#__m256d t@;#4
#endif
            #GHOST_UNROLL#__m256d vvec@;#4
            #GHOST_UNROLL#__m256d wvec@;#4

            double x_priv[CFGM*CFGK];
            memset(x_priv,0,sizeof(x_priv));

#pragma omp for schedule(runtime)
            for (i=0; i<n; i++) {
                for (k=0; k+4<=CFGK; k+=4) {
                    #GHOST_UNROLL#wvec@ = _mm256_mul_pd(_mm256_set1_pd(wval[i*ldw+(k+@)]),alphavec);#4
                    
                    for (j=0; j+4<=CFGM; j+=4) {
                        #GHOST_UNROLL#vvec@ = _mm256_mul_pd(_mm256_load_pd(&vval[i*ldv+j]),wvec@);#4

#ifdef GHOST_TSMTTSM_KAHAN
                        #GHOST_UNROLL#y@ = _mm256_sub_pd(vvec@,c@);#4
                        #GHOST_UNROLL#t@ = _mm256_add_pd(y@,_mm256_load_pd(&x_priv[(k+@)*CFGM+j]));#4
                        #GHOST_UNROLL#c@ = _mm256_sub_pd(_mm256_sub_pd(t@,_mm256_load_pd(&x_priv[(k+@)*CFGM+j])),y@);#4
                        #GHOST_UNROLL#_mm256_store_pd(&x_priv[(k+@)*CFGM+j],t@);#4
#else
                        #GHOST_UNROLL#_mm256_store_pd(&x_priv[(k+@)*CFGM+j],_mm256_add_pd(_mm256_load_pd(&x_priv[(k+@)*CFGM+j]),vvec@));#4
#endif

                    }
                    for (; j<CFGM; j++) {
                        x_priv[(k+0)*CFGM+j] += dalpha*vval[i*ldv+j]*wval[i*ldw+(k+0)];
                        x_priv[(k+1)*CFGM+j] += dalpha*vval[i*ldv+j]*wval[i*ldw+(k+1)];
                        x_priv[(k+2)*CFGM+j] += dalpha*vval[i*ldv+j]*wval[i*ldw+(k+2)];
                        x_priv[(k+3)*CFGM+j] += dalpha*vval[i*ldv+j]*wval[i*ldw+(k+3)];
                    }
                }
#ifdef GHOST_TSMTTSM_KAHAN
                c0 = _mm256_setzero_pd();
#endif
                for (; k<CFGK; k++) {
                    wvec0 = _mm256_mul_pd(_mm256_set1_pd(wval[i*ldw+(k+0)]),alphavec);
                    for (j=0; j+4<=CFGM; j+=4) {
                        vvec0 = _mm256_mul_pd(_mm256_load_pd(&vval[i*ldv+j]),wvec0);

#ifdef GHOST_TSMTTSM_KAHAN
                        y0 = _mm256_sub_pd(vvec0,c0);
                        t0 = _mm256_add_pd(y0,_mm256_load_pd(&x_priv[(k+0)*CFGM+j]));
                        c0 = _mm256_sub_pd(_mm256_sub_pd(t0,_mm256_load_pd(&x_priv[(k+0)*CFGM+j])),y0);
                        _mm256_store_pd(&x_priv[(k+0)*CFGM+j],t0);
#else
                        _mm256_store_pd(&x_priv[(k+0)*CFGM+j],_mm256_add_pd(_mm256_load_pd(&x_priv[(k+0)*CFGM+j]),vvec0));
#endif

                    }
                    for (; j<CFGM; j++) {
                        x_priv[k*CFGM+j] += dalpha*vval[i*ldv+j]*wval[i*ldw+k];
                    }
                }
            }
            
#ifdef GHOST_TSMTTSM_KAHAN
            #GHOST_UNROLL#c@ = _mm256_setzero_pd();#4
#endif

#pragma omp critical
            {
                j=0;
                for (k=0; k+4<=CFGK; k+=4) {
                    for (j=0; j+4<=CFGM; j+=4) {
#ifdef GHOST_TSMTTSM_KAHAN
                        #GHOST_UNROLL#y@ = _mm256_sub_pd(_mm256_load_pd(&x_priv[(k+@)*CFGM+j]),c@);#4
                        #GHOST_UNROLL#t@ = _mm256_add_pd(y@,_mm256_load_pd(&xval[(k+@)*ldx+j]));#4
                        #GHOST_UNROLL#c@ = _mm256_sub_pd(_mm256_sub_pd(t@,_mm256_load_pd(&xval[(k+@)*ldx+j])),y@);#4
                        #GHOST_UNROLL#_mm256_store_pd(&xval[(k+@)*ldx+j],t@);#4
#else
                        #GHOST_UNROLL#_mm256_store_pd(&xval[(k+@)*ldx+j],_mm256_add_pd(_mm256_load_pd(&xval[(k+@)*ldx+j]),_mm256_load_pd(&x_priv[(k+@)*CFGM+j])));#4
#endif

                    }
                    for (; j<CFGM; j++) {
                        xval[(k+0)*ldx+j] += x_priv[(k+0)*CFGM+j];
                        xval[(k+1)*ldx+j] += x_priv[(k+1)*CFGM+j];
                        xval[(k+2)*ldx+j] += x_priv[(k+2)*CFGM+j];
                        xval[(k+3)*ldx+j] += x_priv[(k+3)*CFGM+j];
                    }
                }
#ifdef GHOST_TSMTTSM_KAHAN
                c0 = _mm256_setzero_pd();
#endif
                for (; k<CFGK; k++) {
                    for (j=0; j+4<=CFGM; j+=4) {
#ifdef GHOST_TSMTTSM_KAHAN
                        y0 = _mm256_sub_pd(_mm256_load_pd(&x_priv[(k+0)*CFGM+j]),c0);
                        t0 = _mm256_add_pd(y0,_mm256_load_pd(&xval[(k+0)*ldx+j]));
                        c0 = _mm256_sub_pd(_mm256_sub_pd(t0,_mm256_load_pd(&xval[(k+0)*ldx+j])),y0);
                        _mm256_store_pd(&xval[(k+0)*ldx+j],t0);
#else
                        _mm256_store_pd(&xval[(k+0)*ldx+j],_mm256_add_pd(_mm256_load_pd(&xval[(k+0)*ldx+j]),_mm256_load_pd(&x_priv[(k+0)*CFGM+j])));
#endif

                    }
                    for (; j<CFGM; j++) {
                        xval[k*ldx+j] += x_priv[k*CFGM+j];
                    }
                }
            }
    }
            
#ifdef GHOST_HAVE_MPI
        if (x->traits.flags & GHOST_DENSEMAT_VIEW) {
            for (k=0; k<CFGK; k++) {
                MPI_CALL_GOTO(MPI_Allreduce(MPI_IN_PLACE,xval+k*ldx,x->traits.ncols,MPI_DOUBLE,MPI_SUM,v->context->mpicomm),err,ret);
            }
        } else {
            MPI_CALL_GOTO(MPI_Allreduce(MPI_IN_PLACE,xval,CFGK*ldx,MPI_DOUBLE,MPI_SUM,v->context->mpicomm),err,ret);
        }
#endif
    
    } else {
        ERROR_LOG("Will be implemented soon :-)!");
        ret = GHOST_ERR_NOT_IMPLEMENTED;
        goto err;
    }
   
    goto out;
err:

out:
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH)
    return ret;
#else
    UNUSED(x);
    UNUSED(v);
    UNUSED(w);
    UNUSED(alpha);
    UNUSED(beta);
    ERROR_LOG("No AVX available!");
    return GHOST_ERR_UNKNOWN;
#endif
}
#GHOST_FUNC_END
