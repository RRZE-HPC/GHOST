#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/util.h"
#include "ghost/densemat.h"
#include "ghost/log.h"
#include "ghost/timing.h"
#include "ghost/locality.h"
#include "ghost/instr.h"
#include "ghost/rand.h"
#include "ghost/tsmm_inplace_cu_gen.h"

#include <cuda_runtime.h>
#include <stdio.h>
#include <cublas_v2.h>
#include <curand.h>
#include <sys/types.h>
#include <unistd.h>
#include <complex.h>

#include "ghost/cu_complex.h"
#include "ghost/complex.h"

#define THREADSPERBLOCK 512

template<typename T,int NCOLSOUT, int NCOLSIN> __global__ static void ghost_tsmm_inplace_cu_rm_cm(T * x, const T * const __restrict__ w, const T alpha, const T beta, ghost_lidx_t nrows, ghost_lidx_t stridex, ghost_lidx_t stridew)
{
    int row = blockIdx.x*blockDim.x+threadIdx.x;
    int k;
    int m;
    T tmp[NCOLSOUT];

    for (;row < nrows; row+=gridDim.x*blockDim.x) {
        for (k=0; k<NCOLSOUT; k++) {
            tmp[k] = scale<T>(x[row*stridex+k],beta);
            for (m=0; m<NCOLSIN; m++) {
                tmp[k] = axpy<T,T>(tmp[k],alpha,scale<T>(x[row*stridex+m],w[k*stridew+m]));
            }
        }
        for (k=0; k<NCOLSOUT; k++) {
            x[row*stridex+k] = tmp[k];
        }
    }
}

template<typename T> __global__ static void ghost_tsmm_inplace_cu_rm_cm_fallback(T * x, const T * const __restrict__ w, const T alpha, const T beta, ghost_lidx_t nrows, ghost_lidx_t stridex, ghost_lidx_t stridew, int NCOLSOUT, int NCOLSIN)
{
    int row = blockIdx.x*blockDim.x+threadIdx.x;
    int k;
    int m;
    T * tmp = new T[NCOLSOUT];

    for (;row < nrows; row+=gridDim.x*blockDim.x) {
        for (k=0; k<NCOLSOUT; k++) {
            tmp[k] = scale<T>(x[row*stridex+k],beta);
            for (m=0; m<NCOLSIN; m++) {
                tmp[k] = axpy<T,T>(tmp[k],alpha,scale<T>(__ldg(&x[row*stridex+m]),__ldg(&w[k*stridew+m])));
            }
        }
        for (k=0; k<NCOLSOUT; k++) {
            x[row*stridex+k] = tmp[k];
        }
    }
    delete tmp;
}

#GHOST_FUNC_BEGIN#NCOLSIN=${CFG_BLOCKVECTOR_SIZES}#NCOLSOUT=${CFG_BLOCKVECTOR_SIZES}
ghost_error_t ghost_tsmm_inplace__cuda_x_NCOLSIN_NCOLSOUT(ghost_densemat_t *x, ghost_densemat_t *w, void *alpha, void *beta) 
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL); 
    ghost_error_t ret = GHOST_SUCCESS;
    dim3 block, grid;
    grid.x = (int)ceil((double)x->traits.nrows/THREADSPERBLOCK);
    grid.y = 1;
    grid.z = 1;
    block.x = THREADSPERBLOCK;
    block.y = 1;
    block.z = 1;

    if (x->traits.datatype & GHOST_DT_COMPLEX) {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
            ghost_tsmm_inplace_cu_rm_cm<cuDoubleComplex,NCOLSOUT,NCOLSIN><<< grid,block >>>(
                    (cuDoubleComplex *)x->cu_val,(const cuDoubleComplex *)w->cu_val,*(cuDoubleComplex *)alpha,*(cuDoubleComplex *)beta,x->traits.nrows,x->stride,w->stride);
        } else {
            ghost_tsmm_inplace_cu_rm_cm<cuFloatComplex,NCOLSOUT,NCOLSIN><<< grid,block >>>(
                    (cuFloatComplex *)x->cu_val,(const cuFloatComplex *)w->cu_val,*(cuFloatComplex *)alpha,*(cuFloatComplex *)beta,x->traits.nrows,x->stride,w->stride);
        }
    } else {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
            ghost_tsmm_inplace_cu_rm_cm<double,NCOLSOUT,NCOLSIN><<< grid,block >>>(
                   (double *)x->cu_val,(const double *)w->cu_val,*(double *)alpha,*(double *)beta,x->traits.nrows,x->stride,w->stride);
        } else {
            ghost_tsmm_inplace_cu_rm_cm<float,NCOLSOUT,NCOLSIN><<< grid,block >>>(
                   (float *)x->cu_val,(const float *)w->cu_val,*(float *)alpha,*(float *)beta,x->traits.nrows,x->stride,w->stride);
        }
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL); 
    CUDA_CALL_RETURN(cudaGetLastError());
    return ret;
}
#GHOST_FUNC_END

ghost_error_t ghost_tsmm_inplace__cuda_x_x_x(ghost_densemat_t *x, ghost_densemat_t *w, void *alpha, void *beta) 
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL); 
    ghost_error_t ret = GHOST_SUCCESS;
    dim3 block, grid;
    grid.x = (int)ceil((double)x->traits.nrows/THREADSPERBLOCK);
    grid.y = 1;
    grid.z = 1;
    block.x = THREADSPERBLOCK;
    block.y = 1;
    block.z = 1;

    if (x->traits.datatype & GHOST_DT_COMPLEX) {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
            ghost_tsmm_inplace_cu_rm_cm_fallback<cuDoubleComplex><<< grid,block >>>(
                    (cuDoubleComplex *)x->cu_val,(const cuDoubleComplex *)w->cu_val,*(cuDoubleComplex *)alpha,*(cuDoubleComplex *)beta,x->traits.nrows,x->stride,w->stride,x->traits.ncols,w->traits.nrows);
        } else {
            ghost_tsmm_inplace_cu_rm_cm_fallback<cuFloatComplex><<< grid,block >>>(
                    (cuFloatComplex *)x->cu_val,(const cuFloatComplex *)w->cu_val,*(cuFloatComplex *)alpha,*(cuFloatComplex *)beta,x->traits.nrows,x->stride,w->stride,x->traits.ncols,w->traits.nrows);
        }
    } else {
        if (x->traits.datatype & GHOST_DT_DOUBLE) {
            ghost_tsmm_inplace_cu_rm_cm_fallback<double><<< grid,block >>>(
                   (double *)x->cu_val,(const double *)w->cu_val,*(double *)alpha,*(double *)beta,x->traits.nrows,x->stride,w->stride,x->traits.ncols,w->traits.nrows);
        } else {
            ghost_tsmm_inplace_cu_rm_cm_fallback<float><<< grid,block >>>(
                   (float *)x->cu_val,(const float *)w->cu_val,*(float *)alpha,*(float *)beta,x->traits.nrows,x->stride,w->stride,x->traits.ncols,w->traits.nrows);
        }
    }

    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH|GHOST_FUNCTYPE_KERNEL); 
    CUDA_CALL_RETURN(cudaGetLastError());
    return ret;
}
