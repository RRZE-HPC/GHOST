/*!GHOST_AUTOGEN  */
#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/math.h"
#include "ghost/instr.h"
#include "ghost/util.h"
#include "ghost/tsmttsm_var2_avx_gen.h"
#include <immintrin.h>
#include <math.h>
#include <float.h>
#include "iaca/iacaMarks.h"

#define complex_mul(a,b) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(b,b,0),a),_mm256_mul_pd(_mm256_shuffle_pd(b,b,0xF),_mm256_shuffle_pd(a,a,5)))
#define complex_mul_conj1(b,a) _mm256_addsub_pd(_mm256_mul_pd(_mm256_shuffle_pd(b,b,0),a),_mm256_mul_pd(_mm256_xor_pd(_mm256_shuffle_pd(b,b,0xF),_mm256_set1_pd(-0.)),_mm256_shuffle_pd(a,a,5)))

#define complex_mulf(a,b) _mm256_addsub_ps(_mm256_mul_ps(_mm256_shuffle_ps(b,b,0xA0),a),_mm256_mul_ps(_mm256_shuffle_ps(b,b,0xF5),_mm256_shuffle_ps(a,a,0xB1)))
#define complex_mulf_conj1(b,a) _mm256_addsub_ps(_mm256_mul_ps(_mm256_shuffle_ps(b,b,0xA0),a),_mm256_mul_ps(_mm256_xor_ps(_mm256_shuffle_ps(b,b,0xF5),_mm256_set1_ps(-0.f)),_mm256_shuffle_ps(a,a,0xB1)))

ghost_error ghost_tsmttsm__a_avx_d_x_x_1_rm(ghost_densemat *x, ghost_densemat *v, ghost_densemat *w, void *alpha, void *beta, int conjv)
{
    UNUSED(conjv);
#ifdef GHOST_BUILD_AVX
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH)
    ghost_error ret = GHOST_SUCCESS;

    const double * const restrict vval = (const double *) v->val;
    const double * const restrict wval = (const double *) w->val;
    double * const restrict xval = (double *) x->val;

    const ghost_lidx ldv = v->stride;
    const ghost_lidx ldw = w->stride;
    const ghost_lidx ldx = x->stride;

    ghost_lidx n = v->traits.nrows;
    INFO_LOG("In AVX TSMTTSM with two non-fixed block sizes %dx%d <- %dx%d * %dx%d",ldv,ldw,ldv,n,n,ldw);
    

    __m256d betavec, alphavec;
   
    betavec = _mm256_broadcast_sd(beta);
    alphavec = _mm256_broadcast_sd(alpha);
    
    double dalpha = *(double *)alpha;
    double dbeta = *(double *)beta;
    
    ghost_lidx i,m;

    ghost_lidx k;
    for (k=0; k<x->traits.ncols; k++) {
        for (m=0; m+4<=x->traits.nrows; m+=4) {
            _mm256_storeu_pd(&xval[k*ldx+m],_mm256_mul_pd(_mm256_loadu_pd(&xval[k*ldx+m]),betavec));
        }
        for (; m<x->traits.nrows; m++) {
            xval[k*ldx+m] = dbeta*xval[k*ldx+m];
        }
    }
#pragma omp parallel private(m,k)
    {
        m=0;
        #GHOST_UNROLL#__m256d vvec@;#4
        #GHOST_UNROLL#__m256d wvec@;#4

        double *x_priv;
        ghost_lidx ldxpriv = PAD(ldv,4);
        if (ldxpriv != ldv) {
            INFO_LOG("Pad xpriv from %d to %d",ldv,ldxpriv);
        }
        ghost_malloc_align((void **)&x_priv,ldxpriv*ldw*sizeof(double),32);
        memset(x_priv,0,ldxpriv*ldw*sizeof(double));
        
        if (fabs(dalpha-1.) > DBL_MIN) { 

#pragma omp for schedule(runtime)
            for (i=0; i<n; i++) {
                for (k=0; k+4<=x->traits.ncols; k+=4) {
                    #GHOST_UNROLL#wvec@ = _mm256_mul_pd(_mm256_set1_pd(wval[i*ldw+(k+@)]),alphavec);#4
                   
                    for (m=0; m+4<=x->traits.nrows; m+=4) {
                        #GHOST_UNROLL#vvec@ = _mm256_mul_pd(_mm256_loadu_pd(&vval[i*ldv+m]),wvec@);#4
                        #GHOST_UNROLL#_mm256_store_pd(&x_priv[(k+@)*ldxpriv+m],_mm256_add_pd(_mm256_load_pd(&x_priv[(k+@)*ldxpriv+m]),vvec@));#4

                    }
                    for (; m<x->traits.nrows; m++) {
                        x_priv[(k+0)*ldxpriv+m] += dalpha*vval[i*ldv+m]*wval[i*ldw+(k+0)];
                        x_priv[(k+1)*ldxpriv+m] += dalpha*vval[i*ldv+m]*wval[i*ldw+(k+1)];
                        x_priv[(k+2)*ldxpriv+m] += dalpha*vval[i*ldv+m]*wval[i*ldw+(k+2)];
                        x_priv[(k+3)*ldxpriv+m] += dalpha*vval[i*ldv+m]*wval[i*ldw+(k+3)];
                    }
                }
                for (; k<x->traits.ncols; k++) {
                    wvec0 = _mm256_mul_pd(_mm256_set1_pd(wval[i*ldw+(k+0)]),alphavec);
                    for (m=0; m+4<=x->traits.nrows; m+=4) {
                        vvec0 = _mm256_mul_pd(_mm256_loadu_pd(&vval[i*ldv+m]),wvec0);
                        _mm256_store_pd(&x_priv[(k+0)*ldxpriv+m],_mm256_add_pd(_mm256_load_pd(&x_priv[(k+0)*ldxpriv+m]),vvec0));
                    }
                    for (; m<x->traits.nrows; m++) {
                        x_priv[k*ldxpriv+m] += dalpha*vval[i*ldv+m]*wval[i*ldw+k];
                    }
                }
            }
        } else {
            INFO_LOG("fast case alpha=1");
#pragma omp for schedule(runtime)
            for (i=0; i<n; i++) {
                for (k=0; k+4<=x->traits.ncols; k+=4) {
                    #GHOST_UNROLL#wvec@ = _mm256_set1_pd(wval[i*ldw+(k+@)]);#4
                   
                    for (m=0; m+4<=x->traits.nrows; m+=4) {
                        #GHOST_UNROLL#vvec@ = _mm256_mul_pd(_mm256_loadu_pd(&vval[i*ldv+m]),wvec@);#4
                        #GHOST_UNROLL#_mm256_store_pd(&x_priv[(k+@)*ldxpriv+m],_mm256_add_pd(_mm256_load_pd(&x_priv[(k+@)*ldxpriv+m]),vvec@));#4

                    }
                    for (; m<x->traits.nrows; m++) {
                        x_priv[(k+0)*ldxpriv+m] += vval[i*ldv+m]*wval[i*ldw+(k+0)];
                        x_priv[(k+1)*ldxpriv+m] += vval[i*ldv+m]*wval[i*ldw+(k+1)];
                        x_priv[(k+2)*ldxpriv+m] += vval[i*ldv+m]*wval[i*ldw+(k+2)];
                        x_priv[(k+3)*ldxpriv+m] += vval[i*ldv+m]*wval[i*ldw+(k+3)];
                    }
                }
                for (; k<x->traits.ncols; k++) {
                    wvec0 = _mm256_set1_pd(wval[i*ldw+(k+0)]);
                    for (m=0; m+4<=x->traits.nrows; m+=4) {
                        vvec0 = _mm256_mul_pd(_mm256_loadu_pd(&vval[i*ldv+m]),wvec0);
                        _mm256_store_pd(&x_priv[(k+0)*ldxpriv+m],_mm256_add_pd(_mm256_load_pd(&x_priv[(k+0)*ldxpriv+m]),vvec0));
                    }
                    for (; m<x->traits.nrows; m++) {
                        x_priv[k*ldxpriv+m] += vval[i*ldv+m]*wval[i*ldw+k];
                    }
                }
            }
        }
            
        
#pragma omp critical
        {
            m=0;
            for (k=0; k+4<=x->traits.ncols; k+=4) {
                for (m=0; m+4<=x->traits.nrows; m+=4) {
                    #GHOST_UNROLL#_mm256_storeu_pd(&xval[(k+@)*ldx+m],_mm256_add_pd(_mm256_loadu_pd(&xval[(k+@)*ldx+m]),_mm256_load_pd(&x_priv[(k+@)*ldxpriv+m])));#4
                }
                for (; m<x->traits.nrows; m++) {
                    xval[(k+0)*ldx+m] += x_priv[(k+0)*ldxpriv+m];
                    xval[(k+1)*ldx+m] += x_priv[(k+1)*ldxpriv+m];
                    xval[(k+2)*ldx+m] += x_priv[(k+2)*ldxpriv+m];
                    xval[(k+3)*ldx+m] += x_priv[(k+3)*ldxpriv+m];
                }
            }
            for (; k<x->traits.ncols; k++) {
                for (m=0; m+4<=x->traits.nrows; m+=4) {
                    _mm256_storeu_pd(&xval[(k+0)*ldx+m],_mm256_add_pd(_mm256_loadu_pd(&xval[(k+0)*ldx+m]),_mm256_load_pd(&x_priv[(k+0)*ldxpriv+m])));
                }
                for (; m<x->traits.nrows; m++) {
                    xval[k*ldx+m] += x_priv[k*ldxpriv+m];
                }
            }
        }
        free(x_priv);
    }
            
   
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH)
    return ret;
#else
    UNUSED(x);
    UNUSED(v);
    UNUSED(w);
    UNUSED(alpha);
    UNUSED(beta);
    ERROR_LOG("No AVX available!");
    return GHOST_ERR_UNKNOWN;
#endif
}
