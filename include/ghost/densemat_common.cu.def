#include "ghost/config.h"
#include "ghost/types.h"
#include "ghost/util.h"
#include "ghost/densemat_rm.h"
#include "ghost/log.h"
#include "ghost/timing.h"
#include "ghost/locality.h"
#include "ghost/instr.h"
#include "ghost/rand.h"
#include "ghost/cu_sell_kernel.h"

#include <cuda_runtime.h>
#include <stdio.h>
#include <cublas_v2.h>
#include <curand.h>
#include <sys/types.h>
#include <unistd.h>
#include <complex.h>

#include "ghost/cu_complex.h"

#define DENSEMAT_DT v_t
#define THREADSPERBLOCK 256

#ifdef ROWMAJOR
#ifdef COLMAJOR
#error "Only one of ROWMAJOR or COLMAJOR must be defined!"
#endif
#define PREFIX rm
#define IDX(row,col,ld) ((row)*(ld)+(col))
#elif defined (COLMAJOR)
#define PREFIX cm
#define IDX(row,col,ld) ((col)*(ld)+(row))
#else
#error "One of ROWMAJOR or COLMAJOR must be defined!"
#endif

// macros for a 2D thread block with leading dimension along the columns
// this access pattern might seem bad for row-major densemats, but it is
// required for fast reduction with shuffle instructions
#define BLOCK_ROWSTRIDE blockDim.x*gridDim.x
#define BLOCK_ROW blockIdx.x*blockDim.x+threadIdx.x
#define BLOCK_COL blockIdx.y*blockDim.y+threadIdx.y

#define PASTER(x,y) x ## _ ## y
#define EVALUATOR(x,y) PASTER(x,y)
#define FUNCNAME(fun) EVALUATOR(ghost_densemat_cu,EVALUATOR(PREFIX,fun))

extern __shared__ char shared[];

template<typename T>  
__global__ static void FUNCNAME(dot_kernel)(T *res, T *v1, T *v2, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld1, ghost_lidx_t ld2)
{
    T dot;
    zero<T>(dot);
    int row = BLOCK_ROW;
    int col = BLOCK_COL;

    if (col < ncols) {
        T * shmem = (T *)shared; // Shared mem for 32 partial sums

        if (row < nrows) {
            dot = mulConj<T>(v1[IDX(row,col,ld1)],v2[IDX(row,col,ld2)]);
        } else {
            zero<T>(dot);
        }


        dot = accu<T>(dot,ghost_shfl_down(dot, 16));
        dot = accu<T>(dot,ghost_shfl_down(dot, 8));
        dot = accu<T>(dot,ghost_shfl_down(dot, 4));
        dot = accu<T>(dot,ghost_shfl_down(dot, 2));
        dot = accu<T>(dot,ghost_shfl_down(dot, 1));

        if (threadIdx.x%warpSize == 0) {
            int wid = (threadIdx.x / warpSize) + ((blockDim.x / warpSize)*threadIdx.y);
            shmem[wid]=dot; // Write reduced value to shared memory
        }

        __syncthreads();              // Wait for all partial reductions

        //read from shared memory only if that warp existed
        if (threadIdx.x < blockDim.x / warpSize) {
            int lane = (threadIdx.x % warpSize) + ((blockDim.x / warpSize)*threadIdx.y);
            dot = shmem[lane];
        } else {
            zero<T>(dot);
        }

        if (threadIdx.x < warpSize) {
            dot = accu<T>(dot,ghost_shfl_down(dot, 16));
            dot = accu<T>(dot,ghost_shfl_down(dot, 8));
            dot = accu<T>(dot,ghost_shfl_down(dot, 4));
            dot = accu<T>(dot,ghost_shfl_down(dot, 2));
            dot = accu<T>(dot,ghost_shfl_down(dot, 1));
            if (threadIdx.x == 0) {
                res[col*gridDim.x+blockIdx.x] = dot;
            }
        }
    }
}

template<typename T>  
__global__ static void FUNCNAME(vaxpby_kernel)(T *v1, T *v2, T *a, T *b, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld1, ghost_lidx_t ld2)
{
    int idx = blockIdx.x*blockDim.x+threadIdx.x;

    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            v1[IDX(idx,v,ld1)] = axpby<T>(v2[IDX(idx,v,ld2)],v1[IDX(idx,v,ld1)],a[v],b[v]);
        }
    }
}

template<typename T>  
__global__ static void FUNCNAME(axpby_kernel)(T *v1, T *v2, T a, T b, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld1, ghost_lidx_t ld2) 
{
    int idx = blockIdx.x*blockDim.x+threadIdx.x;
    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            v1[IDX(idx,v,ld1)] = axpby<T>(v2[IDX(idx,v,ld2)],v1[IDX(idx,v,ld1)],a,b);
        }
    }
}

template<typename T>  
__global__ static void FUNCNAME(axpbypcz_kernel)(T *v1, T *v2, T *v3, T a, T b, T c, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld1, ghost_lidx_t ld2, ghost_lidx_t ld3) 
{
    T o;
    one(o);
    int idx = blockIdx.x*blockDim.x+threadIdx.x;
    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            v1[IDX(idx,v,ld1)] = axpby<T>(axpby<T>(v2[IDX(idx,v,ld2)],v1[IDX(idx,v,ld1)],a,b),v3[IDX(idx,v,ld3)],o,c);
        }
    }
}

template<typename T>  
__global__ static void FUNCNAME(vaxpbypcz_kernel)(T *v1, T *v2, T *v3, T *a, T *b, T *c, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld1, ghost_lidx_t ld2, ghost_lidx_t ld3) 
{
    T o;
    one(o);
    int idx = blockIdx.x*blockDim.x+threadIdx.x;
    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            v1[IDX(idx,v,ld1)] = axpby<T>(axpby<T>(v2[IDX(idx,v,ld2)],v1[IDX(idx,v,ld1)],a[v],b[v]),v3[IDX(idx,v,ld3)],o,c[v]);
        }
    }
}

template<typename T>  
__global__ static void FUNCNAME(scale_kernel)(T *vec, T a, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld)
{
    int idx = blockIdx.x*blockDim.x+threadIdx.x;

    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            vec[IDX(idx,v,ld)] = scale<T>(a,vec[IDX(idx,v,ld)]);
        }
    }

}

template<typename T>  
__global__ static void FUNCNAME(conj_kernel)(T *vec, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld)
{
    int idx = blockIdx.x*blockDim.x+threadIdx.x;

    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            vec[IDX(idx,v,ld)] = conj(vec[IDX(idx,v,ld)]);
        }
    }

}

template<typename T>  
__global__ static void FUNCNAME(vscale_kernel)(T *vec, T *a, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld)
{
    int idx = blockIdx.x*blockDim.x+threadIdx.x;

    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            vec[IDX(idx,v,ld)] = scale<T>(a[v],vec[IDX(idx,v,ld)]);
        }
    }
}

template<typename T>  
__global__ static void FUNCNAME(fromscalar_kernel)(T *vec, T a, ghost_lidx_t nrows, ghost_lidx_t ncols, ghost_lidx_t ld)
{
    int idx = blockIdx.x*blockDim.x+threadIdx.x;

    for (;idx < nrows; idx+=gridDim.x*blockDim.x) {
        ghost_lidx_t v;
        for (v=0; v<ncols; v++) {
            vec[IDX(idx,v,ld)] = a;
        }
    }
}


template<typename T>  
__global__ static void FUNCNAME(communicationassembly_kernel)(T *vec, T *work, ghost_lidx_t offs, ghost_lidx_t *duelist, ghost_lidx_t ncols, ghost_lidx_t ndues, ghost_lidx_t ld, ghost_lidx_t *perm)
{
    int due = blockIdx.x*blockDim.x+threadIdx.x;
    int col = threadIdx.y;

    if (perm) {
        for (;due < ndues; due+=gridDim.x*blockDim.x) {
            work[(offs+due)*ncols+col] = vec[IDX(perm[duelist[due]],col,ld)];
        }
    } else {
        for (;due < ndues; due+=gridDim.x*blockDim.x) {
            work[(offs+due)*ncols+col] = vec[IDX(duelist[due],col,ld)];
        }
    }
}

extern "C" ghost_error_t FUNCNAME(communicationassembly)(void * work, ghost_lidx_t *dueptr, ghost_densemat_t *vec, ghost_lidx_t *perm)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
  
    if (!vec->context->cu_duelist) {
       ERROR_LOG("cu_duelist must not be NULL!");
       return GHOST_ERR_INVALID_ARG;
    }
    if (!dueptr) {
       ERROR_LOG("dueptr must not be NULL!");
       return GHOST_ERR_INVALID_ARG;
    }


    int nrank, proc, me;
    ghost_context_t *ctx = vec->context;
    
    ghost_nrank(&nrank,ctx->mpicomm); 
    ghost_rank(&me,ctx->mpicomm);
            
    for (proc=0 ; proc<nrank ; proc++){
        dim3 block((int)ceil((double)THREADSPERBLOCK/vec->traits.ncols),vec->traits.ncols);
        dim3 grid((int)ceil((double)ctx->dues[proc]/block.x));
        DEBUG_LOG(1,"communication assembly with grid %d block %dx%d %d->%d",grid.x,block.x,block.y,me,proc);
        if (vec->traits.datatype & GHOST_DT_COMPLEX)
        {
            if (vec->traits.datatype & GHOST_DT_DOUBLE)
            {
                if (ctx->dues[proc]) {
                    FUNCNAME(communicationassembly_kernel)<cuDoubleComplex><<< grid,block >>>((cuDoubleComplex *)vec->cu_val, ((cuDoubleComplex *)work),dueptr[proc],ctx->cu_duelist[proc],vec->traits.ncols,ctx->dues[proc],vec->stride,perm);
                }
            } 
            else 
            {
                if (ctx->dues[proc]) {
                    FUNCNAME(communicationassembly_kernel)<cuFloatComplex><<< grid,block >>>((cuFloatComplex *)vec->cu_val, ((cuFloatComplex *)work),dueptr[proc],ctx->cu_duelist[proc],vec->traits.ncols,ctx->dues[proc],vec->stride,perm);
                }
            }
        }
        else
        {
            if (vec->traits.datatype & GHOST_DT_DOUBLE)
            {
                if (ctx->dues[proc]) {
                    FUNCNAME(communicationassembly_kernel)<double><<< grid,block >>>((double *)vec->cu_val, ((double *)work),dueptr[proc],ctx->cu_duelist[proc],vec->traits.ncols,ctx->dues[proc],vec->stride,perm);
                }
            } 
            else 
            {
                if (ctx->dues[proc]) {
                    FUNCNAME(communicationassembly_kernel)<float><<< grid,block >>>((float *)vec->cu_val, ((float *)work),dueptr[proc],ctx->cu_duelist[proc],vec->traits.ncols,ctx->dues[proc],vec->stride,perm);
                }
            }
        }
    }
    cudaDeviceSynchronize();

    if (cudaPeekAtLastError() != cudaSuccess) {
        ERROR_LOG("Error in kernel");
        return GHOST_ERR_CUDA;
    }
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return GHOST_SUCCESS;

}

extern "C" ghost_error_t FUNCNAME(vaxpy)(ghost_densemat_t *v1, ghost_densemat_t *v2, void *a)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS; 
    
    if (v1->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            complex double *one;
            GHOST_CALL_RETURN(ghost_malloc((void **)&one,v1->traits.ncols*sizeof(complex double)));
            int v;
            for (v=0; v<v1->traits.ncols; v++) {
                one[v] = 1.+I*0.;
            }
            ret =  FUNCNAME(vaxpby)(v1,v2,a,one);
        } 
        else 
        {
            complex float *one;
            GHOST_CALL_RETURN(ghost_malloc((void **)&one,v1->traits.ncols*sizeof(complex float)));
            int v;
            for (v=0; v<v1->traits.ncols; v++) {
                one[v] = 1.+I*0.;
            }
            ret =  FUNCNAME(vaxpby)(v1,v2,a,one);
        }
    }
    else
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            double *one;
            GHOST_CALL_RETURN(ghost_malloc((void **)&one,v1->traits.ncols*sizeof(double)));
            int v;
            for (v=0; v<v1->traits.ncols; v++) {
                one[v] = 1.;
            }
            ret =  FUNCNAME(vaxpby)(v1,v2,a,one);
        } 
        else 
        {
            float *one;
            GHOST_CALL_RETURN(ghost_malloc((void **)&one,v1->traits.ncols*sizeof(float)));
            int v;
            for (v=0; v<v1->traits.ncols; v++) {
                one[v] = 1.;
            }
            ret =  FUNCNAME(vaxpby)(v1,v2,a,one);
        }
    }
    
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}
    
extern "C" ghost_error_t FUNCNAME(vaxpby)(ghost_densemat_t *v1, ghost_densemat_t *v2, void *a, void *b)
{
    if (v1->traits.datatype != v2->traits.datatype)
    {
        ERROR_LOG("Cannot VAXPBY vectors with different data types");
        return GHOST_ERR_NOT_IMPLEMENTED;
    }
    
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;

    void *d_a;
    void *d_b;
    size_t sizeofdt;

    ghost_datatype_size(&sizeofdt,v1->traits.datatype);
    
    GHOST_CALL_GOTO(ghost_cu_malloc(&d_a,v1->traits.ncols*sizeofdt),err,ret);
    GHOST_CALL_GOTO(ghost_cu_malloc(&d_b,v1->traits.ncols*sizeofdt),err,ret);
   
    ghost_lidx_t c; 
    for (c=0; c<v1->traits.ncols; c++) {
            GHOST_CALL_GOTO(ghost_cu_upload(&((char *)d_a)[c*sizeofdt],&((char *)a)[c*sizeofdt],sizeofdt),err,ret);
            GHOST_CALL_GOTO(ghost_cu_upload(&((char *)d_b)[c*sizeofdt],&((char *)b)[c*sizeofdt],sizeofdt),err,ret);
    }
    
    
    void *v1val, *v2val;
    ghost_densemat_t *v1compact, *v2compact;
    
    if (v1->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v1 before operation");
        GHOST_CALL_GOTO(v1->clone(v1,&v1compact,v1->traits.nrows,0,v1->traits.ncols,0),err,ret);
    } else {
        v1compact = v1;
    }
    if (v2->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v2 before operation");
        GHOST_CALL_GOTO(v2->clone(v2,&v2compact,v2->traits.nrows,0,v2->traits.ncols,0),err,ret);
    } else {
        v2compact = v2;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v1compact,&v1val),err,ret);
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v2compact,&v2val),err,ret);
    

    if (v1->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(vaxpby_kernel)<cuDoubleComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuDoubleComplex *)v1val, (cuDoubleComplex *)v2val,(cuDoubleComplex *)d_a,(cuDoubleComplex *)d_b,
                 v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        } 
        else 
        {
            FUNCNAME(vaxpby_kernel)<cuFloatComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuFloatComplex *)v1val, (cuFloatComplex *)v2val,(cuFloatComplex *)d_a,(cuFloatComplex *)d_b,
                 v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        }
    }
    else
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(vaxpby_kernel)<double><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (double *)v1val, (double *)v2val,(double *)d_a,(double *)d_b,
                 v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        } 
        else 
        {
            FUNCNAME(vaxpby_kernel)<float><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (float *)v1val, (float *)v2val,(float *)d_a,(float *)d_b,
                 v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        }
    }
    if (v1compact != v1) {
        GHOST_CALL_GOTO(v1->fromVec(v1,v1compact,0,0),err,ret);
        v1compact->destroy(v1compact);
    }
    if (v2compact != v2) {
        v2compact->destroy(v2compact);
    }
    
    goto out;
err:
out:
    GHOST_CALL_RETURN(ghost_cu_free(d_a));
    GHOST_CALL_RETURN(ghost_cu_free(d_b));
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(vaxpbypcz)(ghost_densemat_t *v1, ghost_densemat_t *v2, void *a, void *b, ghost_densemat_t *v3, void *c)
{
    if (v1->traits.datatype != v2->traits.datatype || v1->traits.datatype != v3->traits.datatype)
    {
        ERROR_LOG("Cannot VAXPBYPCZ vectors with different data types");
        return GHOST_ERR_NOT_IMPLEMENTED;
    }
    
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;

    void *d_a;
    void *d_b;
    void *d_c;
    size_t sizeofdt;

    ghost_datatype_size(&sizeofdt,v1->traits.datatype);
    
    GHOST_CALL_GOTO(ghost_cu_malloc(&d_a,v1->traits.ncols*sizeofdt),err,ret);
    GHOST_CALL_GOTO(ghost_cu_malloc(&d_b,v1->traits.ncols*sizeofdt),err,ret);
    GHOST_CALL_GOTO(ghost_cu_malloc(&d_c,v1->traits.ncols*sizeofdt),err,ret);
   
    ghost_lidx_t col; 
    for (col=0; col<v1->traits.ncols; col++) {
            GHOST_CALL_GOTO(ghost_cu_upload(&((char *)d_a)[col*sizeofdt],&((char *)a)[col*sizeofdt],sizeofdt),err,ret);
            GHOST_CALL_GOTO(ghost_cu_upload(&((char *)d_b)[col*sizeofdt],&((char *)b)[col*sizeofdt],sizeofdt),err,ret);
            GHOST_CALL_GOTO(ghost_cu_upload(&((char *)d_c)[col*sizeofdt],&((char *)c)[col*sizeofdt],sizeofdt),err,ret);
    }
    
    
    void *v1val, *v2val, *v3val;
    ghost_densemat_t *v1compact, *v2compact, *v3compact;
    
    if (v1->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v1 before operation");
        GHOST_CALL_GOTO(v1->clone(v1,&v1compact,v1->traits.nrows,0,v1->traits.ncols,0),err,ret);
    } else {
        v1compact = v1;
    }
    if (v2->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v2 before operation");
        GHOST_CALL_GOTO(v2->clone(v2,&v2compact,v2->traits.nrows,0,v2->traits.ncols,0),err,ret);
    } else {
        v2compact = v2;
    }
    if (v3->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v3 before operation");
        GHOST_CALL_GOTO(v3->clone(v3,&v3compact,v3->traits.nrows,0,v3->traits.ncols,0),err,ret);
    } else {
        v3compact = v3;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v1compact,&v1val),err,ret);
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v2compact,&v2val),err,ret);
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v3compact,&v3val),err,ret);
    

    if (v1->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(vaxpbypcz_kernel)<cuDoubleComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuDoubleComplex *)v1val, (cuDoubleComplex *)v2val, (cuDoubleComplex *)v3val, (cuDoubleComplex *)d_a,(cuDoubleComplex *)d_b,
                (cuDoubleComplex *)d_c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        } 
        else 
        {
            FUNCNAME(vaxpbypcz_kernel)<cuFloatComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuFloatComplex *)v1val, (cuFloatComplex *)v2val, (cuFloatComplex *)v3val, (cuFloatComplex *)d_a,(cuFloatComplex *)d_b,
                (cuFloatComplex *)d_c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        }
    }
    else
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(vaxpbypcz_kernel)<double><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (double *)v1val, (double *)v2val, (double *)v3val, (double *)d_a,(double *)d_b,
                (double *)d_c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        } 
        else 
        {
            FUNCNAME(vaxpbypcz_kernel)<float><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (float *)v1val, (float *)v2val, (float *)v3val, (float *)d_a,(float *)d_b,
                (float *)d_c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        }
    }
    if (v1compact != v1) {
        GHOST_CALL_GOTO(v1->fromVec(v1,v1compact,0,0),err,ret);
        v1compact->destroy(v1compact);
    }
    if (v2compact != v2) {
        v2compact->destroy(v2compact);
    }
    if (v3compact != v3) {
        v3compact->destroy(v3compact);
    }
    
    goto out;
err:
out:
    GHOST_CALL_RETURN(ghost_cu_free(d_a));
    GHOST_CALL_RETURN(ghost_cu_free(d_b));
    GHOST_CALL_RETURN(ghost_cu_free(d_c));
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(dotprod)(ghost_densemat_t *vec, void *res, ghost_densemat_t *vec2)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;
   
    if (vec->traits.datatype != vec2->traits.datatype)
    {
        ERROR_LOG("Cannot DOT vectors with different data types (%s and %s)",ghost_datatype_string(vec->traits.datatype),ghost_datatype_string(vec2->traits.datatype));
        return GHOST_ERR_NOT_IMPLEMENTED;
    }
    size_t sizeofdt;
    ghost_datatype_size(&sizeofdt,vec->traits.datatype);
    ghost_densemat_t *veccompact;
    ghost_densemat_t *vec2compact;

    if (vec->traits.flags & GHOST_DENSEMAT_VIEW) {
        INFO_LOG("Cloning (and compressing) vec1 before dotproduct");
        vec->clone(vec,&veccompact,vec->traits.nrows,0,vec->traits.ncols,0);
    } else {
        veccompact = vec;
    }
    if (vec2->traits.flags & GHOST_DENSEMAT_VIEW) {
        INFO_LOG("Cloning (and compressing) vec2 before dotproduct");
        vec2->clone(vec2,&vec2compact,vec2->traits.nrows,0,vec2->traits.ncols,0);
    } else {
        vec2compact = vec2;
    }
 
#ifdef COLMAJOR
    int vec1stride = 1;
    int vec2stride = 1;
#else
    int vec1stride = veccompact->stride;
    int vec2stride = vec2compact->stride;
#endif

    dim3 block(PAD(CEILDIV(THREADSPERBLOCK,vec->traits.ncols),32),vec->traits.ncols,1);
    while ((block.x*block.y) > THREADSPERBLOCK && (block.x > 32)) {
        block.x -= 32;
    }
    while ((block.x*block.y) > THREADSPERBLOCK) {
        block.y--;
    }

    dim3 grid(CEILDIV(vec->traits.nrows,block.x),CEILDIV(vec->traits.ncols,block.y),1);
    
    if (vec->traits.storage == GHOST_DENSEMAT_ROWMAJOR && vec->stride > 1) {
        if (vec->traits.datatype & GHOST_DT_COMPLEX) {
            if (vec->traits.datatype & GHOST_DT_DOUBLE) {
                cuDoubleComplex * partres;
                ghost_cu_malloc((void **)&partres,vec->traits.ncols*grid.x*sizeof(complex double));
                FUNCNAME(dot_kernel)<cuDoubleComplex><<< grid,block,block.x/32*sizeof(complex double)*block.y >>>(
                        (cuDoubleComplex *)partres,
                        (cuDoubleComplex *)veccompact->cu_val, (cuDoubleComplex *)vec2compact->cu_val,
                        vec->traits.nrows,vec->traits.ncols,vec->stride,vec2->stride);
                int col;
                for (col=0; col<vec->traits.ncols; col++) {
                    ghost_deviceReduceSum<cuDoubleComplex><<<1,1024,32*sizeof(complex double)>>>(&partres[col*grid.x],&partres[col],grid.x);
                }
                ghost_cu_download(res,partres,vec->traits.ncols*sizeof(complex double));
                ghost_cu_free(partres);
            } else {
                cuFloatComplex * partres;
                ghost_cu_malloc((void **)&partres,vec->traits.ncols*grid.x*sizeof(complex float));
                FUNCNAME(dot_kernel)<cuFloatComplex><<< grid,block,block.x/32*sizeof(complex float)*block.y >>>(
                        (cuFloatComplex *)partres,
                        (cuFloatComplex *)veccompact->cu_val, (cuFloatComplex *)vec2compact->cu_val,
                        vec->traits.nrows,vec->traits.ncols,vec->stride,vec2->stride);
                int col;
                for (col=0; col<vec->traits.ncols; col++) {
                    ghost_deviceReduceSum<cuFloatComplex><<<1,1024,32*sizeof(complex float)>>>(&partres[col*grid.x],&partres[col],grid.x);
                }
                ghost_cu_download(res,partres,vec->traits.ncols*sizeof(complex float));
                ghost_cu_free(partres);
            }
        } else {
            if (vec->traits.datatype & GHOST_DT_DOUBLE) {
                INFO_LOG("%d %d",vec->traits.ncols,grid.x);
                double * partres;
                ghost_cu_malloc((void **)&partres,vec->traits.ncols*grid.x*sizeof(double));
                FUNCNAME(dot_kernel)<double><<< grid,block,block.x/32*sizeof(double)*block.y >>>(
                        (double *)partres,
                        (double *)veccompact->cu_val, (double *)vec2compact->cu_val,
                        vec->traits.nrows,vec->traits.ncols,vec->stride,vec2->stride);
                int col;
                for (col=0; col<vec->traits.ncols; col++) {
                    ghost_deviceReduceSum<double><<<1,1024,32*sizeof(double)>>>(&partres[col*grid.x],&partres[col],grid.x);
                }
                ghost_cu_download(res,partres,vec->traits.ncols*sizeof(double));
                ghost_cu_free(partres);
            } else {
                float * partres;
                ghost_cu_malloc((void **)&partres,vec->traits.ncols*grid.x*sizeof(float));
                FUNCNAME(dot_kernel)<float><<< grid,block,block.x/32*sizeof(float)*block.y >>>(
                        (float *)partres,
                        (float *)veccompact->cu_val, (float *)vec2compact->cu_val,
                        vec->traits.nrows,vec->traits.ncols,vec->stride,vec2->stride);
                int col;
                for (col=0; col<vec->traits.ncols; col++) {
                    ghost_deviceReduceSum<float><<<1,1024,32*sizeof(float)>>>(&partres[col*grid.x],&partres[col],grid.x);
                }
                ghost_cu_download(res,partres,vec->traits.ncols*sizeof(float));
                ghost_cu_free(partres);
            }
        }

    } else { 
        cublasHandle_t ghost_cublas_handle;
        GHOST_CALL_GOTO(ghost_cu_cublas_handle(&ghost_cublas_handle),err,ret); 
        ghost_lidx_t v;
        for (v=0; v<veccompact->traits.ncols; v++)
        {
            char *v1 = veccompact->cu_val+v*veccompact->stride*veccompact->elSize;
            char *v2 = vec2compact->cu_val+v*vec2compact->stride*veccompact->elSize;
            if (vec->traits.datatype & GHOST_DT_COMPLEX)
            {
                if (vec->traits.datatype & GHOST_DT_DOUBLE)
                {
                    CUBLAS_CALL_GOTO(cublasZdotc(ghost_cublas_handle,vec->traits.nrows,
                                (const cuDoubleComplex *)v1,vec1stride,(const cuDoubleComplex *)v2,vec2stride,&((cuDoubleComplex *)res)[v]),err,ret);
                } 
                else 
                {
                    CUBLAS_CALL_GOTO(cublasCdotc(ghost_cublas_handle,vec->traits.nrows,
                                (const cuFloatComplex *)v1,vec1stride,(const cuFloatComplex *)v2,vec2stride,&((cuFloatComplex *)res)[v]),err,ret);
                }
            }
            else
            {
                if (vec->traits.datatype & GHOST_DT_DOUBLE)
                {
                    CUBLAS_CALL_GOTO(cublasDdot(ghost_cublas_handle,vec->traits.nrows,
                                (const double *)v1,vec1stride,(const double *)v2,vec2stride,&((double *)res)[v]),err,ret);
                } 
                else 
                {
                    CUBLAS_CALL_GOTO(cublasSdot(ghost_cublas_handle,vec->traits.nrows,
                                (const float *)v1,vec1stride,(const float *)v2,vec2stride,&((float *)res)[v]),err,ret);
                }
            }
        }
    }
    
    if (veccompact != vec) {
        veccompact->destroy(veccompact);
    }
    if (vec2compact != vec2) {
        vec2compact->destroy(vec2compact);
    }
    goto out;
err:
out:
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(axpy)(ghost_densemat_t *v1, ghost_densemat_t *v2, void *a)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS; 
    
    if (v1->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            const cuDoubleComplex one = make_cuDoubleComplex(1.,0);
            ret =  FUNCNAME(axpby)(v1,v2,a,(void *)&one);
        } 
        else 
        {
            const cuFloatComplex one = make_cuFloatComplex(1.,0.);
            ret = FUNCNAME(axpby)(v1,v2,a,(void *)&one);
        }
    }
    else
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            const double one = 1.;
            ret = FUNCNAME(axpby)(v1,v2,a,(void *)&one);
        } 
        else 
        {
            const float one = 1.f;
            ret = FUNCNAME(axpby)(v1,v2,a,(void *)&one);
        }
    }
    
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(axpby)(ghost_densemat_t *v1, ghost_densemat_t *v2, void *a, void *b)
{
    if (v1->traits.datatype != v2->traits.datatype)
    {
        ERROR_LOG("Cannot AXPBY vectors with different data types");
        return GHOST_ERR_NOT_IMPLEMENTED;
    }
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;
    void *v1val, *v2val;
    ghost_densemat_t *v1compact, *v2compact;
    
    if (v1->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v1 before operation");
        GHOST_CALL_GOTO(v1->clone(v1,&v1compact,v1->traits.nrows,0,v1->traits.ncols,0),err,ret);
    } else {
        v1compact = v1;
    }
    if (v2->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v2 before operation");
        GHOST_CALL_GOTO(v2->clone(v2,&v2compact,v2->traits.nrows,0,v2->traits.ncols,0),err,ret);
    } else {
        v2compact = v2;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v1compact,&v1val),err,ret);
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v2compact,&v2val),err,ret);

    if (v1->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(axpby_kernel)<cuDoubleComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>
                ((cuDoubleComplex *)v1val, (cuDoubleComplex *)v2val,*((cuDoubleComplex *)a),*((cuDoubleComplex *)b),v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        } 
        else 
        {
            FUNCNAME(axpby_kernel)<cuFloatComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>
                ((cuFloatComplex *)v1val, (cuFloatComplex *)v2val,*((cuFloatComplex *)a),*((cuFloatComplex *)b),v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
            
        }
    }
    else
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(axpby_kernel)<double><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>
                ((double *)v1val, (double *)v2val,*((double *)a),*((double *)b),v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        } 
        else 
        {
            FUNCNAME(axpby_kernel)<float><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>
                ((float *)v1val, (float *)v2val,*((float *)a),*((float *)b),v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride);
        }
    }
    if (v1compact != v1) {
        GHOST_CALL_GOTO(v1->fromVec(v1,v1compact,0,0),err,ret);
        v1compact->destroy(v1compact);
    }
    if (v2compact != v2) {
        v2compact->destroy(v2compact);
    }

    goto out;
err:
out:
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(axpbypcz)(ghost_densemat_t *v1, ghost_densemat_t *v2, void *a, void *b, ghost_densemat_t *v3, void *c)
{
    if (v1->traits.datatype != v2->traits.datatype || v1->traits.datatype != v3->traits.datatype)
    {
        ERROR_LOG("Cannot VAXPBYPCZ vectors with different data types");
        return GHOST_ERR_NOT_IMPLEMENTED;
    }
    
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;

    size_t sizeofdt;

    ghost_datatype_size(&sizeofdt,v1->traits.datatype);
    
    void *v1val, *v2val, *v3val;
    ghost_densemat_t *v1compact, *v2compact, *v3compact;
    
    if (v1->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v1 before operation");
        GHOST_CALL_GOTO(v1->clone(v1,&v1compact,v1->traits.nrows,0,v1->traits.ncols,0),err,ret);
    } else {
        v1compact = v1;
    }
    if (v2->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v2 before operation");
        GHOST_CALL_GOTO(v2->clone(v2,&v2compact,v2->traits.nrows,0,v2->traits.ncols,0),err,ret);
    } else {
        v2compact = v2;
    }
    if (v3->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) v3 before operation");
        GHOST_CALL_GOTO(v3->clone(v3,&v3compact,v3->traits.nrows,0,v3->traits.ncols,0),err,ret);
    } else {
        v3compact = v3;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v1compact,&v1val),err,ret);
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v2compact,&v2val),err,ret);
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(v3compact,&v3val),err,ret);
    

    if (v1->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(axpbypcz_kernel)<cuDoubleComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuDoubleComplex *)v1val, (cuDoubleComplex *)v2val, (cuDoubleComplex *)v3val, *(cuDoubleComplex *)a,*(cuDoubleComplex *)b,
                *(cuDoubleComplex *)c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        } 
        else 
        {
            FUNCNAME(axpbypcz_kernel)<cuFloatComplex><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuFloatComplex *)v1val, (cuFloatComplex *)v2val, (cuFloatComplex *)v3val, *(cuFloatComplex *)a,*(cuFloatComplex *)b,
                *(cuFloatComplex *)c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        }
    }
    else
    {
        if (v1->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(axpbypcz_kernel)<double><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (double *)v1val, (double *)v2val, (double *)v3val, *(double *)a,*(double *)b,
                *(double *)c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        } 
        else 
        {
            FUNCNAME(axpbypcz_kernel)<float><<< (int)ceil((double)v1->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (float *)v1val, (float *)v2val, (float *)v3val, *(float *)a,*(float *)b,
                *(float *)c, v1->traits.nrows,v1->traits.ncols,v1->stride,v2->stride, v3->stride);
        }
    }
    if (v1compact != v1) {
        GHOST_CALL_GOTO(v1->fromVec(v1,v1compact,0,0),err,ret);
        v1compact->destroy(v1compact);
    }
    if (v2compact != v2) {
        v2compact->destroy(v2compact);
    }
    if (v3compact != v3) {
        v3compact->destroy(v3compact);
    }
    
    goto out;
err:
out:
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(scale)(ghost_densemat_t *vec, void *a)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;
    
    void *vecval;
    ghost_densemat_t *veccompact;
    
    if (vec->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) vec before operation");
        GHOST_CALL_GOTO(vec->clone(vec,&veccompact,vec->traits.nrows,0,vec->traits.ncols,0),err,ret);
    } else {
        veccompact = vec;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(veccompact,&vecval),err,ret);
    
    if (vec->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (vec->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(scale_kernel)<cuDoubleComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (cuDoubleComplex *)vecval, *(cuDoubleComplex *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        } 
        else 
        {
            FUNCNAME(scale_kernel)<cuFloatComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (cuFloatComplex *)vecval, *(cuFloatComplex *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        }
    }
    else
    {
        if (vec->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(scale_kernel)<double><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (double *)vecval, *(double *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        } 
        else 
        {
            FUNCNAME(scale_kernel)<float><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (float *)vecval, *(float *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        }
    }
    if (veccompact != vec) {
        INFO_LOG("Transform back");
        GHOST_CALL_GOTO(vec->fromVec(vec,veccompact,0,0),err,ret);
        veccompact->destroy(veccompact);
    }
    
    goto out;

err:

out:
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    
    return ret;
}

extern "C" ghost_error_t FUNCNAME(vscale)(ghost_densemat_t *vec, void *a)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;

    void *d_a;
    ghost_idx_t c;
    void *vecval;
    ghost_densemat_t *veccompact;
    
    if (vec->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) vec before operation");
        GHOST_CALL_GOTO(vec->clone(vec,&veccompact,vec->traits.nrows,0,vec->traits.ncols,0),err,ret);
    } else {
        veccompact = vec;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(veccompact,&vecval),err,ret);

    GHOST_CALL_GOTO(ghost_cu_malloc(&d_a,vec->traits.ncols*vec->elSize),err,ret);
    
    for (c=0; c<vec->traits.ncols; c++) {
        GHOST_CALL_GOTO(ghost_cu_upload(&((char *)d_a)[c*vec->elSize],&((char *)a)[c*vec->elSize],vec->elSize),err,ret);
    }
    
    if (vec->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (vec->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(vscale_kernel)<cuDoubleComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (cuDoubleComplex *)vecval, (cuDoubleComplex *)d_a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        } 
        else 
        {
            FUNCNAME(vscale_kernel)<cuFloatComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (cuFloatComplex *)vecval, (cuFloatComplex *)d_a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        }
    }
    else
    {
        if (vec->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(vscale_kernel)<double><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (double *)vecval, (double *)d_a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        } 
        else 
        {
            FUNCNAME(vscale_kernel)<float><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (float *)vecval, (float *)d_a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        }
    }
    if (veccompact != vec) {
        INFO_LOG("Transform back");
        GHOST_CALL_GOTO(vec->fromVec(vec,veccompact,0,0),err,ret);
        veccompact->destroy(veccompact);
    }

    goto out;
err:
out:
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(fromScalar)(ghost_densemat_t *vec, void *a)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_INITIALIZATION);
    ghost_error_t ret = GHOST_SUCCESS;
    int needInit = 0;
    ghost_densemat_rm_malloc(vec,&needInit);
    
    void *vecval;
    ghost_densemat_t *veccompact;
    
    if (vec->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) vec before operation");
        GHOST_CALL_GOTO(vec->clone(vec,&veccompact,vec->traits.nrows,0,vec->traits.ncols,0),err,ret);
    } else {
        veccompact = vec;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(veccompact,&vecval),err,ret);

    if (vec->traits.datatype & GHOST_DT_COMPLEX)
    {
        if (vec->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(fromscalar_kernel)<cuDoubleComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (cuDoubleComplex *)vecval, *(cuDoubleComplex *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        } 
        else 
        {
            FUNCNAME(fromscalar_kernel)<cuFloatComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (cuFloatComplex *)vecval, *(cuFloatComplex *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        }
    }
    else
    {
        if (vec->traits.datatype & GHOST_DT_DOUBLE)
        {
            FUNCNAME(fromscalar_kernel)<double><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (double *)vecval, *(double *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        } 
        else 
        {
            FUNCNAME(fromscalar_kernel)<float><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                    (float *)vecval, *(float *)a,
                    vec->traits.nrows,vec->traits.ncols,vec->stride);
        }
    }
    if (veccompact != vec) {
        INFO_LOG("Transform back");
        GHOST_CALL_GOTO(vec->fromVec(vec,veccompact,0,0),err,ret);
        veccompact->destroy(veccompact);
    }
    
    goto out;
err:
out:
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_INITIALIZATION);
    return ret;
}

extern "C" ghost_error_t FUNCNAME(fromRand)(ghost_densemat_t *vec)
{
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_INITIALIZATION);
    ghost_error_t ret = GHOST_SUCCESS;

    void *raw = NULL;
    ghost_densemat_t *onevec;
    long pid = getpid();
    double time;
    double one[] = {1.,1.};
    float fone[] = {1.,0.};
    double minusahalf[] = {-0.5,0.};
    float fminusahalf[] = {-0.5,0.};
    
    ghost_timing_wcmilli(&time);
    int needInit = 0;
    ghost_densemat_rm_malloc(vec,&needInit);
    curandGenerator_t gen;
    GHOST_CALL_GOTO(ghost_cu_rand_generator_get(&gen),err,ret);

    vec->clone(vec,&onevec,vec->traits.nrows,0,vec->traits.ncols,0);
    onevec->fromScalar(onevec,one);

    one[1] = 0.;
    ghost_densemat_t *compactvec;
    
    if ((vec->traits.ncolsorig != vec->traits.ncols) || (vec->traits.flags & GHOST_DENSEMAT_SCATTERED)) {
        INFO_LOG("Cloning (and compressing) vec before operation");
        vec->clone(vec,&compactvec,vec->traits.nrows,0,vec->traits.ncols,0);
    } else {
        compactvec = vec;
    }

    cudaMalloc(&raw,vec->traits.nrows*vec->traits.ncols*vec->elSize);
    if (vec->traits.datatype & GHOST_DT_COMPLEX) {
        if (vec->traits.datatype & GHOST_DT_DOUBLE) {
            CURAND_CALL_GOTO(curandGenerateUniformDouble(gen,(double *)raw,vec->traits.nrows*vec->traits.ncols*2),err,ret);
        } else {
            CURAND_CALL_GOTO(curandGenerateUniform(gen,(float *)raw,vec->traits.nrows*vec->traits.ncols*2),err,ret);
        }
    } else {
        if (vec->traits.datatype & GHOST_DT_DOUBLE) {
            CURAND_CALL_GOTO(curandGenerateUniformDouble(gen,(double *)raw,vec->traits.nrows*vec->traits.ncols),err,ret);
        } else {
            CURAND_CALL_GOTO(curandGenerateUniform(gen,(float *)raw,vec->traits.nrows*vec->traits.ncols),err,ret);
        }
    }

    ghost_cu_memcpy2d(compactvec->cu_val,compactvec->traits.ncolspadded*vec->elSize,raw,vec->traits.ncols*vec->elSize,compactvec->traits.ncols*vec->elSize,compactvec->traits.nrows);

    if (compactvec->traits.datatype & GHOST_DT_DOUBLE) {
        compactvec->axpby(compactvec,onevec,minusahalf,one);
    } else {
        compactvec->axpby(compactvec,onevec,fminusahalf,fone);
    }
    if (compactvec != vec) {
        vec->fromVec(vec,compactvec,0,0);
        compactvec->destroy(compactvec);
    }
    goto out;
err:
out:
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_INITIALIZATION);
    onevec->destroy(onevec);
    cudaFree(raw);

    return ret;
}

extern "C" ghost_error_t FUNCNAME(conj)(ghost_densemat_t *vec)
{
    if (vec->traits.datatype & GHOST_DT_REAL) {
        return GHOST_SUCCESS;
    }
    GHOST_FUNC_ENTER(GHOST_FUNCTYPE_MATH);
    ghost_error_t ret = GHOST_SUCCESS;
    
    void *vecval;
    ghost_densemat_t *veccompact;
    
    if (vec->traits.flags & GHOST_DENSEMAT_SCATTERED) {
        INFO_LOG("Cloning (and compressing) vec before operation");
        GHOST_CALL_GOTO(vec->clone(vec,&veccompact,vec->traits.nrows,0,vec->traits.ncols,0),err,ret);
    } else {
        veccompact = vec;
    }
    GHOST_CALL_GOTO(ghost_densemat_cu_valptr(veccompact,&vecval),err,ret);
    
    if (vec->traits.datatype & GHOST_DT_DOUBLE)
    {
        FUNCNAME(conj_kernel)<cuDoubleComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuDoubleComplex *)vecval,
                vec->traits.nrows,vec->traits.ncols,vec->stride);
    } 
    else 
    {
        FUNCNAME(conj_kernel)<cuFloatComplex><<< (int)ceil((double)vec->traits.nrows/THREADSPERBLOCK),THREADSPERBLOCK >>>(
                (cuFloatComplex *)vecval,
                vec->traits.nrows,vec->traits.ncols,vec->stride);
    }
    if (veccompact != vec) {
        INFO_LOG("Transform back");
        GHOST_CALL_GOTO(vec->fromVec(vec,veccompact,0,0),err,ret);
        veccompact->destroy(veccompact);
    }
    
    goto out;

err:

out:
    cudaDeviceSynchronize();
    GHOST_FUNC_EXIT(GHOST_FUNCTYPE_MATH);

    
    return ret;
}
